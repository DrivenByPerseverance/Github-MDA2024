{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 1: LOAD DATA FROM FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 1: LOAD DATA FROM FOLDER\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your Parquet gzip file\n",
    "file_path_1 = '../../1_Data/Medical_transport/ambulance_locations.parquet.gzip'\n",
    "file_path_2 = '../../1_Data/Medical_transport/pit_locations.parquet.gzip'\n",
    "file_path_3 = '../../1_Data/Medical_transport/mug_locations.parquet.gzip'\n",
    "file_path_4 = '../../1_Data/AED_locations/aed_locations.parquet.gzip'\n",
    "file_path_5 = '../../1_Data/Interventions_data/interventions_bxl.parquet.gzip'\n",
    "file_path_6 = '../../1_Data/Interventions_data/interventions_bxl2.parquet.gzip'\n",
    "file_path_7 = '../../1_Data/Interventions_data/interventions1.parquet.gzip'\n",
    "file_path_8 = '../../1_Data/Interventions_data/interventions2.parquet.gzip'\n",
    "file_path_9 = '../../1_Data/Interventions_data/interventions3.parquet.gzip'\n",
    "file_path_10 = '../../1_Data/Interventions_data/cad9.parquet.gzip'\n",
    "\n",
    "# Read the Parquet file into a pandas DataFrame\n",
    "df_ambulance_locations = pd.read_parquet(file_path_1, engine='pyarrow')\n",
    "df_pit_locations = pd.read_parquet(file_path_2, engine='pyarrow')\n",
    "df_mug_locations = pd.read_parquet(file_path_3, engine='pyarrow')\n",
    "df_aed_locations = pd.read_parquet(file_path_4, engine='pyarrow')\n",
    "df_interventions_bxl = pd.read_parquet(file_path_5, engine='pyarrow')\n",
    "df_interventions_bxl2 = pd.read_parquet(file_path_6, engine='pyarrow')\n",
    "df_interventions1 = pd.read_parquet(file_path_7, engine='pyarrow')\n",
    "df_interventions2 = pd.read_parquet(file_path_8, engine='pyarrow')\n",
    "df_interventions3 = pd.read_parquet(file_path_9, engine='pyarrow')\n",
    "df_cad9 = pd.read_parquet(file_path_10, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 2: LOAD DATA FROM PUBLIC S3 BUCKET IN WHICH EACH FILE IS MADE PUBLIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 2: LOAD DATA FROM PUBLIC S3 BUCKET IN WHICH EACH FILE IS MADE PUBLIC\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your Parquet gzip file\n",
    "file_path_1 = 's3://mda2024public/Medical_transport/ambulance_locations.parquet.gzip'\n",
    "file_path_2 = 's3://mda2024public/Medical_transport/pit_locations.parquet.gzip'\n",
    "file_path_3 = 's3://mda2024public/Medical_transport/mug_locations.parquet.gzip'\n",
    "file_path_4 = 's3://mda2024public/AED_locations/aed_locations.parquet.gzip'\n",
    "file_path_5 = 's3://mda2024public/Interventions_data/interventions_bxl.parquet.gzip'\n",
    "file_path_6 = 's3://mda2024public/Interventions_data/interventions_bxl2.parquet.gzip'\n",
    "file_path_7 = 's3://mda2024public/Interventions_data/interventions1.parquet.gzip'\n",
    "file_path_8 = 's3://mda2024public/Interventions_data/interventions2.parquet.gzip'\n",
    "file_path_9 = 's3://mda2024public/Interventions_data/interventions3.parquet.gzip'\n",
    "file_path_10 = 's3://mda2024public/Interventions_data/cad9.parquet.gzip'\n",
    "\n",
    "# Read the Parquet file into a pandas DataFrame\n",
    "df_ambulance_locations = pd.read_parquet(file_path_1, engine='pyarrow')\n",
    "df_pit_locations = pd.read_parquet(file_path_2, engine='pyarrow')\n",
    "df_mug_locations = pd.read_parquet(file_path_3, engine='pyarrow')\n",
    "df_aed_locations = pd.read_parquet(file_path_4, engine='pyarrow')\n",
    "df_interventions_bxl = pd.read_parquet(file_path_5, engine='pyarrow')\n",
    "df_interventions_bxl2 = pd.read_parquet(file_path_6, engine='pyarrow')\n",
    "df_interventions1 = pd.read_parquet(file_path_7, engine='pyarrow')\n",
    "df_interventions2 = pd.read_parquet(file_path_8, engine='pyarrow')\n",
    "df_interventions3 = pd.read_parquet(file_path_9, engine='pyarrow')\n",
    "df_cad9 = pd.read_parquet(file_path_10, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 3: LOAD DATA FROM PRIVATE S3 BUCKET BY USING credentials.py OR .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 3: LOAD DATA FROM PRIVATE S3 BUCKET BY USING credentials.py OR .env\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# OPTION 1: Get AWS credentials from credentials.py\n",
    "#from credentials import AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY\n",
    "\n",
    "# OPTION 2: Get AWS credentials from environment variables\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Get AWS credentials from environment variables\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "# Create an S3 client\n",
    "s3_client = boto3.client('s3', \n",
    "                         aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                         aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "# Specify the bucket name\n",
    "bucket_name = \"mda2024\"\n",
    "\n",
    "# Define a function to read Parquet files from S3\n",
    "def read_parquet_from_s3(file_key):\n",
    "    obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    return pd.read_parquet(BytesIO(obj['Body'].read()), engine='pyarrow')\n",
    "\n",
    "# List of file paths in the S3 bucket\n",
    "file_keys = [\n",
    "    'Medical_transport/ambulance_locations.parquet.gzip',\n",
    "    'Medical_transport/pit_locations.parquet.gzip',\n",
    "    'Medical_transport/mug_locations.parquet.gzip',\n",
    "    'AED_locations/aed_locations.parquet.gzip',\n",
    "    'Interventions_data/interventions_bxl.parquet.gzip',\n",
    "    'Interventions_data/interventions_bxl2.parquet.gzip',\n",
    "    'Interventions_data/interventions1.parquet.gzip',\n",
    "    'Interventions_data/interventions2.parquet.gzip',\n",
    "    'Interventions_data/interventions3.parquet.gzip',\n",
    "    'Interventions_data/cad9.parquet.gzip'\n",
    "]\n",
    "\n",
    "# Read Parquet files into pandas DataFrames\n",
    "dfs = [read_parquet_from_s3(file_key) for file_key in file_keys]\n",
    "\n",
    "# Assign DataFrames to variables\n",
    "(df_ambulance_locations, df_pit_locations, df_mug_locations,\n",
    " df_aed_locations, df_interventions_bxl, df_interventions_bxl2,\n",
    " df_interventions1, df_interventions2, df_interventions3, df_cad9) = dfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
