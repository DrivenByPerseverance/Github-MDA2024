{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your Parquet gzip file\n",
    "file_path_1 = '../../1_Data/Medical_transport/ambulance_locations.parquet.gzip'\n",
    "file_path_2 = '../../1_Data/Medical_transport/pit_locations.parquet.gzip'\n",
    "file_path_3 = '../../1_Data/Medical_transport/mug_locations.parquet.gzip'\n",
    "file_path_4 = '../../1_Data/AED_locations/aed_locations.parquet.gzip'\n",
    "file_path_5 = '../../1_Data/Interventions_data/interventions_bxl.parquet.gzip'\n",
    "file_path_6 = '../../1_Data/Interventions_data/interventions_bxl2.parquet.gzip'\n",
    "file_path_7 = '../../1_Data/Interventions_data/interventions1.parquet.gzip'\n",
    "file_path_8 = '../../1_Data/Interventions_data/interventions2.parquet.gzip'\n",
    "file_path_9 = '../../1_Data/Interventions_data/interventions3.parquet.gzip'\n",
    "file_path_10 = '../../1_Data/Interventions_data/cad9.parquet.gzip'\n",
    "\n",
    "# Read the Parquet file into a pandas DataFrame\n",
    "df_ambulance_locations = pd.read_parquet(file_path_1, engine='pyarrow')\n",
    "df_pit_locations = pd.read_parquet(file_path_2, engine='pyarrow')\n",
    "df_mug_locations = pd.read_parquet(file_path_3, engine='pyarrow')\n",
    "df_aed_locations = pd.read_parquet(file_path_4, engine='pyarrow')\n",
    "df_interventions_bxl = pd.read_parquet(file_path_5, engine='pyarrow')\n",
    "df_interventions_bxl2 = pd.read_parquet(file_path_6, engine='pyarrow')\n",
    "df_interventions1 = pd.read_parquet(file_path_7, engine='pyarrow')\n",
    "df_interventions2 = pd.read_parquet(file_path_8, engine='pyarrow')\n",
    "df_interventions3 = pd.read_parquet(file_path_9, engine='pyarrow')\n",
    "df_cad9 = pd.read_parquet(file_path_10, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data sets from data.py\n",
    "#from data import df_ambulance_locations, df_pit_locations, df_mug_locations, df_aed_locations, df_interventions_bxl, df_interventions_bxl2, df_interventions1, df_interventions2, df_interventions3, df_cad9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCATIONS: Print data set + data type of each variable + values of specific columns\n",
    "\n",
    "#print(df_ambulance_locations)\n",
    "#print(df_ambulance_locations.dtypes)\n",
    "#print(df_ambulance_locations.departure_location)\n",
    "\n",
    "#print(df_pit_locations)\n",
    "#print(df_pit_locations.dtypes)\n",
    "#print(df_pit_locations['campus'].unique())\n",
    "\n",
    "#print(df_mug_locations)\n",
    "#print(df_mug_locations.dtypes)\n",
    "#print(df_mug_locations.name_hospital)\n",
    "#print(df_mug_locations.address_campus)\n",
    "\n",
    "#print(df_aed_locations)\n",
    "#print(df_aed_locations.dtypes)\n",
    "#print(df_aed_locations['address'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERVENTIONS: Print data set + data type of each variable + values of specific columns\n",
    "\n",
    "#print(df_interventions_bxl)\n",
    "#print(df_interventions_bxl.dtypes)\n",
    "#print(df_interventions_bxl.postalcode_permanence)\n",
    "#print(df_interventions_bxl.t0)\n",
    "\n",
    "#print(df_interventions_bxl2)\n",
    "#print(df_interventions_bxl2.dtypes)\n",
    "#print(df_interventions_bxl2['EventType and EventLevel'].unique())\n",
    "#print(df_interventions_bxl2['EventType'].unique())\n",
    "#print(df_interventions_bxl2['eventlevel_trip'].unique())\n",
    "#print(df_interventions_bxl2.T0)\n",
    "\n",
    "#print(df_interventions1)\n",
    "#print(df_interventions1.dtypes)\n",
    "#print(df_interventions1.T0)\n",
    "\n",
    "#print(df_interventions2)\n",
    "#print(df_interventions2.dtypes)\n",
    "#print(df_interventions2.T0)\n",
    "\n",
    "#print(df_interventions3)\n",
    "#print(df_interventions3.dtypes)\n",
    "#print(df_interventions3.T0)\n",
    "\n",
    "#print(df_combined)\n",
    "#print(df_combined.dtypes)\n",
    "#print(df_combined.T0)\n",
    "#print(combined_df['Vector type'].unique())\n",
    "#print(combined_df['EventType Firstcall'].unique())\n",
    "#print(combined_df['EventLevel Firstcall'].unique())\n",
    "#print(combined_df['Abandon reason'].unique())\n",
    "\n",
    "#print(df_cad9)\n",
    "#print(df_cad9.dtypes)\n",
    "#print(df_cad9.T0)\n",
    "\n",
    "#print(df_interventions1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset\n",
    "all_datasets = [df_ambulance_locations, df_pit_locations, df_mug_locations, df_aed_locations, df_interventions_bxl, df_interventions_bxl2, df_interventions1, df_interventions2, df_interventions3, df_cad9]\n",
    "all_interventions = [df_interventions_bxl, df_interventions_bxl2, df_interventions1, df_interventions2, df_interventions3, df_cad9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check non-null counts of each dataset\n",
    "#for df in all_datasets: df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each dataset and remove duplicate rows\n",
    "for i, dataset in enumerate(all_datasets, start=1):\n",
    "    #print(f\"Original size of Dataset {i}: {len(dataset)}\")\n",
    "    dataset.drop_duplicates(inplace=True)\n",
    "    #print(f\"Size of Dataset {i} after removing duplicates: {len(dataset)}\")\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove multiple spaces\n",
    "def remove_multiple_spaces(event):\n",
    "    if isinstance(event, str):\n",
    "        return ' '.join(event.split())\n",
    "    else:\n",
    "        return event\n",
    "\n",
    "# Apply the function to all columns of each dataset\n",
    "for idx, dataset in enumerate(all_datasets):\n",
    "    for column in dataset.columns:\n",
    "        dataset[column] = dataset[column].apply(remove_multiple_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns names\n",
    "df_interventions_bxl.rename(columns={'intervention_time_t1reported': 'intervention_time_(t1reported)', \n",
    "                        'departure_time_t1reported': 'departure_time_(t1reported)',\n",
    "                        'calculated_distance_destination_': 'calculated_distance_destination'}, inplace=True)\n",
    "df_cad9.rename(columns={'Province invervention': 'province_intervention'}, inplace=True)\n",
    "#Drop columns\n",
    "df_interventions_bxl.drop(columns={'calculated_distance_departure_to', 'calculated_traveltime_departure_'}, inplace=True)\n",
    "df_interventions_bxl2.drop(columns={'creationtime', 'description_nl','ic_description_nl'}, inplace=True)\n",
    "df_cad9.drop(columns={'province', 'EventSubType Trip', 'CitysectionName intervention', 'UI', 'ID', 'MISSION_NR', 'AMBUCODE', 'UNIT_ID'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Cityname Intervention\n",
      "0                            schaerbeek (schaerbeek)\n",
      "1                            schaerbeek (schaerbeek)\n",
      "2                            koekelberg (koekelberg)\n",
      "3                            koekelberg (koekelberg)\n",
      "4                            schaerbeek (schaerbeek)\n",
      "...                                              ...\n",
      "38615  saint-josse-ten-noode (saint-josse-ten-noode)\n",
      "38616                        anderlecht (anderlecht)\n",
      "38617                                  uccle (uccle)\n",
      "38618                                  uccle (uccle)\n",
      "38619                          bruxelles (bruxelles)\n",
      "\n",
      "[36710 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove postal code from \"Cityname Intervention\"\n",
    "# Split the 'Cityname Intervention' column by space and keep only the second part\n",
    "df_interventions_bxl2['Cityname Intervention'] = df_interventions_bxl2['Cityname Intervention'].apply(lambda x: x.split(' ', 1)[1])\n",
    "\n",
    "# Print the modified DataFrame\n",
    "print(df_interventions_bxl2[['Cityname Intervention']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EventType Trip EventLevel Trip\n",
      "0           P033             N05\n",
      "1           P032             N05\n",
      "2           P010             N01\n",
      "3           P010             N01\n",
      "4           P039             N05\n"
     ]
    }
   ],
   "source": [
    "# Define a function to extract EventType and EventLevel\n",
    "def extract_event_info(event):\n",
    "    if isinstance(event, str):\n",
    "        parts = event.split()\n",
    "        event_type = parts[0]\n",
    "        event_level = parts[1] if len(parts) > 1 else None\n",
    "        return event_type, event_level\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Apply the function to create EventType and EventLevel columns\n",
    "df_interventions_bxl2[['EventType Trip', 'EventLevel Trip']] = df_interventions_bxl2['EventType and EventLevel'].apply(lambda x: pd.Series(extract_event_info(x)))\n",
    "\n",
    "# Drop the original \"EventType and EventLevel\" column if needed\n",
    "df_interventions_bxl2.drop(columns=['EventType and EventLevel'], inplace=True)\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(df_interventions_bxl2[['EventType Trip', 'EventLevel Trip']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EventType Trip\n",
      "0           P034\n",
      "1           P010\n",
      "2           Y_TI\n",
      "3           Y_TI\n",
      "4           P020\n"
     ]
    }
   ],
   "source": [
    "# Assuming the \"EventType Trip\" column contains strings\n",
    "df_cad9['EventType Trip'] = df_cad9['EventType Trip'].str.split(' ').str[0]\n",
    "\n",
    "# Print the first few rows to verify the changes\n",
    "print(df_cad9[['EventType Trip']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      EventType Trip\n",
      "0                                      P033 - Trauma\n",
      "1                          P032 - Allergic reactions\n",
      "2                        P010 - Respiratory problems\n",
      "3                        P010 - Respiratory problems\n",
      "4  P039 - Cardiac problem (other than thoracic pain)\n",
      "                EventType Trip\n",
      "0          P034 - Skull trauma\n",
      "1  P010 - Respiratory problems\n",
      "2                         Y_TI\n",
      "3                         Y_TI\n",
      "4  P020 - Intoxication alcohol\n"
     ]
    }
   ],
   "source": [
    "# Create function to add the description to the Event type code PXXX\n",
    "def update_event_type(dataset_to_update, dataset_with_reference):\n",
    "    # Iterate over unique values in \"EventType Trip\" of df_interventions_bxl2\n",
    "    for event_type in dataset_to_update['EventType Trip'].unique():\n",
    "        # Check if the event_type is not NaN\n",
    "        if isinstance(event_type, str):\n",
    "            # Iterate over all values in \"eventtype_trip\" of df_interventions_bxl\n",
    "            for event_trip in dataset_with_reference['eventtype_trip'].unique():\n",
    "                # Check if the event_trip is not NaN\n",
    "                if isinstance(event_trip, str):\n",
    "                    # Check if the event type from df_interventions_bxl2 is present in eventtype_trip from df_interventions_bxl\n",
    "                    if event_type in event_trip:\n",
    "                        # Update \"EventType Trip\" in df_interventions_bxl2 with the corresponding value from df_interventions_bxl                       \n",
    "                        dataset_to_update.loc[dataset_to_update['EventType Trip'] == event_type, 'EventType Trip'] = event_trip\n",
    "                        # Break the inner loop once a match is found for better performance\n",
    "                        break\n",
    "\n",
    "# Apply the function to df_interventions_bxl2 using df_interventions_bxl as a reference\n",
    "update_event_type(df_interventions_bxl2, df_interventions_bxl)\n",
    "\n",
    "# Apply the function to df_cad9 using df_interventions_bxl as a reference\n",
    "update_event_type(df_cad9, df_interventions_bxl)\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(df_interventions_bxl2[['EventType Trip']].head())\n",
    "print(df_cad9[['EventType Trip']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for standardizing the format\n",
    "format_mapping = {'N05': 'N5', 'N01': 'N1', 'N03': 'N3', 'N04': 'N4', 'N02': 'N2', 'N07': 'N7', 'N06': 'N6', 'N08': 'N8'}\n",
    "\n",
    "# Replace the values in the eventlevel_trip column using the mapping\n",
    "df_interventions_bxl2['EventLevel Trip'] = df_interventions_bxl2['EventLevel Trip'].replace(format_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regex pattern to match values not in the specified list\n",
    "pattern = r'^(?!(N[0-8]|Interventieplan)$).*$'\n",
    "\n",
    "# Replace values not in the specified list with NaN\n",
    "df_cad9['EventLevel Trip'] = df_cad9['EventLevel Trip'].replace(to_replace=pattern, value=np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets to clean\n",
    "datasets_to_clean = [df_interventions_bxl, df_interventions_bxl2]\n",
    "\n",
    "# Iterate over the datasets\n",
    "for dataset in datasets_to_clean:\n",
    "    # Remove columns containing 'FR' or 'fr' in their names\n",
    "    columns_to_remove = [col for col in dataset.columns if 'FR' in col or 'fr' in col]\n",
    "    dataset.drop(columns=columns_to_remove, inplace=True)\n",
    "    \n",
    "    # Replace ' NL' and ' nl' with empty strings in column names\n",
    "    dataset.columns = dataset.columns.str.replace(' NL', '').str.replace(' nl', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names\n",
    "for dataset in all_interventions:\n",
    "    dataset.columns = dataset.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        t0  \\\n",
      "0       2022-09-06 11:49:21.5868598 +02:00   \n",
      "1       2022-09-06 11:49:21.5868598 +02:00   \n",
      "2       2022-09-06 11:55:35.7936791 +02:00   \n",
      "3       2022-09-06 12:39:23.4337324 +02:00   \n",
      "4       2022-09-06 13:26:48.3379147 +02:00   \n",
      "...                                    ...   \n",
      "115642  2023-05-31 23:33:23.8187792 +02:00   \n",
      "115643  2023-05-31 23:33:23.8187792 +02:00   \n",
      "115644  2023-05-31 23:41:50.1818455 +02:00   \n",
      "115645  2023-05-31 23:41:50.1818455 +02:00   \n",
      "115646  2023-05-31 23:42:36.3935358 +02:00   \n",
      "\n",
      "                                        t1  \\\n",
      "0       2022-09-06 09:51:01.8972360 +00:00   \n",
      "1       2022-09-06 09:53:01.0232493 +00:00   \n",
      "2       2022-09-06 09:57:49.5158049 +00:00   \n",
      "3       2022-09-06 10:42:31.1121581 +00:00   \n",
      "4       2022-09-06 11:29:34.8646697 +00:00   \n",
      "...                                    ...   \n",
      "115642  2023-05-31 21:35:03.4270000 +00:00   \n",
      "115643  2023-05-31 21:35:03.5854187 +00:00   \n",
      "115644  2023-05-31 21:42:49.6908746 +00:00   \n",
      "115645  2023-05-31 21:42:49.9140000 +00:00   \n",
      "115646  2023-05-31 21:48:30.2205116 +00:00   \n",
      "\n",
      "                                        t2  \\\n",
      "0                                     None   \n",
      "1       2022-09-06 09:58:27.8729389 +00:00   \n",
      "2                                     None   \n",
      "3       2022-09-06 10:43:03.1332226 +00:00   \n",
      "4                                     None   \n",
      "...                                    ...   \n",
      "115642  2023-05-31 21:38:31.6307434 +00:00   \n",
      "115643                                None   \n",
      "115644                                None   \n",
      "115645  2023-05-31 21:46:01.6667078 +00:00   \n",
      "115646  2023-05-31 21:51:56.2253036 +00:00   \n",
      "\n",
      "                                        t3  \\\n",
      "0                                     None   \n",
      "1       2022-09-06 10:07:00.7842800 +00:00   \n",
      "2                                     None   \n",
      "3                                     None   \n",
      "4                                     None   \n",
      "...                                    ...   \n",
      "115642  2023-05-31 21:42:56.0215922 +00:00   \n",
      "115643                                None   \n",
      "115644                                None   \n",
      "115645  2023-05-31 21:47:42.0045704 +00:00   \n",
      "115646  2023-05-31 21:54:04.9165612 +00:00   \n",
      "\n",
      "                                        t4  \\\n",
      "0                                     None   \n",
      "1       2022-09-06 10:16:56.2028727 +00:00   \n",
      "2                                     None   \n",
      "3                                     None   \n",
      "4                                     None   \n",
      "...                                    ...   \n",
      "115642                                None   \n",
      "115643                                None   \n",
      "115644                                None   \n",
      "115645  2023-05-31 22:00:21.0906304 +00:00   \n",
      "115646  2023-05-31 21:59:38.9532987 +00:00   \n",
      "\n",
      "                                        t5  \\\n",
      "0                                     None   \n",
      "1       2022-09-06 10:36:50.5682394 +00:00   \n",
      "2                                     None   \n",
      "3                                     None   \n",
      "4                                     None   \n",
      "...                                    ...   \n",
      "115642                                None   \n",
      "115643                                None   \n",
      "115644                                None   \n",
      "115645  2023-05-31 22:06:24.8411904 +00:00   \n",
      "115646  2023-05-31 22:01:59.2422068 +00:00   \n",
      "\n",
      "                                        t6                                  t7  \n",
      "0                                     None  2022-09-06 09:52:43.7446626 +00:00  \n",
      "1                                     None  2022-09-06 10:41:59.4592739 +00:00  \n",
      "2                                     None  2022-09-06 09:58:47.1029350 +00:00  \n",
      "3       2022-09-06 10:56:26.8268314 +00:00  2022-09-06 11:02:19.5976460 +00:00  \n",
      "4                                     None  2022-09-06 11:30:23.3610743 +00:00  \n",
      "...                                    ...                                 ...  \n",
      "115642  2023-05-31 21:51:33.5137031 +00:00  2023-05-31 21:59:59.4578056 +00:00  \n",
      "115643                                None  2023-05-31 21:38:33.6776727 +00:00  \n",
      "115644                                None  2023-05-31 21:46:03.0417682 +00:00  \n",
      "115645  2023-05-31 22:24:05.7635091 +00:00  2023-05-31 22:28:22.9505087 +00:00  \n",
      "115646  2023-05-31 22:23:48.8097201 +00:00  2023-05-31 22:24:00.1852243 +00:00  \n",
      "\n",
      "[115645 rows x 8 columns]\n",
      "                     t0                t1                t2                t3  \\\n",
      "0      01JUN22:00:02:45  01JUN22:00:04:22  01JUN22:00:15:56  01JUN22:00:15:59   \n",
      "1      01JUN22:00:04:58  01JUN22:00:08:00  01JUN22:00:10:03  01JUN22:00:19:43   \n",
      "2      01JUN22:00:07:43              None              None              None   \n",
      "3      01JUN22:00:07:43              None              None              None   \n",
      "4      01JUN22:00:09:18  01JUN22:00:12:05  01JUN22:00:13:03  01JUN22:00:20:29   \n",
      "...                 ...               ...               ...               ...   \n",
      "38615  06SEP22:14:02:31  06SEP22:14:05:10  06SEP22:14:06:15  06SEP22:14:10:15   \n",
      "38616  06SEP22:14:11:22  06SEP22:14:12:08  06SEP22:14:12:09              None   \n",
      "38617  06SEP22:14:11:47              None              None              None   \n",
      "38618  06SEP22:14:11:47              None              None              None   \n",
      "38619  06SEP22:14:13:51  06SEP22:14:17:36  06SEP22:14:17:37  06SEP22:14:27:13   \n",
      "\n",
      "                     t4                t5                t6                t7  \n",
      "0                  None              None  01JUN22:00:38:06  01JUN22:00:43:54  \n",
      "1      01JUN22:00:19:49  01JUN22:00:24:25  01JUN22:01:59:18  01JUN22:01:59:19  \n",
      "2                  None              None              None              None  \n",
      "3                  None              None              None              None  \n",
      "4                  None              None  01JUN22:00:34:11  01JUN22:00:55:36  \n",
      "...                 ...               ...               ...               ...  \n",
      "38615  06SEP22:14:10:33  06SEP22:14:39:02  06SEP22:14:25:39  06SEP22:14:35:48  \n",
      "38616              None              None  06SEP22:15:32:06  06SEP22:15:36:50  \n",
      "38617              None              None              None              None  \n",
      "38618              None              None              None              None  \n",
      "38619  06SEP22:14:42:10  06SEP22:14:48:11  06SEP22:15:24:23  06SEP22:15:37:04  \n",
      "\n",
      "[36710 rows x 8 columns]\n",
      "                      t0                t1                       t2  \\\n",
      "0       01JUN22:00:01:34  01JUN22:00:03:26  2022-06-01 00:07:34.655   \n",
      "1       01JUN22:00:03:51  01JUN22:00:06:14  2022-06-01 00:10:00.017   \n",
      "2       01JUN22:00:03:51  01JUN22:00:22:18  2022-06-01 00:25:35.404   \n",
      "3       01JUN22:00:08:56  01JUN22:00:12:59  2022-06-01 00:15:36.856   \n",
      "4       01JUN22:00:10:38  01JUN22:00:15:00  2022-06-01 00:18:01.345   \n",
      "...                  ...               ...                      ...   \n",
      "200622  11JUL22:06:59:08  11JUL22:07:01:30  2022-07-11 07:02:55.511   \n",
      "200623  11JUL22:07:01:37  11JUL22:07:03:26  2022-07-11 07:04:28.501   \n",
      "200624  11JUL22:07:05:40  11JUL22:07:08:41  2022-07-11 07:14:04.569   \n",
      "200625  11JUL22:07:09:58  11JUL22:07:12:09  2022-07-11 07:15:56.741   \n",
      "200626  11JUL22:07:09:58  11JUL22:07:12:09  2022-07-11 07:15:58.751   \n",
      "\n",
      "                             t3                       t4  \\\n",
      "0       2022-06-01 00:17:53.888                     None   \n",
      "1       2022-06-01 00:16:25.356  2022-06-01 00:46:34.089   \n",
      "2       2022-06-01 00:35:58.397  2022-06-01 00:46:30.026   \n",
      "3       2022-06-01 00:27:35.760                     None   \n",
      "4       2022-06-01 00:25:28.053  2022-06-01 00:50:17.593   \n",
      "...                         ...                      ...   \n",
      "200622  2022-07-11 07:13:41.402  2022-07-11 07:29:08.581   \n",
      "200623  2022-07-11 07:09:30.712  2022-07-11 07:20:45.476   \n",
      "200624  2022-07-11 07:21:20.130  2022-07-11 07:43:08.643   \n",
      "200625  2022-07-11 07:23:44.727  2022-07-11 07:43:40.960   \n",
      "200626  2022-07-11 07:33:37.331                     None   \n",
      "\n",
      "                             t5                       t6  \\\n",
      "0                          None  2022-06-01 00:28:18.934   \n",
      "1       2022-06-01 00:56:48.871  2022-06-01 01:08:30.111   \n",
      "2       2022-06-01 01:06:50.953  2022-06-01 01:06:52.142   \n",
      "3                          None  2022-06-01 00:40:57.191   \n",
      "4       2022-06-01 00:59:50.136  2022-06-01 01:23:34.290   \n",
      "...                         ...                      ...   \n",
      "200622  2022-07-11 07:50:31.522  2022-07-11 08:12:03.972   \n",
      "200623  2022-07-11 07:31:31.721  2022-07-11 07:39:09.480   \n",
      "200624  2022-07-11 07:43:14.808  2022-07-11 08:06:08.703   \n",
      "200625  2022-07-11 07:58:36.802  2022-07-11 08:06:58.932   \n",
      "200626                     None  2022-07-11 07:40:38.960   \n",
      "\n",
      "                             t7  \n",
      "0       2022-06-01 00:53:17.876  \n",
      "1       2022-06-01 01:25:44.611  \n",
      "2       2022-06-01 01:20:48.471  \n",
      "3       2022-06-01 00:48:03.555  \n",
      "4                          None  \n",
      "...                         ...  \n",
      "200622  2022-07-11 08:29:01.763  \n",
      "200623  2022-07-11 08:02:15.653  \n",
      "200624  2022-07-11 08:13:09.447  \n",
      "200625  2022-07-11 08:32:28.872  \n",
      "200626  2022-07-11 07:54:38.570  \n",
      "\n",
      "[200627 rows x 8 columns]\n",
      "                      t0                t1                       t2  \\\n",
      "0       11JUL22:07:13:11  11JUL22:07:18:50  2022-07-11 07:22:08.119   \n",
      "1       11JUL22:07:14:34  11JUL22:07:15:53  2022-07-11 07:17:08.742   \n",
      "2       11JUL22:07:25:15  11JUL22:07:27:35  2022-07-11 07:48:20.678   \n",
      "3       11JUL22:07:45:26  11JUL22:07:47:50                     None   \n",
      "4       11JUL22:07:55:06  11JUL22:07:57:42  2022-07-11 07:59:16.532   \n",
      "...                  ...               ...                      ...   \n",
      "200622  05JAN23:01:04:14  05JAN23:01:05:16  2023-01-05 01:08:26.402   \n",
      "200623  05JAN23:01:03:40  05JAN23:01:06:33  2023-01-05 01:08:40.194   \n",
      "200624  05JAN23:01:03:40  05JAN23:01:06:33  2023-01-05 01:08:15.852   \n",
      "200625  05JAN23:01:12:59  05JAN23:01:14:45  2023-01-05 01:17:41.371   \n",
      "200626  05JAN23:01:14:14  05JAN23:01:15:24  2023-01-05 01:19:58.007   \n",
      "\n",
      "                             t3                       t4  \\\n",
      "0       2022-07-11 07:28:57.144  2022-07-11 07:45:22.173   \n",
      "1       2022-07-11 07:29:44.622  2022-07-11 07:41:28.712   \n",
      "2       2022-07-11 07:48:21.816  2022-07-11 07:52:23.829   \n",
      "3                          None                     None   \n",
      "4       2022-07-11 08:07:27.738  2022-07-11 08:37:04.903   \n",
      "...                         ...                      ...   \n",
      "200622  2023-01-05 01:12:39.404  2023-01-05 01:17:52.820   \n",
      "200623  2023-01-05 01:15:05.927  2023-01-05 01:39:50.156   \n",
      "200624  2023-01-05 01:15:05.783  2023-01-05 01:25:55.517   \n",
      "200625  2023-01-05 01:23:21.449  2023-01-05 01:38:40.676   \n",
      "200626                     None                     None   \n",
      "\n",
      "                             t5                       t6  \\\n",
      "0       2022-07-11 08:07:54.768  2022-07-11 08:36:55.435   \n",
      "1       2022-07-11 07:57:43.712  2022-07-11 08:13:43.882   \n",
      "2                          None                     None   \n",
      "3                          None                     None   \n",
      "4       2022-07-11 08:43:28.982  2022-07-11 08:57:17.163   \n",
      "...                         ...                      ...   \n",
      "200622  2023-01-05 01:20:03.729  2023-01-05 01:26:43.229   \n",
      "200623  2023-01-05 01:40:32.079  2023-01-05 02:16:19.238   \n",
      "200624  2023-01-05 01:39:24.322                     None   \n",
      "200625  2023-01-05 01:44:17.804  2023-01-05 01:55:18.746   \n",
      "200626                     None  2023-01-05 02:12:35.300   \n",
      "\n",
      "                             t7  \n",
      "0       2022-07-11 09:07:44.134  \n",
      "1       2022-07-11 08:16:46.852  \n",
      "2       2022-07-11 09:55:32.359  \n",
      "3       2022-07-11 17:51:31.334  \n",
      "4       2022-07-11 09:18:02.762  \n",
      "...                         ...  \n",
      "200622  2023-01-05 01:33:02.546  \n",
      "200623  2023-01-05 02:25:53.991  \n",
      "200624  2023-01-05 01:39:40.532  \n",
      "200625  2023-01-05 02:23:03.680  \n",
      "200626  2023-01-05 02:15:07.793  \n",
      "\n",
      "[200627 rows x 8 columns]\n",
      "                      t0                t1                       t2  \\\n",
      "0       05JAN23:01:18:27  05JAN23:01:20:35  2023-01-05 01:22:58.369   \n",
      "1       05JAN23:01:20:59  05JAN23:01:22:13  2023-01-05 01:23:02.798   \n",
      "2       05JAN23:01:34:07  05JAN23:01:36:05  2023-01-05 01:39:20.636   \n",
      "3       05JAN23:01:33:32  05JAN23:01:37:23  2023-01-05 01:41:30.735   \n",
      "4       05JAN23:01:42:33  05JAN23:01:44:33  2023-01-05 01:48:31.662   \n",
      "...                  ...               ...                      ...   \n",
      "200622  31MAY23:22:39:45  31MAY23:22:44:33                     None   \n",
      "200623  31MAY23:22:50:56  31MAY23:22:52:38                     None   \n",
      "200624  31MAY23:23:00:15  31MAY23:23:02:37                     None   \n",
      "200625  31MAY23:23:41:10  31MAY23:23:44:47                     None   \n",
      "200626  31MAY23:23:47:52  31MAY23:23:49:37                     None   \n",
      "\n",
      "                             t3                       t4  \\\n",
      "0       2023-01-05 01:26:06.798                     None   \n",
      "1       2023-01-05 01:28:53.947                     None   \n",
      "2       2023-01-05 01:44:38.018  2023-01-05 01:49:30.303   \n",
      "3       2023-01-05 01:47:22.982                     None   \n",
      "4       2023-01-05 01:55:18.586  2023-01-05 02:05:59.829   \n",
      "...                         ...                      ...   \n",
      "200622                     None                     None   \n",
      "200623                     None                     None   \n",
      "200624                     None                     None   \n",
      "200625                     None                     None   \n",
      "200626                     None                     None   \n",
      "\n",
      "                             t5                       t6  \\\n",
      "0                          None  2023-01-05 01:45:51.789   \n",
      "1                          None  2023-01-05 01:45:25.279   \n",
      "2       2023-01-05 01:54:36.206  2023-01-05 02:08:45.980   \n",
      "3                          None  2023-01-05 02:03:42.469   \n",
      "4       2023-01-05 02:10:11.140  2023-01-05 02:23:53.379   \n",
      "...                         ...                      ...   \n",
      "200622                     None                     None   \n",
      "200623                     None                     None   \n",
      "200624                     None                     None   \n",
      "200625                     None                     None   \n",
      "200626                     None                     None   \n",
      "\n",
      "                             t7  \n",
      "0       2023-01-05 01:49:10.253  \n",
      "1       2023-01-05 03:01:42.012  \n",
      "2       2023-01-05 02:26:12.779  \n",
      "3       2023-01-05 02:08:21.270  \n",
      "4       2023-01-05 02:32:04.849  \n",
      "...                         ...  \n",
      "200622                     None  \n",
      "200623                     None  \n",
      "200624                     None  \n",
      "200625                     None  \n",
      "200626                     None  \n",
      "\n",
      "[200627 rows x 8 columns]\n",
      "                             t0                t1                       t2  \\\n",
      "0       2022-06-01 00:12:50.000  01JUN22:00:14:32  2022-06-01 00:16:56.000   \n",
      "1       2022-06-01 00:11:02.000  01JUN22:00:15:09  2022-06-01 00:18:12.000   \n",
      "2       2022-07-14 16:54:37.000  01JUN22:01:07:46  2022-06-01 01:11:48.000   \n",
      "3       2022-07-14 16:54:37.000  01JUN22:01:10:19  2022-06-01 01:11:48.000   \n",
      "4       2022-06-01 01:14:59.000  01JUN22:01:17:29  2022-06-01 01:24:25.000   \n",
      "...                         ...               ...                      ...   \n",
      "289396                     None  06JUL22:18:40:51                     None   \n",
      "289397                     None  06JUL22:19:08:28                     None   \n",
      "289398                     None  06JUL22:19:20:02                     None   \n",
      "289399                     None  07JUL22:12:57:39                     None   \n",
      "289400                     None  16AUG22:10:22:18                     None   \n",
      "\n",
      "                             t3                       t4  \\\n",
      "0       2022-06-01 00:24:46.000  2022-06-01 00:35:52.000   \n",
      "1       2022-06-01 00:22:34.000  2022-06-01 00:35:33.000   \n",
      "2       2022-06-01 01:17:45.000  2022-06-01 01:47:37.000   \n",
      "3       2022-06-01 01:17:45.000  2022-06-01 01:47:37.000   \n",
      "4       2022-06-01 01:31:13.000  2022-06-01 01:44:03.000   \n",
      "...                         ...                      ...   \n",
      "289396  2022-07-06 18:40:52.000                     None   \n",
      "289397  2022-07-06 19:08:28.000                     None   \n",
      "289398  2022-07-06 19:20:02.000                     None   \n",
      "289399  2022-07-07 12:57:39.000                     None   \n",
      "289400  2022-08-16 10:22:18.000                     None   \n",
      "\n",
      "                             t5                       t6  \\\n",
      "0       2022-06-01 00:46:30.000  2022-06-01 00:57:15.000   \n",
      "1       2022-06-01 00:43:14.000  2022-06-01 00:49:39.000   \n",
      "2       2022-06-01 02:36:49.000  2022-06-01 02:36:54.000   \n",
      "3       2022-06-01 02:36:49.000  2022-06-01 02:36:54.000   \n",
      "4                          None                     None   \n",
      "...                         ...                      ...   \n",
      "289396                     None                     None   \n",
      "289397                     None                     None   \n",
      "289398                     None                     None   \n",
      "289399                     None                     None   \n",
      "289400                     None                     None   \n",
      "\n",
      "                             t7  \n",
      "0                          None  \n",
      "1                          None  \n",
      "2       2022-06-01 01:10:17.000  \n",
      "3       2022-06-01 01:10:17.000  \n",
      "4       2022-06-01 01:17:25.000  \n",
      "...                         ...  \n",
      "289396                     None  \n",
      "289397                     None  \n",
      "289398                     None  \n",
      "289399                     None  \n",
      "289400                     None  \n",
      "\n",
      "[289401 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Select columns\n",
    "columns_to_show = ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']\n",
    "selected_columns_1 = df_interventions_bxl[columns_to_show]\n",
    "selected_columns_2 = df_interventions_bxl2[columns_to_show]\n",
    "selected_columns_3 = df_interventions1[columns_to_show]\n",
    "selected_columns_4 = df_interventions2[columns_to_show]\n",
    "selected_columns_5 = df_interventions3[columns_to_show]\n",
    "selected_columns_6 = df_cad9[columns_to_show]\n",
    "\n",
    "# Show the selected columns\n",
    "print(selected_columns_1)\n",
    "print(selected_columns_2)\n",
    "print(selected_columns_3)\n",
    "print(selected_columns_4)\n",
    "print(selected_columns_5)\n",
    "print(selected_columns_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the correct date format strings\n",
    "date_format_1 = \"%d%b%y:%H:%M:%S\"\n",
    "date_format_2 = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "\n",
    "# Define datasets and their corresponding date columns\n",
    "datasets = {\n",
    "    'df_interventions_bxl': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "    'df_interventions_bxl2': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "    'df_interventions1': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "    'df_interventions2': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "    'df_interventions3': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "    'df_cad9': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']\n",
    "}\n",
    "\n",
    "# Apply the correct format string to each date column for each dataset\n",
    "for dataset_name, date_cols in datasets.items():\n",
    "    dataset = globals()[dataset_name]  # Get the dataset using its name\n",
    "    for column in date_cols:\n",
    "        try:\n",
    "            dataset[column] = pd.to_datetime(dataset[column], utc=True, format=date_format_2).dt.strftime(date_format_2)\n",
    "        except ValueError:\n",
    "            dataset[column] = pd.to_datetime(dataset[column], utc=True, format=date_format_1).dt.strftime(date_format_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         mission_id                          service_name  \\\n",
      "0       20222490011         FB PDS BRUX [PASI CitÈ] SIAMU   \n",
      "1       20222490011         FB PDS BRUX [PASI CitÈ] SIAMU   \n",
      "2       20222490012              HB UR BRUX CHU St Pierre   \n",
      "3       20222490015         FB PDS BRUX [PASI CitÈ] SIAMU   \n",
      "4       20222490019          FB PDS WOLL [PASI UCL] SIAMU   \n",
      "...             ...                                   ...   \n",
      "115642  20231510397         FB PDS BRUX [PASI CitÈ] SIAMU   \n",
      "115643  20231510397         FB PDS BRUX [PASI CitÈ] SIAMU   \n",
      "115644  20231510399   FB PDS ANDE [PASI Anderlecht] SIAMU   \n",
      "115645  20231510399   FB PDS ANDE [PASI Anderlecht] SIAMU   \n",
      "115646  20231510400  FB PDS BRUX [Caserne HÈliport] SIAMU   \n",
      "\n",
      "        postalcode_permanence                          cityname_permanence  \\\n",
      "0                        1000                            Brussel (Brussel)   \n",
      "1                        1000                            Brussel (Brussel)   \n",
      "2                        1000                            Brussel (Brussel)   \n",
      "3                        1000                            Brussel (Brussel)   \n",
      "4                        1200  Woluwe-Saint-Lambert (Woluwe-Saint-Lambert)   \n",
      "...                       ...                                          ...   \n",
      "115642                   1000                            Brussel (Brussel)   \n",
      "115643                   1000                            Brussel (Brussel)   \n",
      "115644                   1070                                   Anderlecht   \n",
      "115645                   1070                                   Anderlecht   \n",
      "115646                   1000                        Bruxelles (Bruxelles)   \n",
      "\n",
      "                      streetname_permanence housenumber_permanence  \\\n",
      "0                            Vesaliusstraat                   None   \n",
      "1                            Vesaliusstraat                   None   \n",
      "2                                 Rue Haute                   None   \n",
      "3                            Vesaliusstraat                   None   \n",
      "4       Avenue Hippocrate - Hippokrateslaan                   None   \n",
      "...                                     ...                    ...   \n",
      "115642                       Vesaliusstraat                   None   \n",
      "115643                       Vesaliusstraat                   None   \n",
      "115644                    Bergense Steenweg                   None   \n",
      "115645                    Bergense Steenweg                   None   \n",
      "115646                 Avenue de l'HÈliport                   None   \n",
      "\n",
      "        latitude_permanence  longitude_permanence permanence_short_name  \\\n",
      "0                   5085097                436411             ABBRUX11A   \n",
      "1                   5085097                436411             ABBRUX03A   \n",
      "2                 508343302              43454504             ABBRUX06A   \n",
      "3                   5085097                436411             ABBRUX03A   \n",
      "4                   5085211                 44604             ABWOLL01A   \n",
      "...                     ...                   ...                   ...   \n",
      "115642              5085097                436411             ABBRUX10A   \n",
      "115643              5085097                436411             ABBRUX11A   \n",
      "115644              5083254                431199             ABANDE01A   \n",
      "115645              5083254                431199             ABANDE04A   \n",
      "115646              5085946                435181             ABBRUX04A   \n",
      "\n",
      "       permanence_long_name  ... t0_Hour t0_Day t0_Month t0_Year t0_DayName  \\\n",
      "0                AMB CITE 2  ...       9      6        9    2022    Tuesday   \n",
      "1                AMB HELI 3  ...       9      6        9    2022    Tuesday   \n",
      "2                 AMB HSP 1  ...       9      6        9    2022    Tuesday   \n",
      "3                AMB HELI 3  ...      10      6        9    2022    Tuesday   \n",
      "4                 AMB UCL 1  ...      11      6        9    2022    Tuesday   \n",
      "...                     ...  ...     ...    ...      ...     ...        ...   \n",
      "115642           AMB CITE 1  ...      21     31        5    2023  Wednesday   \n",
      "115643           AMB CITE 2  ...      21     31        5    2023  Wednesday   \n",
      "115644            AMB AND 1  ...      21     31        5    2023  Wednesday   \n",
      "115645            AMB AND 4  ...      21     31        5    2023  Wednesday   \n",
      "115646           AMB HELI 4  ...      21     31        5    2023  Wednesday   \n",
      "\n",
      "        t7_Hour t7_Day  t7_Month  t7_Year t7_DayName  \n",
      "0           9.0    6.0       9.0   2022.0    Tuesday  \n",
      "1          10.0    6.0       9.0   2022.0    Tuesday  \n",
      "2           9.0    6.0       9.0   2022.0    Tuesday  \n",
      "3          11.0    6.0       9.0   2022.0    Tuesday  \n",
      "4          11.0    6.0       9.0   2022.0    Tuesday  \n",
      "...         ...    ...       ...      ...        ...  \n",
      "115642     21.0   31.0       5.0   2023.0  Wednesday  \n",
      "115643     21.0   31.0       5.0   2023.0  Wednesday  \n",
      "115644     21.0   31.0       5.0   2023.0  Wednesday  \n",
      "115645     22.0   31.0       5.0   2023.0  Wednesday  \n",
      "115646     22.0   31.0       5.0   2023.0  Wednesday  \n",
      "\n",
      "[115645 rows x 53 columns]\n",
      "        mission_id                          t0  \\\n",
      "0      20221520003  2022-06-01 00:02:45.000000   \n",
      "1      20221520005  2022-06-01 00:04:58.000000   \n",
      "2      20221520006  2022-06-01 00:07:43.000000   \n",
      "3      20221520006  2022-06-01 00:07:43.000000   \n",
      "4      20221520007  2022-06-01 00:09:18.000000   \n",
      "...            ...                         ...   \n",
      "38615  20222495211  2022-09-06 14:02:31.000000   \n",
      "38616  20222495213  2022-09-06 14:11:22.000000   \n",
      "38617  20222495214  2022-09-06 14:11:47.000000   \n",
      "38618  20222495214  2022-09-06 14:11:47.000000   \n",
      "38619  20222495215  2022-09-06 14:13:51.000000   \n",
      "\n",
      "                               cityname_intervention  longitude_intervention  \\\n",
      "0                            schaerbeek (schaerbeek)            4.406731e+10   \n",
      "1                            schaerbeek (schaerbeek)            4.368798e+10   \n",
      "2                            koekelberg (koekelberg)            4.332312e+10   \n",
      "3                            koekelberg (koekelberg)            4.332312e+10   \n",
      "4                            schaerbeek (schaerbeek)            4.383695e+10   \n",
      "...                                              ...                     ...   \n",
      "38615  saint-josse-ten-noode (saint-josse-ten-noode)            4.369210e+10   \n",
      "38616                        anderlecht (anderlecht)            4.302817e+10   \n",
      "38617                                  uccle (uccle)            4.339438e+10   \n",
      "38618                                  uccle (uccle)            4.339438e+10   \n",
      "38619                          bruxelles (bruxelles)            4.389640e+10   \n",
      "\n",
      "       latitude_intervention  number_of_transported_persons  \\\n",
      "0               5.085516e+10                            NaN   \n",
      "1               5.086318e+10                            1.0   \n",
      "2               5.085755e+10                            1.0   \n",
      "3               5.085755e+10                            NaN   \n",
      "4               5.085225e+10                            NaN   \n",
      "...                      ...                            ...   \n",
      "38615           5.085144e+10                            NaN   \n",
      "38616           5.083399e+10                            NaN   \n",
      "38617           5.081487e+10                            1.0   \n",
      "38618           5.081487e+10                            NaN   \n",
      "38619           5.084106e+10                            1.0   \n",
      "\n",
      "      permanence_long_name permanence_short_name        service_name  \\\n",
      "0              ZW STMICHEL            A-STMICHEL     ST-MICHEL (MED)   \n",
      "1                 ZW CITE2                A-CIT2          CITE (MED)   \n",
      "2                 ZW2 STAF                 A-EM2      Heliport (MED)   \n",
      "3                MUG HSP 1                 M-HSP           HSP (MED)   \n",
      "4                 ZW CITE1                A-CIT1          CITE (MED)   \n",
      "...                    ...                   ...                 ...   \n",
      "38615           MUG STJEAN                  M-SJ        STJEAN (MED)   \n",
      "38616              ZW AND4                A-AND4    Anderlecht (MED)   \n",
      "38617         ZW ELISABETH                 A-ELI  St-Elisabeth (MED)   \n",
      "38618            MUG HSP 1                 M-HSP           HSP (MED)   \n",
      "38619             ZW CITE1                A-CIT1          CITE (MED)   \n",
      "\n",
      "                cityname_permanence  ... t0_Hour  t0_Day  t0_Month  t0_Year  \\\n",
      "0        1040 etterbeek (etterbeek)  ...       0       1         6     2022   \n",
      "1            1000 brussel (brussel)  ...       0       1         6     2022   \n",
      "2            1000 brussel (brussel)  ...       0       1         6     2022   \n",
      "3        1000 bruxelles (bruxelles)  ...       0       1         6     2022   \n",
      "4            1000 brussel (brussel)  ...       0       1         6     2022   \n",
      "...                             ...  ...     ...     ...       ...      ...   \n",
      "38615        1000 brussel (brussel)  ...      14       6         9     2022   \n",
      "38616  1070 anderlecht (anderlecht)  ...      14       6         9     2022   \n",
      "38617            1180 ukkel (ukkel)  ...      14       6         9     2022   \n",
      "38618    1000 bruxelles (bruxelles)  ...      14       6         9     2022   \n",
      "38619        1000 brussel (brussel)  ...      14       6         9     2022   \n",
      "\n",
      "      t0_DayName t7_Hour t7_Day t7_Month t7_Year t7_DayName  \n",
      "0      Wednesday     0.0    1.0      6.0  2022.0  Wednesday  \n",
      "1      Wednesday     1.0    1.0      6.0  2022.0  Wednesday  \n",
      "2      Wednesday     NaN    NaN      NaN     NaN        NaN  \n",
      "3      Wednesday     NaN    NaN      NaN     NaN        NaN  \n",
      "4      Wednesday     0.0    1.0      6.0  2022.0  Wednesday  \n",
      "...          ...     ...    ...      ...     ...        ...  \n",
      "38615    Tuesday    14.0    6.0      9.0  2022.0    Tuesday  \n",
      "38616    Tuesday    15.0    6.0      9.0  2022.0    Tuesday  \n",
      "38617    Tuesday     NaN    NaN      NaN     NaN        NaN  \n",
      "38618    Tuesday     NaN    NaN      NaN     NaN        NaN  \n",
      "38619    Tuesday    15.0    6.0      9.0  2022.0    Tuesday  \n",
      "\n",
      "[36710 rows x 39 columns]\n",
      "         mission_id                 service_name  postalcode_permanence  \\\n",
      "0       10221520001     HA UR MECH AZ St Maarten                 2800.0   \n",
      "1       10221520002               BA KAPE AMBUCE                 2950.0   \n",
      "2       10221520002       HA UR ANTW Stuivenberg                 2060.0   \n",
      "3       10221520004  BA ANTW [Borgerhout] AMBUCE                 2140.0   \n",
      "4       10221520005               BA WIJN AMBUCE                 2110.0   \n",
      "...             ...                          ...                    ...   \n",
      "200622  50221920082      FH PDS BEAU Hainaut Est                 6500.0   \n",
      "200623  50221920083            BH LOUV AS Grande                 7110.0   \n",
      "200624  50221920084  BH GERP SAPG Poste Loverval                 6280.0   \n",
      "200625  50221920085   FH PDS BINC Hainaut Centre                 7130.0   \n",
      "200626  50221920085        HH UR LOUV CHU Tivoli                 7100.0   \n",
      "\n",
      "                        cityname_permanence       streetname_permanence  \\\n",
      "0                       Mechelen (Mechelen)              Liersesteenweg   \n",
      "1                       Kapellen (Kapellen)             Essenhoutstraat   \n",
      "2                     Antwerpen (Antwerpen)      Lange Beeldekensstraat   \n",
      "3                    Antwerpen (Borgerhout)               Gijselsstraat   \n",
      "4                       Wijnegem (Wijnegem)               Bijkhoevelaan   \n",
      "...                                     ...                         ...   \n",
      "200622                  Beaumont (Beaumont)           Chauss√©e de Mons   \n",
      "200623  La Louvi√®re (Str√©py-Bracquegnies)             Rue de Nivelles   \n",
      "200624                 Gerpinnes (Loverval)  Chauss√©e de Philippeville   \n",
      "200625                      Binche (Binche)       Rue de la P√©pini√®re   \n",
      "200626          La Louvi√®re (La Louvi√®re)            Avenue Max Buset   \n",
      "\n",
      "       housenumber_permanence  latitude_permanence  longitude_permanence  \\\n",
      "0                        None            51.051020              4.478030   \n",
      "1                        None            51.312075              4.424398   \n",
      "2                        None            51.222490              4.436290   \n",
      "3                        None            51.215620              4.443920   \n",
      "4                        None            51.233550              4.493180   \n",
      "...                       ...                  ...                   ...   \n",
      "200622                   None            50.241870              4.234680   \n",
      "200623                   None            50.469758              4.128008   \n",
      "200624                   None            50.373360              4.467450   \n",
      "200625                    57A            50.415230                   NaN   \n",
      "200626                   None            50.477800              4.205120   \n",
      "\n",
      "       permanence_short_name permanence_long_name  ... t0_Hour t0_Day  \\\n",
      "0                  AAMECH01A        ZW MECHELEN 1  ...       0      1   \n",
      "1                  AAKAPE01A        ZW KAPELLEN 1  ...       0      1   \n",
      "2                  UAANTW01A      MUG ANTWERPEN 1  ...       0      1   \n",
      "3                  AAANTW07A       ZW ANTWERPEN 7  ...       0      1   \n",
      "4                  AAWIJN01A        ZW WIJNEGEM 1  ...       0      1   \n",
      "...                      ...                  ...  ...     ...    ...   \n",
      "200622             AHBEAU01A        ZS BEAUMONT 1  ...       6     11   \n",
      "200623             AHLOUV03A         AMB STREPY 1  ...       7     11   \n",
      "200624             AHGERP01A       AMB LOVERVAL 1  ...       7     11   \n",
      "200625             AHBINC01A          ZS BINCHE 1  ...       7     11   \n",
      "200626             UHLOUV02A     SMUR LOUV TIVOLI  ...       7     11   \n",
      "\n",
      "       t0_Month t0_Year t0_DayName t7_Hour t7_Day  t7_Month  t7_Year  \\\n",
      "0             6    2022  Wednesday     0.0    1.0       6.0   2022.0   \n",
      "1             6    2022  Wednesday     1.0    1.0       6.0   2022.0   \n",
      "2             6    2022  Wednesday     1.0    1.0       6.0   2022.0   \n",
      "3             6    2022  Wednesday     0.0    1.0       6.0   2022.0   \n",
      "4             6    2022  Wednesday     NaN    NaN       NaN      NaN   \n",
      "...         ...     ...        ...     ...    ...       ...      ...   \n",
      "200622        7    2022     Monday     8.0   11.0       7.0   2022.0   \n",
      "200623        7    2022     Monday     8.0   11.0       7.0   2022.0   \n",
      "200624        7    2022     Monday     8.0   11.0       7.0   2022.0   \n",
      "200625        7    2022     Monday     8.0   11.0       7.0   2022.0   \n",
      "200626        7    2022     Monday     7.0   11.0       7.0   2022.0   \n",
      "\n",
      "       t7_DayName  \n",
      "0       Wednesday  \n",
      "1       Wednesday  \n",
      "2       Wednesday  \n",
      "3       Wednesday  \n",
      "4             NaN  \n",
      "...           ...  \n",
      "200622     Monday  \n",
      "200623     Monday  \n",
      "200624     Monday  \n",
      "200625     Monday  \n",
      "200626     Monday  \n",
      "\n",
      "[200627 rows x 56 columns]\n",
      "         mission_id                       service_name  postalcode_permanence  \\\n",
      "0       50221920087       FH PDS COMI Wallonie Picarde                 7784.0   \n",
      "1       50221920088       FH PDS TOUR Wallonie Picarde                 7500.0   \n",
      "2       50221920089         FH PDS LOUV Hainaut Centre                 7100.0   \n",
      "3       50221920090  FW HVP HEUV [Nieuwkerke] Westhoek                 8950.0   \n",
      "4       50221920092                BH SGHI Croix Rouge                 7331.0   \n",
      "...             ...                                ...                    ...   \n",
      "200622  60230050005                BG BLEG Croix Rouge                 4671.0   \n",
      "200623  60230050006                      BG AWAN PARAM                 4340.0   \n",
      "200624  60230050006           HG UR LIEG CHC Montlegia                 4000.0   \n",
      "200625  60230050007                BG LIEG Croix Rouge                 4000.0   \n",
      "200626  60230050008                BG OUPE Croix Rouge                 4680.0   \n",
      "\n",
      "                cityname_permanence              streetname_permanence  \\\n",
      "0       Comines-Warneton (Warneton)                  Chauss√©e d'Ypres   \n",
      "1                 Tournai (Tournai)                    Avenue de Maire   \n",
      "2       La Louvi√®re (La Louvi√®re)          Boulevard du Roi Baudouin   \n",
      "3           Heuvelland (Nieuwkerke)                    Dranouterstraat   \n",
      "4          Saint-Ghislain (Baudour)                     Rue Louis Caty   \n",
      "...                             ...                                ...   \n",
      "200622             Blegny (Barchon)                   Rue Pr√©s-Champs   \n",
      "200623         4342 awans (hognoul)                rue de la chauss√©e   \n",
      "200624               Li√®ge (Glain)  Boulevard de Patience et Beaujonc   \n",
      "200625              Li√®ge (Li√®ge)                        Rue Darchis   \n",
      "200626              Oupeye (Oupeye)                  Rue du Roi Albert   \n",
      "\n",
      "       housenumber_permanence  latitude_permanence  longitude_permanence  \\\n",
      "0                        None            50.760340              2.940230   \n",
      "1                        None            50.616520              3.375120   \n",
      "2                        None            50.468000              4.192170   \n",
      "3                        None            50.746210              2.822500   \n",
      "4                        None            50.469630              3.842559   \n",
      "...                       ...                  ...                   ...   \n",
      "200622                   None            50.662920              5.688700   \n",
      "200623                   None            50.680611              5.469060   \n",
      "200624                   None            50.646980              5.536010   \n",
      "200625                   None            50.637357              5.566551   \n",
      "200626                   None            50.702841              5.652668   \n",
      "\n",
      "       permanence_short_name permanence_long_name  ... t0_Hour t0_Day  \\\n",
      "0                  AHCOMI01A         ZS COMINES 1  ...       7     11   \n",
      "1                  AHTOUR01A         ZS TOURNAI 1  ...       7     11   \n",
      "2                  AHLOUV02A   ZS LA LOUVIERE PIM  ...       7     11   \n",
      "3                  AWHEUV01A        ZW NIEUWKERKE  ...       7     11   \n",
      "4                  AHSGHI01A  CR SAINT-GHISLAIN 1  ...       7     11   \n",
      "...                      ...                  ...  ...     ...    ...   \n",
      "200622             AGBLEG01A          CR.BLEGNY 1  ...       1      5   \n",
      "200623             AGAWAN01A           PR.PARAM 1  ...       1      5   \n",
      "200624             UGLIEG07A      SMUR MT LEGIA 1  ...       1      5   \n",
      "200625             AGLIEG04A           CR LIEGE 1  ...       1      5   \n",
      "200626             AGOUPE01A          CR.OUPEYE 1  ...       1      5   \n",
      "\n",
      "       t0_Month t0_Year t0_DayName t7_Hour t7_Day  t7_Month  t7_Year  \\\n",
      "0             7    2022     Monday     9.0   11.0       7.0   2022.0   \n",
      "1             7    2022     Monday     8.0   11.0       7.0   2022.0   \n",
      "2             7    2022     Monday     9.0   11.0       7.0   2022.0   \n",
      "3             7    2022     Monday    17.0   11.0       7.0   2022.0   \n",
      "4             7    2022     Monday     9.0   11.0       7.0   2022.0   \n",
      "...         ...     ...        ...     ...    ...       ...      ...   \n",
      "200622        1    2023   Thursday     1.0    5.0       1.0   2023.0   \n",
      "200623        1    2023   Thursday     2.0    5.0       1.0   2023.0   \n",
      "200624        1    2023   Thursday     1.0    5.0       1.0   2023.0   \n",
      "200625        1    2023   Thursday     2.0    5.0       1.0   2023.0   \n",
      "200626        1    2023   Thursday     2.0    5.0       1.0   2023.0   \n",
      "\n",
      "       t7_DayName  \n",
      "0          Monday  \n",
      "1          Monday  \n",
      "2          Monday  \n",
      "3          Monday  \n",
      "4          Monday  \n",
      "...           ...  \n",
      "200622   Thursday  \n",
      "200623   Thursday  \n",
      "200624   Thursday  \n",
      "200625   Thursday  \n",
      "200626   Thursday  \n",
      "\n",
      "[200627 rows x 56 columns]\n",
      "         mission_id                        service_name  \\\n",
      "0       60230050009                 BG LIEG Croix Rouge   \n",
      "1       60230050010  FG PDS PEPI Vesdre Ho√´gne Plateau   \n",
      "2       60230050012                    BG LIEG Courtois   \n",
      "3       60230050013             FG PDS ANS_ Li√®ge IILE   \n",
      "4       60230050014             FG PDS FLEM Li√®ge IILE   \n",
      "...             ...                                 ...   \n",
      "200622  90231510181                    FN PDS NAMU NAGE   \n",
      "200623  90231510182                 FN PDS DINA DINAPHI   \n",
      "200624  90231510183          FF PDS TUBI Brabant Wallon   \n",
      "200625  90231510184                    FN PDS NAMU NAGE   \n",
      "200626  90231510185                 BN NAMU Croix Rouge   \n",
      "\n",
      "        postalcode_permanence          cityname_permanence  \\\n",
      "0                      4000.0              Li√®ge (Li√®ge)   \n",
      "1                      4860.0        Pepinster (Pepinster)   \n",
      "2                      4030.0          Li√®ge (Grivegn√©e)   \n",
      "3                      4000.0               li√®ge (glain)   \n",
      "4                      4400.0  Fl√©malle (Fl√©malle-Haute)   \n",
      "...                       ...                          ...   \n",
      "200622                 5100.0               Namur (Jambes)   \n",
      "200623                 5500.0              Dinant (Dinant)   \n",
      "200624                 1480.0              Tubize (Tubize)   \n",
      "200625                 5100.0               Namur (Jambes)   \n",
      "200626                 5002.0        Namur (Saint-Servais)   \n",
      "\n",
      "        streetname_permanence housenumber_permanence  latitude_permanence  \\\n",
      "0                 Rue Darchis                   None            50.637357   \n",
      "1              Rue Jean Simon                   None            50.565940   \n",
      "2                 Rue Belvaux                   None            50.619828   \n",
      "3       p537;rue jean jaur√®s                   None            50.652240   \n",
      "4                 Grand'Route                   None            50.599360   \n",
      "...                       ...                    ...                  ...   \n",
      "200622      ChaussÈe de LiËge                  55/57                  NaN   \n",
      "200623   Rue de Philippeville                   None                  NaN   \n",
      "200624             Rue Ferrer                   None                  NaN   \n",
      "200625      ChaussÈe de LiËge                  55/57                  NaN   \n",
      "200626     Rue de l'Industrie                   None                  NaN   \n",
      "\n",
      "        longitude_permanence permanence_short_name permanence_long_name  ...  \\\n",
      "0                   5.566551             AGLIEG05A           CR LIEGE 2  ...   \n",
      "1                   5.802540             AGPEPI01A         ZS.PEPINST 1  ...   \n",
      "2                   5.602470             AGLIEG02A            PR.COURT2  ...   \n",
      "3                   5.526410             AGANS_01A             ZS ANS 1  ...   \n",
      "4                   5.472130             AGFLEM01A        ZS FLEMALLE 1  ...   \n",
      "...                      ...                   ...                  ...  ...   \n",
      "200622            488.008000             ANNAMU01A           ZS NAMUR 1  ...   \n",
      "200623            489.808000             ANDINA01A          ZS DINANT 1  ...   \n",
      "200624            419.797000             AFTUBI02A          ZS TUBIZE 2  ...   \n",
      "200625            488.008000             ANNAMU01A           ZS NAMUR 1  ...   \n",
      "200626            484.735000             ANNAMU04A           CR NAMUR 4  ...   \n",
      "\n",
      "       t0_Hour t0_Day t0_Month t0_Year t0_DayName t7_Hour t7_Day  t7_Month  \\\n",
      "0            1      5        1    2023   Thursday     1.0    5.0       1.0   \n",
      "1            1      5        1    2023   Thursday     3.0    5.0       1.0   \n",
      "2            1      5        1    2023   Thursday     2.0    5.0       1.0   \n",
      "3            1      5        1    2023   Thursday     2.0    5.0       1.0   \n",
      "4            1      5        1    2023   Thursday     2.0    5.0       1.0   \n",
      "...        ...    ...      ...     ...        ...     ...    ...       ...   \n",
      "200622      22     31        5    2023  Wednesday     NaN    NaN       NaN   \n",
      "200623      22     31        5    2023  Wednesday     NaN    NaN       NaN   \n",
      "200624      23     31        5    2023  Wednesday     NaN    NaN       NaN   \n",
      "200625      23     31        5    2023  Wednesday     NaN    NaN       NaN   \n",
      "200626      23     31        5    2023  Wednesday     NaN    NaN       NaN   \n",
      "\n",
      "        t7_Year t7_DayName  \n",
      "0        2023.0   Thursday  \n",
      "1        2023.0   Thursday  \n",
      "2        2023.0   Thursday  \n",
      "3        2023.0   Thursday  \n",
      "4        2023.0   Thursday  \n",
      "...         ...        ...  \n",
      "200622      NaN        NaN  \n",
      "200623      NaN        NaN  \n",
      "200624      NaN        NaN  \n",
      "200625      NaN        NaN  \n",
      "200626      NaN        NaN  \n",
      "\n",
      "[200627 rows x 56 columns]\n",
      "         mission_id                 service_name  latitude_permanence  \\\n",
      "0       21221520003             MV HVP VILV West            50.925277   \n",
      "1       21221520004             MV HVP HALL West            50.743200   \n",
      "2       21221520007             MV HVP VILV West            50.925277   \n",
      "3       21221520007   HV UR VILV AZ Jan Portaels            50.926869   \n",
      "4       21221520008  BB BRUX Hôpital Militair KA            50.905331   \n",
      "...             ...                          ...                  ...   \n",
      "289396  90221870001             BF SART BHN AMBU                  NaN   \n",
      "289397  90221870002   MF PDS BRAL Brabant Wallon            50.686351   \n",
      "289398  90221870003   MF PDS WAVR Brabant Wallon            50.698985   \n",
      "289399  90221880001             BF SART BHN AMBU                  NaN   \n",
      "289400  90222280001      MH PDS BEAU Hainaut Est                  NaN   \n",
      "\n",
      "        longitude_permanence permanence_short_name      permanence_long_name  \\\n",
      "0                   4.423057             AVVILV01A            ZW VILVOORDE 1   \n",
      "1                   4.241053             AVHALL02A                ZW HALLE 2   \n",
      "2                   4.423057             AVVILV01A            ZW VILVOORDE 1   \n",
      "3                   4.420968             UVVILV01A             MUG VILVOORDE   \n",
      "4                   4.387662             ABBRUX13A                AMB HMB 13   \n",
      "...                      ...                   ...                       ...   \n",
      "289396                   NaN             AFSART02A  AMB SART-DAME-AVELINES 2   \n",
      "289397              4.395892             AFBRAL03A     AMB BRAINE-L'ALLEUD 3   \n",
      "289398              4.615385             AFWAVR01A               AMB WAVRE 1   \n",
      "289399                   NaN             AFSART01A  AMB SART-DAME-AVELINES 1   \n",
      "289400                   NaN             AHBEAU01A            AMB BEAUMONT 1   \n",
      "\n",
      "       vector_type               eventtype_trip eventlevel_trip  \\\n",
      "0        Ambulance          P034 - Skull trauma              N5   \n",
      "1        Ambulance  P010 - Respiratory problems              N5   \n",
      "2        Ambulance                         Y_TI             NaN   \n",
      "3              MUG                         Y_TI             NaN   \n",
      "4        Ambulance  P020 - Intoxication alcohol              N5   \n",
      "...            ...                          ...             ...   \n",
      "289396   Ambulance                    PERSONNES             NaN   \n",
      "289397   Ambulance                    PERSONNES             NaN   \n",
      "289398   Ambulance                    PERSONNES             NaN   \n",
      "289399   Ambulance                    PERSONNES             NaN   \n",
      "289400   Ambulance                    PERSONNES             NaN   \n",
      "\n",
      "       cityname_intervention  ...  t0_Hour  t0_Day t0_Month t0_Year  \\\n",
      "0                   MACHELEN  ...      0.0     1.0      6.0  2022.0   \n",
      "1                    BEERSEL  ...      0.0     1.0      6.0  2022.0   \n",
      "2                  VILVOORDE  ...     16.0    14.0      7.0  2022.0   \n",
      "3                  VILVOORDE  ...     16.0    14.0      7.0  2022.0   \n",
      "4                   MACHELEN  ...      1.0     1.0      6.0  2022.0   \n",
      "...                      ...  ...      ...     ...      ...     ...   \n",
      "289396                  None  ...      NaN     NaN      NaN     NaN   \n",
      "289397                  None  ...      NaN     NaN      NaN     NaN   \n",
      "289398                  None  ...      NaN     NaN      NaN     NaN   \n",
      "289399             CHARLEROI  ...      NaN     NaN      NaN     NaN   \n",
      "289400                  None  ...      NaN     NaN      NaN     NaN   \n",
      "\n",
      "       t0_DayName t7_Hour t7_Day t7_Month t7_Year t7_DayName  \n",
      "0       Wednesday     NaN    NaN      NaN     NaN        NaN  \n",
      "1       Wednesday     NaN    NaN      NaN     NaN        NaN  \n",
      "2        Thursday     1.0    1.0      6.0  2022.0  Wednesday  \n",
      "3        Thursday     1.0    1.0      6.0  2022.0  Wednesday  \n",
      "4       Wednesday     1.0    1.0      6.0  2022.0  Wednesday  \n",
      "...           ...     ...    ...      ...     ...        ...  \n",
      "289396        NaN     NaN    NaN      NaN     NaN        NaN  \n",
      "289397        NaN     NaN    NaN      NaN     NaN        NaN  \n",
      "289398        NaN     NaN    NaN      NaN     NaN        NaN  \n",
      "289399        NaN     NaN    NaN      NaN     NaN        NaN  \n",
      "289400        NaN     NaN    NaN      NaN     NaN        NaN  \n",
      "\n",
      "[289401 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the desired date format\n",
    "date_format_2 = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "\n",
    "# Columns for which to add Hour, Day, Month, Name of the Day column\n",
    "date_columns_extra = ['t0', 't7']\n",
    "\n",
    "# Apply the function to each dataset and column\n",
    "for dataset in all_interventions:\n",
    "    for column in date_columns_extra:\n",
    "        dataset[column] = pd.to_datetime(dataset[column], utc=True)\n",
    "        dataset[f'{column}_Hour'] = dataset[column].dt.hour\n",
    "        dataset[f'{column}_Day'] = dataset[column].dt.day\n",
    "        dataset[f'{column}_Month'] = dataset[column].dt.month\n",
    "        dataset[f'{column}_Year'] = dataset[column].dt.year\n",
    "        dataset[f'{column}_DayName'] = dataset[column].dt.day_name()\n",
    "        # Convert datetime column to desired format\n",
    "        dataset[column] = dataset[column].dt.strftime(date_format_2)\n",
    "\n",
    "# Example usage:\n",
    "for dataset in all_interventions:\n",
    "    print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_interventions_bxl: Rows with consecutive timestamps in increasing order: 29161\n",
      "df_interventions_bxl: Rows with consecutive timestamps not in increasing order: 1666\n",
      "df_interventions_bxl2: Rows with consecutive timestamps in increasing order: 4627\n",
      "df_interventions_bxl2: Rows with consecutive timestamps not in increasing order: 2074\n",
      "df_interventions1: Rows with consecutive timestamps in increasing order: 70347\n",
      "df_interventions1: Rows with consecutive timestamps not in increasing order: 902\n",
      "df_interventions2: Rows with consecutive timestamps in increasing order: 71788\n",
      "df_interventions2: Rows with consecutive timestamps not in increasing order: 816\n",
      "df_interventions3: Rows with consecutive timestamps in increasing order: 73249\n",
      "df_interventions3: Rows with consecutive timestamps not in increasing order: 954\n",
      "df_cad9: Rows with consecutive timestamps in increasing order: 2\n",
      "df_cad9: Rows with consecutive timestamps not in increasing order: 41510\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each dataset\n",
    "for dataset_name, dataset in zip(['df_interventions_bxl', 'df_interventions_bxl2', 'df_interventions1',\n",
    "                                  'df_interventions2', 'df_interventions3', 'df_cad9'], all_interventions):\n",
    "    # Initialize counters\n",
    "    increasing_order_count = 0\n",
    "    not_increasing_order_count = 0\n",
    "    \n",
    "    # Iterate over each row to compare timestamps\n",
    "    for index, row in dataset.iterrows():\n",
    "        # Extract timestamps from the row\n",
    "        t0 = row.get('t0')\n",
    "        t1 = row.get('t1')\n",
    "        t2 = row.get('t2')\n",
    "        t3 = row.get('t3')\n",
    "        t4 = row.get('t4')\n",
    "        t5 = row.get('t5')\n",
    "        t6 = row.get('t6')\n",
    "        t7 = row.get('t7')\n",
    "        \n",
    "        # Check if any timestamps are missing\n",
    "        if any(map(pd.isna, [t0, t1, t2, t3, t4, t5, t6, t7])):\n",
    "            continue\n",
    "        \n",
    "        # Check if timestamps are in increasing order\n",
    "        if t0 < t1 < t2 < t3 < t4 < t5 < t6 < t7:\n",
    "            increasing_order_count += 1\n",
    "        else:\n",
    "            not_increasing_order_count += 1\n",
    "    \n",
    "    # Print the result for each dataset\n",
    "    print(f\"{dataset_name}: Rows with consecutive timestamps in increasing order: {increasing_order_count}\")\n",
    "    print(f\"{dataset_name}: Rows with consecutive timestamps not in increasing order: {not_increasing_order_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_interventions_bxl: Removed 4 values to restore consecutive timestamps\n",
      "df_interventions_bxl2: Removed 67 values to restore consecutive timestamps\n",
      "df_interventions1: Removed 21 values to restore consecutive timestamps\n",
      "df_interventions2: Removed 12 values to restore consecutive timestamps\n",
      "df_interventions3: Removed 19 values to restore consecutive timestamps\n",
      "df_cad9: Removed 167 values to restore consecutive timestamps\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each dataset\n",
    "for dataset_name, dataset in zip(['df_interventions_bxl', 'df_interventions_bxl2', 'df_interventions1',\n",
    "                                  'df_interventions2', 'df_interventions3', 'df_cad9'], all_interventions):\n",
    "    # Initialize counter for removed values\n",
    "    removed_values = 0\n",
    "    \n",
    "    # Flag to indicate whether any change has been made\n",
    "    change_made = True\n",
    "    \n",
    "    # Continue iterating until no further changes are made\n",
    "    while change_made:\n",
    "        change_made = False  # Reset change flag at the beginning of each iteration\n",
    "        \n",
    "        # Iterate over each row to compare timestamps\n",
    "        for index, row in dataset.iterrows():\n",
    "            # Extract timestamps from the row\n",
    "            t0 = row.get('t0')\n",
    "            t1 = row.get('t1')\n",
    "            t2 = row.get('t2')\n",
    "            t3 = row.get('t3')\n",
    "            t4 = row.get('t4')\n",
    "            t5 = row.get('t5')\n",
    "            t6 = row.get('t6')\n",
    "            t7 = row.get('t7')\n",
    "            \n",
    "            # Check if any timestamps are missing\n",
    "            if any(map(pd.isna, [t0, t1, t2, t3, t4, t5, t6, t7])):\n",
    "                continue\n",
    "            \n",
    "            # Check if timestamps are in increasing order\n",
    "            timestamps = [t1, t2, t3, t4, t5, t6, t7]\n",
    "            first_non_increasing = None\n",
    "            for i, t in enumerate(timestamps):\n",
    "                if t0 >= t:\n",
    "                    first_non_increasing = i + 1  # Add 1 because we start from t1\n",
    "                    break\n",
    "            \n",
    "            if first_non_increasing is not None:\n",
    "                # Remove the value causing the issue\n",
    "                dataset.at[index, f\"t{first_non_increasing}\"] = None\n",
    "                removed_values += 1\n",
    "                change_made = True  # Set flag to indicate change\n",
    "                \n",
    "    # Print the result for each dataset after removal\n",
    "    print(f\"{dataset_name}: Removed {removed_values} values to restore consecutive timestamps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_interventions_bxl: Intervention - Inside Belgium: 0, Outside Belgium: 115645, NaNs: 0\n",
      "df_interventions_bxl: Permanence - Inside Belgium: 0, Outside Belgium: 115645, NaNs: 0\n",
      "df_interventions_bxl2: Intervention - Inside Belgium: 0, Outside Belgium: 36700, NaNs: 10\n",
      "df_interventions_bxl2: Permanence - Inside Belgium: 0, Outside Belgium: 19301, NaNs: 17409\n",
      "df_interventions1: Intervention - Inside Belgium: 167165, Outside Belgium: 3124, NaNs: 30338\n",
      "df_interventions1: Permanence - Inside Belgium: 166719, Outside Belgium: 1845, NaNs: 32063\n",
      "df_interventions2: Intervention - Inside Belgium: 176437, Outside Belgium: 2721, NaNs: 21469\n",
      "df_interventions2: Permanence - Inside Belgium: 173871, Outside Belgium: 2935, NaNs: 23821\n",
      "df_interventions3: Intervention - Inside Belgium: 157142, Outside Belgium: 4330, NaNs: 39155\n",
      "df_interventions3: Permanence - Inside Belgium: 156968, Outside Belgium: 2055, NaNs: 41604\n",
      "df_cad9: Intervention - Inside Belgium: 289389, Outside Belgium: 11, NaNs: 1\n",
      "df_cad9: Permanence - Inside Belgium: 233726, Outside Belgium: 0, NaNs: 55675\n"
     ]
    }
   ],
   "source": [
    "# Define the boundaries of Belgium\n",
    "belgium_longitude_range = (2.5, 6.3)\n",
    "belgium_latitude_range = (49.5, 51.5)\n",
    "\n",
    "# Define a function to check if coordinates are within the Belgium boundaries\n",
    "def is_within_belgium(longitude, latitude):\n",
    "    return belgium_longitude_range[0] <= longitude <= belgium_longitude_range[1] and \\\n",
    "           belgium_latitude_range[0] <= latitude <= belgium_latitude_range[1]\n",
    "\n",
    "# Check longitude and latitude in each dataset\n",
    "for dataset_name, dataset in zip(['df_interventions_bxl', 'df_interventions_bxl2', 'df_interventions1',\n",
    "                                  'df_interventions2', 'df_interventions3', 'df_cad9'], all_interventions):\n",
    "    # Initialize counters for coordinates inside and outside Belgium for intervention and permanence\n",
    "    inside_intervention_count = 0\n",
    "    outside_intervention_count = 0\n",
    "    inside_permanence_count = 0\n",
    "    outside_permanence_count = 0\n",
    "    nan_count_intervention = 0\n",
    "    nan_count_permanence = 0\n",
    "    \n",
    "    # Check if longitude and latitude are within Belgium boundaries for intervention\n",
    "    for _, row in dataset.iterrows():\n",
    "        if pd.isna(row['longitude_intervention']) or pd.isna(row['latitude_intervention']):\n",
    "            nan_count_intervention += 1\n",
    "        elif is_within_belgium(row['longitude_intervention'], row['latitude_intervention']):\n",
    "            inside_intervention_count += 1\n",
    "        else:\n",
    "            outside_intervention_count += 1\n",
    "    \n",
    "    # Check if longitude and latitude are within Belgium boundaries for permanence\n",
    "    for _, row in dataset.iterrows():\n",
    "        if pd.isna(row['longitude_permanence']) or pd.isna(row['latitude_permanence']):\n",
    "            nan_count_permanence += 1\n",
    "        elif is_within_belgium(row['longitude_permanence'], row['latitude_permanence']):\n",
    "            inside_permanence_count += 1\n",
    "        else:\n",
    "            outside_permanence_count += 1\n",
    "    \n",
    "    # Print the result for each dataset\n",
    "    print(f\"{dataset_name}: Intervention - Inside Belgium: {inside_intervention_count}, Outside Belgium: {outside_intervention_count}, NaNs: {nan_count_intervention}\")\n",
    "    print(f\"{dataset_name}: Permanence - Inside Belgium: {inside_permanence_count}, Outside Belgium: {outside_permanence_count}, NaNs: {nan_count_permanence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check format\n",
    "#df_interventions_bxl.longitude_intervention\n",
    "#df_interventions_bxl.latitude_intervention\n",
    "#df_interventions_bxl.longitude_permanence\n",
    "#df_interventions_bxl.latitude_permanence\n",
    "#df_interventions_bxl2.longitude_intervention\n",
    "#df_interventions_bxl2.latitude_intervention\n",
    "#df_interventions_bxl2.longitude_permanence\n",
    "#df_interventions_bxl2.latitude_permanence\n",
    "#df_interventions3.longitude_intervention\n",
    "#df_interventions3.latitude_intervention\n",
    "#df_interventions3.longitude_permanence\n",
    "#df_interventions3.latitude_permanence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert longitude and latitude within the ranges of Belgium\n",
    "def loc_convert(l, max_l):\n",
    "    while l > max_l:\n",
    "        l /= 10\n",
    "    return l\n",
    "\n",
    "# Define the latitude and longitude ranges of Belgium\n",
    "belgium_longitude_range = (2.5, 6.3)\n",
    "belgium_latitude_range = (49.5, 51.5)\n",
    "\n",
    "# Adjust mislabelled lat/lon values and remove values outside the range for each DataFrame\n",
    "for dataset in all_interventions:\n",
    "    for col in ['longitude_intervention', 'latitude_intervention', 'longitude_permanence', 'latitude_permanence']:\n",
    "        dataset[col] = dataset[col].apply(loc_convert, args=(belgium_longitude_range[1],) if 'longitude' in col else (belgium_latitude_range[1],))\n",
    "        dataset[col] = dataset[col].where((dataset[col] >= belgium_longitude_range[0]) & (dataset[col] <= belgium_longitude_range[1]) if 'longitude' in col else (dataset[col] >= belgium_latitude_range[0]) & (dataset[col] <= belgium_latitude_range[1]), np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the country codes to be removed\n",
    "country_codes = ['NLD', 'DEU', 'FRA', 'LUX']\n",
    "\n",
    "# Iterate over each DataFrame in all_interventions\n",
    "for dataset in all_interventions:\n",
    "    # Create a boolean mask for rows where cityname_intervention starts with any of the country codes\n",
    "    mask = dataset['cityname_intervention'].str.startswith(tuple(country_codes), na=False)\n",
    "    # Remove the rows where the mask is True\n",
    "    dataset = dataset[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all datasets into one\n",
    "interventions_dataset = pd.concat(all_interventions, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numeric characters\n",
    "interventions_dataset['housenumber_permanence'] = interventions_dataset['housenumber_permanence'].str.extract('(\\d+)')\n",
    "interventions_dataset['postalcode_intervention'] = interventions_dataset['postalcode_intervention'].str.extract('(\\d+)')\n",
    "interventions_dataset['postalcode_destination_hospital'] = interventions_dataset['postalcode_destination_hospital'].str.extract('(\\d+)')\n",
    "interventions_dataset['housenumber_destination_hospital'] = interventions_dataset['housenumber_destination_hospital'].str.extract('(\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t6: 6 datasets\n",
      "t0_Month: 6 datasets\n",
      "eventlevel_trip: 6 datasets\n",
      "t7_Hour: 6 datasets\n",
      "latitude_permanence: 6 datasets\n",
      "t7_Day: 6 datasets\n",
      "cityname_intervention: 6 datasets\n",
      "t7_Year: 6 datasets\n",
      "t0_Day: 6 datasets\n",
      "t3: 6 datasets\n",
      "longitude_permanence: 6 datasets\n",
      "t7_DayName: 6 datasets\n",
      "t7: 6 datasets\n",
      "t0_Year: 6 datasets\n",
      "service_name: 6 datasets\n",
      "t4: 6 datasets\n",
      "t5: 6 datasets\n",
      "t2: 6 datasets\n",
      "t7_Month: 6 datasets\n",
      "vector_type: 6 datasets\n",
      "name_destination_hospital: 6 datasets\n",
      "permanence_long_name: 6 datasets\n",
      "permanence_short_name: 6 datasets\n",
      "mission_id: 6 datasets\n",
      "t0: 6 datasets\n",
      "t0_DayName: 6 datasets\n",
      "t1: 6 datasets\n",
      "eventtype_trip: 6 datasets\n",
      "longitude_intervention: 6 datasets\n",
      "t0_Hour: 6 datasets\n",
      "latitude_intervention: 6 datasets\n",
      "departure_time_(t1reported): 5 datasets\n",
      "intervention_time_(t1reported): 5 datasets\n",
      "housenumber_destination_hospital: 5 datasets\n",
      "number_of_transported_persons: 5 datasets\n",
      "streetname_permanence: 5 datasets\n",
      "cityname_destination_hospital: 5 datasets\n",
      "abandon_reason: 5 datasets\n",
      "streetname_destination_hospital: 5 datasets\n",
      "t1confirmed: 5 datasets\n",
      "cityname_permanence: 5 datasets\n",
      "housenumber_permanence: 5 datasets\n",
      "postalcode_intervention: 4 datasets\n",
      "intervention_time_(t1confirmed): 4 datasets\n",
      "postalcode_destination_hospital: 4 datasets\n",
      "waiting_time: 4 datasets\n",
      "unavailable_time: 4 datasets\n",
      "calculated_traveltime_destinatio: 4 datasets\n",
      "intervention_duration: 4 datasets\n",
      "eventtype_firstcall: 4 datasets\n",
      "calculated_distance_destination: 4 datasets\n",
      "postalcode_permanence: 4 datasets\n",
      "eventlevel_firstcall: 4 datasets\n",
      "departure_time_(t1confirmed): 4 datasets\n",
      "t9: 4 datasets\n",
      "province_intervention: 4 datasets\n"
     ]
    }
   ],
   "source": [
    "# Get unique column names across all datasets\n",
    "all_columns = set().union(*(dataset.columns for dataset in all_interventions))\n",
    "\n",
    "# Count occurrences of each column across datasets\n",
    "column_count = {column: sum(column in dataset.columns for dataset in all_interventions) for column in all_columns}\n",
    "\n",
    "# Print column counts\n",
    "for column, count in sorted(column_count.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{column}: {count} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data sets as a Parquet file\n",
    "df_interventions_bxl.to_parquet('../../1_Data/CLEANED/df_interventions_bxl.parquet')\n",
    "df_interventions_bxl2.to_parquet('../../1_Data/CLEANED/df_interventions_bxl2.parquet')\n",
    "df_interventions1.to_parquet('../../1_Data/CLEANED/df_interventions1.parquet')\n",
    "df_interventions2.to_parquet('../../1_Data/CLEANED/df_interventions2.parquet')\n",
    "df_interventions3.to_parquet('../../1_Data/CLEANED/df_interventions3.parquet')\n",
    "df_cad9.to_parquet('../../1_Data/CLEANED/df_cad9.parquet')\n",
    "interventions_dataset.to_parquet('../../1_Data/CLEANED/interventions_dataset.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter subset on eventtype_trip = P003 (Cardiac arrest)\n",
    "subset_cardiac = interventions_dataset[interventions_dataset['eventtype_trip'].isin(['P003 - Cardiac arrest'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save subset\n",
    "subset_cardiac.to_parquet('../../1_Data/CLEANED/subset_cardiac.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
