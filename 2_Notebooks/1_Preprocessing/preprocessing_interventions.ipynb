{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook with the preprocessing steps of the interventions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 1: LOAD DATA FROM FOLDER -> see load_data.ipynb for other methods\n",
    "# Path to your Parquet gzip file\n",
    "file_path_1 = '../../1_Data/Medical_transport/ambulance_locations.parquet.gzip'\n",
    "file_path_2 = '../../1_Data/Medical_transport/pit_locations.parquet.gzip'\n",
    "file_path_3 = '../../1_Data/Medical_transport/mug_locations.parquet.gzip'\n",
    "file_path_4 = '../../1_Data/AED_locations/aed_locations.parquet.gzip'\n",
    "file_path_5 = '../../1_Data/Interventions_data/interventions_bxl.parquet.gzip'\n",
    "file_path_6 = '../../1_Data/Interventions_data/interventions_bxl2.parquet.gzip'\n",
    "file_path_7 = '../../1_Data/Interventions_data/interventions1.parquet.gzip'\n",
    "file_path_8 = '../../1_Data/Interventions_data/interventions2.parquet.gzip'\n",
    "file_path_9 = '../../1_Data/Interventions_data/interventions3.parquet.gzip'\n",
    "file_path_10 = '../../1_Data/Interventions_data/cad9.parquet.gzip'\n",
    "\n",
    "# Read the Parquet file into a pandas DataFrame\n",
    "df_ambulance_locations = pd.read_parquet(file_path_1, engine='pyarrow')\n",
    "df_pit_locations = pd.read_parquet(file_path_2, engine='pyarrow')\n",
    "df_mug_locations = pd.read_parquet(file_path_3, engine='pyarrow')\n",
    "df_aed_locations = pd.read_parquet(file_path_4, engine='pyarrow')\n",
    "df_interventions_bxl = pd.read_parquet(file_path_5, engine='pyarrow')\n",
    "df_interventions_bxl2 = pd.read_parquet(file_path_6, engine='pyarrow')\n",
    "df_interventions1 = pd.read_parquet(file_path_7, engine='pyarrow')\n",
    "df_interventions2 = pd.read_parquet(file_path_8, engine='pyarrow')\n",
    "df_interventions3 = pd.read_parquet(file_path_9, engine='pyarrow')\n",
    "df_cad9 = pd.read_parquet(file_path_10, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCATIONS: Print data set + data type of each variable + values of specific columns\n",
    "\n",
    "#print(df_ambulance_locations)\n",
    "#print(df_ambulance_locations.dtypes)\n",
    "#print(df_ambulance_locations.departure_location)\n",
    "\n",
    "#print(df_pit_locations)\n",
    "#print(df_pit_locations.dtypes)\n",
    "#print(df_pit_locations['campus'].unique())\n",
    "\n",
    "#print(df_mug_locations)\n",
    "#print(df_mug_locations.dtypes)\n",
    "#print(df_mug_locations.name_hospital)\n",
    "#print(df_mug_locations.address_campus)\n",
    "\n",
    "#print(df_aed_locations)\n",
    "#print(df_aed_locations.dtypes)\n",
    "#print(df_aed_locations['address'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERVENTIONS: Print data set + data type of each variable + values of specific columns\n",
    "\n",
    "#print(df_interventions_bxl)\n",
    "#print(df_interventions_bxl.dtypes)\n",
    "#print(df_interventions_bxl.postalcode_permanence)\n",
    "#print(df_interventions_bxl.t0)\n",
    "\n",
    "#print(df_interventions_bxl2)\n",
    "#print(df_interventions_bxl2.dtypes)\n",
    "#print(df_interventions_bxl2['EventType and EventLevel'].unique())\n",
    "#print(df_interventions_bxl2['EventType'].unique())\n",
    "#print(df_interventions_bxl2['eventlevel_trip'].unique())\n",
    "#print(df_interventions_bxl2.T0)\n",
    "\n",
    "#print(df_interventions1)\n",
    "#print(df_interventions1.dtypes)\n",
    "#print(df_interventions1.T0)\n",
    "\n",
    "#print(df_interventions2)\n",
    "#print(df_interventions2.dtypes)\n",
    "#print(df_interventions2.T0)\n",
    "\n",
    "#print(df_interventions3)\n",
    "#print(df_interventions3.dtypes)\n",
    "#print(df_interventions3.T0)\n",
    "\n",
    "#print(df_combined)\n",
    "#print(df_combined.dtypes)\n",
    "#print(df_combined.T0)\n",
    "#print(combined_df['Vector type'].unique())\n",
    "#print(combined_df['EventType Firstcall'].unique())\n",
    "#print(combined_df['EventLevel Firstcall'].unique())\n",
    "#print(combined_df['Abandon reason'].unique())\n",
    "\n",
    "#print(df_cad9)\n",
    "#print(df_cad9.dtypes)\n",
    "#print(df_cad9.T0)\n",
    "\n",
    "#print(df_interventions1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset\n",
    "all_datasets = [df_ambulance_locations, df_pit_locations, df_mug_locations, df_aed_locations, df_interventions_bxl, df_interventions_bxl2, df_interventions1, df_interventions2, df_interventions3, df_cad9]\n",
    "all_interventions = [df_interventions_bxl, df_interventions_bxl2, df_interventions1, df_interventions2, df_interventions3, df_cad9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check non-null counts of each dataset\n",
    "#for df in all_datasets: df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size of Dataset 1: 279\n",
      "Size of Dataset 1 after removing duplicates: 279\n",
      "\n",
      "Original size of Dataset 2: 24\n",
      "Size of Dataset 2 after removing duplicates: 24\n",
      "\n",
      "Original size of Dataset 3: 94\n",
      "Size of Dataset 3 after removing duplicates: 94\n",
      "\n",
      "Original size of Dataset 4: 15227\n",
      "Size of Dataset 4 after removing duplicates: 15225\n",
      "\n",
      "Original size of Dataset 5: 115647\n",
      "Size of Dataset 5 after removing duplicates: 115645\n",
      "\n",
      "Original size of Dataset 6: 38620\n",
      "Size of Dataset 6 after removing duplicates: 36710\n",
      "\n",
      "Original size of Dataset 7: 200627\n",
      "Size of Dataset 7 after removing duplicates: 200627\n",
      "\n",
      "Original size of Dataset 8: 200627\n",
      "Size of Dataset 8 after removing duplicates: 200627\n",
      "\n",
      "Original size of Dataset 9: 200627\n",
      "Size of Dataset 9 after removing duplicates: 200627\n",
      "\n",
      "Original size of Dataset 10: 289401\n",
      "Size of Dataset 10 after removing duplicates: 289401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each dataset and remove duplicate rows\n",
    "for i, dataset in enumerate(all_datasets, start=1):\n",
    "    print(f\"Original size of Dataset {i}: {len(dataset)}\")\n",
    "    dataset.drop_duplicates(inplace=True)\n",
    "    print(f\"Size of Dataset {i} after removing duplicates: {len(dataset)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove multiple spaces\n",
    "def remove_multiple_spaces(event):\n",
    "    if isinstance(event, str):\n",
    "        return ' '.join(event.split())\n",
    "    else:\n",
    "        return event\n",
    "\n",
    "# Apply the function to all columns of each dataset\n",
    "for idx, dataset in enumerate(all_datasets):\n",
    "    for column in dataset.columns:\n",
    "        dataset[column] = dataset[column].apply(remove_multiple_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names\n",
    "for dataset in all_interventions:\n",
    "    dataset.columns = dataset.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets to clean\n",
    "datasets_to_clean = [df_interventions_bxl, df_interventions_bxl2]\n",
    "\n",
    "# Iterate over the datasets\n",
    "for dataset in datasets_to_clean:\n",
    "    # Remove columns containing 'fr' in their names\n",
    "    columns_to_remove = [col for col in dataset.columns if 'fr' in col]\n",
    "    dataset.drop(columns=columns_to_remove, inplace=True)\n",
    "    \n",
    "    # Replace 'nl' with empty strings in column names\n",
    "    dataset.columns = dataset.columns.str.replace('_nl', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns names\n",
    "df_interventions_bxl.rename(columns={'intervention_time_t1reported': 'intervention_time_(t1reported)', \n",
    "                        'departure_time_t1reported': 'departure_time_(t1reported)',\n",
    "                        'calculated_distance_destination_': 'calculated_distance_destination'}, inplace=True)\n",
    "df_cad9.rename(columns={'province_invervention': 'province_intervention'}, inplace=True)\n",
    "#Drop columns\n",
    "df_interventions_bxl.drop(columns={'calculated_distance_departure_to', 'calculated_traveltime_departure_'}, inplace=True)\n",
    "df_interventions_bxl2.drop(columns={'creationtime', 'description','ic_description'}, inplace=True)\n",
    "df_cad9.drop(columns={'province', 'eventsubtype_trip', 'citysectionname_intervention', 'ui', 'id', 'mission_nr', 'ambucode', 'unit_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               cityname_intervention\n",
      "0                            schaerbeek (schaerbeek)\n",
      "1                            schaerbeek (schaerbeek)\n",
      "2                            koekelberg (koekelberg)\n",
      "3                            koekelberg (koekelberg)\n",
      "4                            schaerbeek (schaerbeek)\n",
      "...                                              ...\n",
      "38615  saint-josse-ten-noode (saint-josse-ten-noode)\n",
      "38616                        anderlecht (anderlecht)\n",
      "38617                                  uccle (uccle)\n",
      "38618                                  uccle (uccle)\n",
      "38619                          bruxelles (bruxelles)\n",
      "\n",
      "[36710 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove postal code from \"Cityname Intervention\"\n",
    "# Split the 'Cityname Intervention' column by space and keep only the second part\n",
    "df_interventions_bxl2['cityname_intervention'] = df_interventions_bxl2['cityname_intervention'].apply(lambda x: x.split(' ', 1)[1])\n",
    "\n",
    "# Print the modified DataFrame\n",
    "print(df_interventions_bxl2[['cityname_intervention']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eventtype_trip eventlevel_trip\n",
      "0           P033             N05\n",
      "1           P032             N05\n",
      "2           P010             N01\n",
      "3           P010             N01\n",
      "4           P039             N05\n"
     ]
    }
   ],
   "source": [
    "# Define a function to extract EventType and EventLevel\n",
    "def extract_event_info(event):\n",
    "    if isinstance(event, str):\n",
    "        parts = event.split()\n",
    "        event_type = parts[0]\n",
    "        event_level = parts[1] if len(parts) > 1 else None\n",
    "        return event_type, event_level\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Apply the function to create EventType and EventLevel columns\n",
    "df_interventions_bxl2[['eventtype_trip', 'eventlevel_trip']] = df_interventions_bxl2['eventtype_and_eventlevel'].apply(lambda x: pd.Series(extract_event_info(x)))\n",
    "\n",
    "# Drop the original \"EventType and EventLevel\" column if needed\n",
    "df_interventions_bxl2.drop(columns=['eventtype_and_eventlevel'], inplace=True)\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(df_interventions_bxl2[['eventtype_trip', 'eventlevel_trip']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eventtype_trip\n",
      "0           P034\n",
      "1           P010\n",
      "2           Y_TI\n",
      "3           Y_TI\n",
      "4           P020\n"
     ]
    }
   ],
   "source": [
    "# Assuming the \"EventType Trip\" column contains strings\n",
    "df_cad9['eventtype_trip'] = df_cad9['eventtype_trip'].str.split(' ').str[0]\n",
    "\n",
    "# Print the first few rows to verify the changes\n",
    "print(df_cad9[['eventtype_trip']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      eventtype_trip\n",
      "0                                      P033 - Trauma\n",
      "1                          P032 - Allergic reactions\n",
      "2                        P010 - Respiratory problems\n",
      "3                        P010 - Respiratory problems\n",
      "4  P039 - Cardiac problem (other than thoracic pain)\n",
      "                eventtype_trip\n",
      "0          P034 - Skull trauma\n",
      "1  P010 - Respiratory problems\n",
      "2                         Y_TI\n",
      "3                         Y_TI\n",
      "4  P020 - Intoxication alcohol\n"
     ]
    }
   ],
   "source": [
    "def update_event_type(dataset_to_update, reference_datasets):\n",
    "    # Create a dictionary to map the original eventtype_trip values to the new ones\n",
    "    eventtype_mapping = {}\n",
    "    for ref_dataset in reference_datasets:\n",
    "        for event_trip in ref_dataset['eventtype_trip'].dropna().unique():\n",
    "            key = event_trip.split(' - ')[0]\n",
    "            eventtype_mapping[key] = event_trip\n",
    "    \n",
    "    # Update the eventtype_trip values in dataset_to_update\n",
    "    dataset_to_update.loc[:, 'eventtype_trip'] = dataset_to_update['eventtype_trip'].apply(lambda x: eventtype_mapping.get(x, x))\n",
    "\n",
    "# Create a list of reference datasets in the desired order\n",
    "reference_datasets = [df_interventions_bxl, df_interventions1, df_interventions2, df_interventions3]\n",
    "\n",
    "# Apply the function to df_interventions_bxl2 using the list of reference datasets\n",
    "update_event_type(df_interventions_bxl2, reference_datasets)\n",
    "\n",
    "# Apply the function to df_cad9 using the list of reference datasets\n",
    "update_event_type(df_cad9, reference_datasets)\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(df_interventions_bxl2[['eventtype_trip']].head())\n",
    "print(df_cad9[['eventtype_trip']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show values\n",
    "#print(df_interventions_bxl2['eventtype_trip'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for standardizing the format\n",
    "format_mapping = {'N05': 'N5', 'N01': 'N1', 'N03': 'N3', 'N04': 'N4', 'N02': 'N2', 'N07': 'N7', 'N06': 'N6', 'N08': 'N8'}\n",
    "\n",
    "# Replace the values in the eventlevel_trip column using the mapping\n",
    "df_interventions_bxl2['eventlevel_trip'] = df_interventions_bxl2['eventlevel_trip'].replace(format_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regex pattern to match values not in the specified list\n",
    "#pattern = r'^(?!(N[0-8]|Interventieplan)$).*$'\n",
    "\n",
    "# Replace values not in the specified list with NaN\n",
    "#df_cad9['eventlevel_trip'] = df_cad9['eventlevel_trip'].replace(to_replace=pattern, value=np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        t0  \\\n",
      "0       2022-09-06 11:49:21.5868598 +02:00   \n",
      "1       2022-09-06 11:49:21.5868598 +02:00   \n",
      "2       2022-09-06 11:55:35.7936791 +02:00   \n",
      "3       2022-09-06 12:39:23.4337324 +02:00   \n",
      "4       2022-09-06 13:26:48.3379147 +02:00   \n",
      "...                                    ...   \n",
      "115642  2023-05-31 23:33:23.8187792 +02:00   \n",
      "115643  2023-05-31 23:33:23.8187792 +02:00   \n",
      "115644  2023-05-31 23:41:50.1818455 +02:00   \n",
      "115645  2023-05-31 23:41:50.1818455 +02:00   \n",
      "115646  2023-05-31 23:42:36.3935358 +02:00   \n",
      "\n",
      "                                        t1  \\\n",
      "0       2022-09-06 09:51:01.8972360 +00:00   \n",
      "1       2022-09-06 09:53:01.0232493 +00:00   \n",
      "2       2022-09-06 09:57:49.5158049 +00:00   \n",
      "3       2022-09-06 10:42:31.1121581 +00:00   \n",
      "4       2022-09-06 11:29:34.8646697 +00:00   \n",
      "...                                    ...   \n",
      "115642  2023-05-31 21:35:03.4270000 +00:00   \n",
      "115643  2023-05-31 21:35:03.5854187 +00:00   \n",
      "115644  2023-05-31 21:42:49.6908746 +00:00   \n",
      "115645  2023-05-31 21:42:49.9140000 +00:00   \n",
      "115646  2023-05-31 21:48:30.2205116 +00:00   \n",
      "\n",
      "                                        t2  \\\n",
      "0                                     None   \n",
      "1       2022-09-06 09:58:27.8729389 +00:00   \n",
      "2                                     None   \n",
      "3       2022-09-06 10:43:03.1332226 +00:00   \n",
      "4                                     None   \n",
      "...                                    ...   \n",
      "115642  2023-05-31 21:38:31.6307434 +00:00   \n",
      "115643                                None   \n",
      "115644                                None   \n",
      "115645  2023-05-31 21:46:01.6667078 +00:00   \n",
      "115646  2023-05-31 21:51:56.2253036 +00:00   \n",
      "\n",
      "                                        t3  \\\n",
      "0                                     None   \n",
      "1       2022-09-06 10:07:00.7842800 +00:00   \n",
      "2                                     None   \n",
      "3                                     None   \n",
      "4                                     None   \n",
      "...                                    ...   \n",
      "115642  2023-05-31 21:42:56.0215922 +00:00   \n",
      "115643                                None   \n",
      "115644                                None   \n",
      "115645  2023-05-31 21:47:42.0045704 +00:00   \n",
      "115646  2023-05-31 21:54:04.9165612 +00:00   \n",
      "\n",
      "                                        t4  \\\n",
      "0                                     None   \n",
      "1       2022-09-06 10:16:56.2028727 +00:00   \n",
      "2                                     None   \n",
      "3                                     None   \n",
      "4                                     None   \n",
      "...                                    ...   \n",
      "115642                                None   \n",
      "115643                                None   \n",
      "115644                                None   \n",
      "115645  2023-05-31 22:00:21.0906304 +00:00   \n",
      "115646  2023-05-31 21:59:38.9532987 +00:00   \n",
      "\n",
      "                                        t5  \\\n",
      "0                                     None   \n",
      "1       2022-09-06 10:36:50.5682394 +00:00   \n",
      "2                                     None   \n",
      "3                                     None   \n",
      "4                                     None   \n",
      "...                                    ...   \n",
      "115642                                None   \n",
      "115643                                None   \n",
      "115644                                None   \n",
      "115645  2023-05-31 22:06:24.8411904 +00:00   \n",
      "115646  2023-05-31 22:01:59.2422068 +00:00   \n",
      "\n",
      "                                        t6                                  t7  \n",
      "0                                     None  2022-09-06 09:52:43.7446626 +00:00  \n",
      "1                                     None  2022-09-06 10:41:59.4592739 +00:00  \n",
      "2                                     None  2022-09-06 09:58:47.1029350 +00:00  \n",
      "3       2022-09-06 10:56:26.8268314 +00:00  2022-09-06 11:02:19.5976460 +00:00  \n",
      "4                                     None  2022-09-06 11:30:23.3610743 +00:00  \n",
      "...                                    ...                                 ...  \n",
      "115642  2023-05-31 21:51:33.5137031 +00:00  2023-05-31 21:59:59.4578056 +00:00  \n",
      "115643                                None  2023-05-31 21:38:33.6776727 +00:00  \n",
      "115644                                None  2023-05-31 21:46:03.0417682 +00:00  \n",
      "115645  2023-05-31 22:24:05.7635091 +00:00  2023-05-31 22:28:22.9505087 +00:00  \n",
      "115646  2023-05-31 22:23:48.8097201 +00:00  2023-05-31 22:24:00.1852243 +00:00  \n",
      "\n",
      "[115645 rows x 8 columns]\n",
      "                     t0                t1                t2                t3  \\\n",
      "0      01JUN22:00:02:45  01JUN22:00:04:22  01JUN22:00:15:56  01JUN22:00:15:59   \n",
      "1      01JUN22:00:04:58  01JUN22:00:08:00  01JUN22:00:10:03  01JUN22:00:19:43   \n",
      "2      01JUN22:00:07:43              None              None              None   \n",
      "3      01JUN22:00:07:43              None              None              None   \n",
      "4      01JUN22:00:09:18  01JUN22:00:12:05  01JUN22:00:13:03  01JUN22:00:20:29   \n",
      "...                 ...               ...               ...               ...   \n",
      "38615  06SEP22:14:02:31  06SEP22:14:05:10  06SEP22:14:06:15  06SEP22:14:10:15   \n",
      "38616  06SEP22:14:11:22  06SEP22:14:12:08  06SEP22:14:12:09              None   \n",
      "38617  06SEP22:14:11:47              None              None              None   \n",
      "38618  06SEP22:14:11:47              None              None              None   \n",
      "38619  06SEP22:14:13:51  06SEP22:14:17:36  06SEP22:14:17:37  06SEP22:14:27:13   \n",
      "\n",
      "                     t4                t5                t6                t7  \n",
      "0                  None              None  01JUN22:00:38:06  01JUN22:00:43:54  \n",
      "1      01JUN22:00:19:49  01JUN22:00:24:25  01JUN22:01:59:18  01JUN22:01:59:19  \n",
      "2                  None              None              None              None  \n",
      "3                  None              None              None              None  \n",
      "4                  None              None  01JUN22:00:34:11  01JUN22:00:55:36  \n",
      "...                 ...               ...               ...               ...  \n",
      "38615  06SEP22:14:10:33  06SEP22:14:39:02  06SEP22:14:25:39  06SEP22:14:35:48  \n",
      "38616              None              None  06SEP22:15:32:06  06SEP22:15:36:50  \n",
      "38617              None              None              None              None  \n",
      "38618              None              None              None              None  \n",
      "38619  06SEP22:14:42:10  06SEP22:14:48:11  06SEP22:15:24:23  06SEP22:15:37:04  \n",
      "\n",
      "[36710 rows x 8 columns]\n",
      "                      t0                t1                       t2  \\\n",
      "0       01JUN22:00:01:34  01JUN22:00:03:26  2022-06-01 00:07:34.655   \n",
      "1       01JUN22:00:03:51  01JUN22:00:06:14  2022-06-01 00:10:00.017   \n",
      "2       01JUN22:00:03:51  01JUN22:00:22:18  2022-06-01 00:25:35.404   \n",
      "3       01JUN22:00:08:56  01JUN22:00:12:59  2022-06-01 00:15:36.856   \n",
      "4       01JUN22:00:10:38  01JUN22:00:15:00  2022-06-01 00:18:01.345   \n",
      "...                  ...               ...                      ...   \n",
      "200622  11JUL22:06:59:08  11JUL22:07:01:30  2022-07-11 07:02:55.511   \n",
      "200623  11JUL22:07:01:37  11JUL22:07:03:26  2022-07-11 07:04:28.501   \n",
      "200624  11JUL22:07:05:40  11JUL22:07:08:41  2022-07-11 07:14:04.569   \n",
      "200625  11JUL22:07:09:58  11JUL22:07:12:09  2022-07-11 07:15:56.741   \n",
      "200626  11JUL22:07:09:58  11JUL22:07:12:09  2022-07-11 07:15:58.751   \n",
      "\n",
      "                             t3                       t4  \\\n",
      "0       2022-06-01 00:17:53.888                     None   \n",
      "1       2022-06-01 00:16:25.356  2022-06-01 00:46:34.089   \n",
      "2       2022-06-01 00:35:58.397  2022-06-01 00:46:30.026   \n",
      "3       2022-06-01 00:27:35.760                     None   \n",
      "4       2022-06-01 00:25:28.053  2022-06-01 00:50:17.593   \n",
      "...                         ...                      ...   \n",
      "200622  2022-07-11 07:13:41.402  2022-07-11 07:29:08.581   \n",
      "200623  2022-07-11 07:09:30.712  2022-07-11 07:20:45.476   \n",
      "200624  2022-07-11 07:21:20.130  2022-07-11 07:43:08.643   \n",
      "200625  2022-07-11 07:23:44.727  2022-07-11 07:43:40.960   \n",
      "200626  2022-07-11 07:33:37.331                     None   \n",
      "\n",
      "                             t5                       t6  \\\n",
      "0                          None  2022-06-01 00:28:18.934   \n",
      "1       2022-06-01 00:56:48.871  2022-06-01 01:08:30.111   \n",
      "2       2022-06-01 01:06:50.953  2022-06-01 01:06:52.142   \n",
      "3                          None  2022-06-01 00:40:57.191   \n",
      "4       2022-06-01 00:59:50.136  2022-06-01 01:23:34.290   \n",
      "...                         ...                      ...   \n",
      "200622  2022-07-11 07:50:31.522  2022-07-11 08:12:03.972   \n",
      "200623  2022-07-11 07:31:31.721  2022-07-11 07:39:09.480   \n",
      "200624  2022-07-11 07:43:14.808  2022-07-11 08:06:08.703   \n",
      "200625  2022-07-11 07:58:36.802  2022-07-11 08:06:58.932   \n",
      "200626                     None  2022-07-11 07:40:38.960   \n",
      "\n",
      "                             t7  \n",
      "0       2022-06-01 00:53:17.876  \n",
      "1       2022-06-01 01:25:44.611  \n",
      "2       2022-06-01 01:20:48.471  \n",
      "3       2022-06-01 00:48:03.555  \n",
      "4                          None  \n",
      "...                         ...  \n",
      "200622  2022-07-11 08:29:01.763  \n",
      "200623  2022-07-11 08:02:15.653  \n",
      "200624  2022-07-11 08:13:09.447  \n",
      "200625  2022-07-11 08:32:28.872  \n",
      "200626  2022-07-11 07:54:38.570  \n",
      "\n",
      "[200627 rows x 8 columns]\n",
      "                      t0                t1                       t2  \\\n",
      "0       11JUL22:07:13:11  11JUL22:07:18:50  2022-07-11 07:22:08.119   \n",
      "1       11JUL22:07:14:34  11JUL22:07:15:53  2022-07-11 07:17:08.742   \n",
      "2       11JUL22:07:25:15  11JUL22:07:27:35  2022-07-11 07:48:20.678   \n",
      "3       11JUL22:07:45:26  11JUL22:07:47:50                     None   \n",
      "4       11JUL22:07:55:06  11JUL22:07:57:42  2022-07-11 07:59:16.532   \n",
      "...                  ...               ...                      ...   \n",
      "200622  05JAN23:01:04:14  05JAN23:01:05:16  2023-01-05 01:08:26.402   \n",
      "200623  05JAN23:01:03:40  05JAN23:01:06:33  2023-01-05 01:08:40.194   \n",
      "200624  05JAN23:01:03:40  05JAN23:01:06:33  2023-01-05 01:08:15.852   \n",
      "200625  05JAN23:01:12:59  05JAN23:01:14:45  2023-01-05 01:17:41.371   \n",
      "200626  05JAN23:01:14:14  05JAN23:01:15:24  2023-01-05 01:19:58.007   \n",
      "\n",
      "                             t3                       t4  \\\n",
      "0       2022-07-11 07:28:57.144  2022-07-11 07:45:22.173   \n",
      "1       2022-07-11 07:29:44.622  2022-07-11 07:41:28.712   \n",
      "2       2022-07-11 07:48:21.816  2022-07-11 07:52:23.829   \n",
      "3                          None                     None   \n",
      "4       2022-07-11 08:07:27.738  2022-07-11 08:37:04.903   \n",
      "...                         ...                      ...   \n",
      "200622  2023-01-05 01:12:39.404  2023-01-05 01:17:52.820   \n",
      "200623  2023-01-05 01:15:05.927  2023-01-05 01:39:50.156   \n",
      "200624  2023-01-05 01:15:05.783  2023-01-05 01:25:55.517   \n",
      "200625  2023-01-05 01:23:21.449  2023-01-05 01:38:40.676   \n",
      "200626                     None                     None   \n",
      "\n",
      "                             t5                       t6  \\\n",
      "0       2022-07-11 08:07:54.768  2022-07-11 08:36:55.435   \n",
      "1       2022-07-11 07:57:43.712  2022-07-11 08:13:43.882   \n",
      "2                          None                     None   \n",
      "3                          None                     None   \n",
      "4       2022-07-11 08:43:28.982  2022-07-11 08:57:17.163   \n",
      "...                         ...                      ...   \n",
      "200622  2023-01-05 01:20:03.729  2023-01-05 01:26:43.229   \n",
      "200623  2023-01-05 01:40:32.079  2023-01-05 02:16:19.238   \n",
      "200624  2023-01-05 01:39:24.322                     None   \n",
      "200625  2023-01-05 01:44:17.804  2023-01-05 01:55:18.746   \n",
      "200626                     None  2023-01-05 02:12:35.300   \n",
      "\n",
      "                             t7  \n",
      "0       2022-07-11 09:07:44.134  \n",
      "1       2022-07-11 08:16:46.852  \n",
      "2       2022-07-11 09:55:32.359  \n",
      "3       2022-07-11 17:51:31.334  \n",
      "4       2022-07-11 09:18:02.762  \n",
      "...                         ...  \n",
      "200622  2023-01-05 01:33:02.546  \n",
      "200623  2023-01-05 02:25:53.991  \n",
      "200624  2023-01-05 01:39:40.532  \n",
      "200625  2023-01-05 02:23:03.680  \n",
      "200626  2023-01-05 02:15:07.793  \n",
      "\n",
      "[200627 rows x 8 columns]\n",
      "                      t0                t1                       t2  \\\n",
      "0       05JAN23:01:18:27  05JAN23:01:20:35  2023-01-05 01:22:58.369   \n",
      "1       05JAN23:01:20:59  05JAN23:01:22:13  2023-01-05 01:23:02.798   \n",
      "2       05JAN23:01:34:07  05JAN23:01:36:05  2023-01-05 01:39:20.636   \n",
      "3       05JAN23:01:33:32  05JAN23:01:37:23  2023-01-05 01:41:30.735   \n",
      "4       05JAN23:01:42:33  05JAN23:01:44:33  2023-01-05 01:48:31.662   \n",
      "...                  ...               ...                      ...   \n",
      "200622  31MAY23:22:39:45  31MAY23:22:44:33                     None   \n",
      "200623  31MAY23:22:50:56  31MAY23:22:52:38                     None   \n",
      "200624  31MAY23:23:00:15  31MAY23:23:02:37                     None   \n",
      "200625  31MAY23:23:41:10  31MAY23:23:44:47                     None   \n",
      "200626  31MAY23:23:47:52  31MAY23:23:49:37                     None   \n",
      "\n",
      "                             t3                       t4  \\\n",
      "0       2023-01-05 01:26:06.798                     None   \n",
      "1       2023-01-05 01:28:53.947                     None   \n",
      "2       2023-01-05 01:44:38.018  2023-01-05 01:49:30.303   \n",
      "3       2023-01-05 01:47:22.982                     None   \n",
      "4       2023-01-05 01:55:18.586  2023-01-05 02:05:59.829   \n",
      "...                         ...                      ...   \n",
      "200622                     None                     None   \n",
      "200623                     None                     None   \n",
      "200624                     None                     None   \n",
      "200625                     None                     None   \n",
      "200626                     None                     None   \n",
      "\n",
      "                             t5                       t6  \\\n",
      "0                          None  2023-01-05 01:45:51.789   \n",
      "1                          None  2023-01-05 01:45:25.279   \n",
      "2       2023-01-05 01:54:36.206  2023-01-05 02:08:45.980   \n",
      "3                          None  2023-01-05 02:03:42.469   \n",
      "4       2023-01-05 02:10:11.140  2023-01-05 02:23:53.379   \n",
      "...                         ...                      ...   \n",
      "200622                     None                     None   \n",
      "200623                     None                     None   \n",
      "200624                     None                     None   \n",
      "200625                     None                     None   \n",
      "200626                     None                     None   \n",
      "\n",
      "                             t7  \n",
      "0       2023-01-05 01:49:10.253  \n",
      "1       2023-01-05 03:01:42.012  \n",
      "2       2023-01-05 02:26:12.779  \n",
      "3       2023-01-05 02:08:21.270  \n",
      "4       2023-01-05 02:32:04.849  \n",
      "...                         ...  \n",
      "200622                     None  \n",
      "200623                     None  \n",
      "200624                     None  \n",
      "200625                     None  \n",
      "200626                     None  \n",
      "\n",
      "[200627 rows x 8 columns]\n",
      "                             t0                t1                       t2  \\\n",
      "0       2022-06-01 00:12:50.000  01JUN22:00:14:32  2022-06-01 00:16:56.000   \n",
      "1       2022-06-01 00:11:02.000  01JUN22:00:15:09  2022-06-01 00:18:12.000   \n",
      "2       2022-07-14 16:54:37.000  01JUN22:01:07:46  2022-06-01 01:11:48.000   \n",
      "3       2022-07-14 16:54:37.000  01JUN22:01:10:19  2022-06-01 01:11:48.000   \n",
      "4       2022-06-01 01:14:59.000  01JUN22:01:17:29  2022-06-01 01:24:25.000   \n",
      "...                         ...               ...                      ...   \n",
      "289396                     None  06JUL22:18:40:51                     None   \n",
      "289397                     None  06JUL22:19:08:28                     None   \n",
      "289398                     None  06JUL22:19:20:02                     None   \n",
      "289399                     None  07JUL22:12:57:39                     None   \n",
      "289400                     None  16AUG22:10:22:18                     None   \n",
      "\n",
      "                             t3                       t4  \\\n",
      "0       2022-06-01 00:24:46.000  2022-06-01 00:35:52.000   \n",
      "1       2022-06-01 00:22:34.000  2022-06-01 00:35:33.000   \n",
      "2       2022-06-01 01:17:45.000  2022-06-01 01:47:37.000   \n",
      "3       2022-06-01 01:17:45.000  2022-06-01 01:47:37.000   \n",
      "4       2022-06-01 01:31:13.000  2022-06-01 01:44:03.000   \n",
      "...                         ...                      ...   \n",
      "289396  2022-07-06 18:40:52.000                     None   \n",
      "289397  2022-07-06 19:08:28.000                     None   \n",
      "289398  2022-07-06 19:20:02.000                     None   \n",
      "289399  2022-07-07 12:57:39.000                     None   \n",
      "289400  2022-08-16 10:22:18.000                     None   \n",
      "\n",
      "                             t5                       t6  \\\n",
      "0       2022-06-01 00:46:30.000  2022-06-01 00:57:15.000   \n",
      "1       2022-06-01 00:43:14.000  2022-06-01 00:49:39.000   \n",
      "2       2022-06-01 02:36:49.000  2022-06-01 02:36:54.000   \n",
      "3       2022-06-01 02:36:49.000  2022-06-01 02:36:54.000   \n",
      "4                          None                     None   \n",
      "...                         ...                      ...   \n",
      "289396                     None                     None   \n",
      "289397                     None                     None   \n",
      "289398                     None                     None   \n",
      "289399                     None                     None   \n",
      "289400                     None                     None   \n",
      "\n",
      "                             t7  \n",
      "0                          None  \n",
      "1                          None  \n",
      "2       2022-06-01 01:10:17.000  \n",
      "3       2022-06-01 01:10:17.000  \n",
      "4       2022-06-01 01:17:25.000  \n",
      "...                         ...  \n",
      "289396                     None  \n",
      "289397                     None  \n",
      "289398                     None  \n",
      "289399                     None  \n",
      "289400                     None  \n",
      "\n",
      "[289401 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Select columns\n",
    "columns_to_show = ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']\n",
    "selected_columns_1 = df_interventions_bxl[columns_to_show]\n",
    "selected_columns_2 = df_interventions_bxl2[columns_to_show]\n",
    "selected_columns_3 = df_interventions1[columns_to_show]\n",
    "selected_columns_4 = df_interventions2[columns_to_show]\n",
    "selected_columns_5 = df_interventions3[columns_to_show]\n",
    "selected_columns_6 = df_cad9[columns_to_show]\n",
    "\n",
    "# Show the selected columns\n",
    "print(selected_columns_1)\n",
    "print(selected_columns_2)\n",
    "print(selected_columns_3)\n",
    "print(selected_columns_4)\n",
    "print(selected_columns_5)\n",
    "print(selected_columns_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the correct date format string\n",
    "date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "# Define custom format string for parsing the datetime format \"01JUN22:00:02:45\"\n",
    "custom_format = \"%d%b%y:%H:%M:%S\"\n",
    "\n",
    "# Define datasets and their corresponding date columns\n",
    "datasets = {\n",
    "    'df_interventions_bxl': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "    'df_interventions_bxl2': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "    'df_interventions1': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "    'df_interventions2': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "    'df_interventions3': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "    'df_cad9': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']\n",
    "}\n",
    "\n",
    "# Apply the correct format string to each date column for each dataset\n",
    "for dataset_name, date_cols in datasets.items():\n",
    "    dataset = globals()[dataset_name]  # Get the dataset using its name\n",
    "    for column in date_cols:\n",
    "        try:\n",
    "            # Try parsing with custom format\n",
    "            dataset[column] = pd.to_datetime(dataset[column], utc=True, format=custom_format).dt.strftime(date_format)\n",
    "        except ValueError:\n",
    "            # If parsing with custom format fails, try the default format\n",
    "            dataset[column] = pd.to_datetime(dataset[column], utc=True).dt.strftime(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "# Define the correct date format strings\n",
    "#date_format_1 = \"%d%b%y:%H:%M:%S\"\n",
    "#date_format_2 = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "#date_format_3 = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "# Define datasets and their corresponding date columns\n",
    "#datasets = {\n",
    "#    'df_interventions_bxl': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "#    'df_interventions_bxl2': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "#    'df_interventions1': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "#    'df_interventions2': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "#    'df_interventions3': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7'],\n",
    "#    'df_cad9': ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']\n",
    "#}\n",
    "\n",
    "# Apply the correct format string to each date column for each dataset\n",
    "#for dataset_name, date_cols in datasets.items():\n",
    "#    dataset = globals()[dataset_name]  # Get the dataset using its name\n",
    "#    for column in date_cols:\n",
    "#        try:\n",
    "#           # Try parsing with date_format_1\n",
    "#            dataset[column] = pd.to_datetime(dataset[column], utc=True, format=date_format_1).dt.strftime(date_format_2)\n",
    "#        except ValueError:\n",
    "#            # If parsing with date_format_1 fails, try date_format_2\n",
    "#            dataset[column] = pd.to_datetime(dataset[column], utc=True, format='mixed').dt.strftime(date_format_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_interventions_bxl: Start Date of t0: 2022-09-06 09:49:21, End Date of t0: 2023-05-31 21:42:36\n",
      "df_interventions_bxl2: Start Date of t0: 2022-06-01 00:02:45, End Date of t0: 2022-09-06 14:13:51\n",
      "df_interventions1: Start Date of t0: 2022-06-01 00:00:20, End Date of t0: 2023-05-31 23:59:47\n",
      "df_interventions2: Start Date of t0: 2022-06-01 00:50:04, End Date of t0: 2023-05-31 23:50:45\n",
      "df_interventions3: Start Date of t0: 2022-06-01 00:10:36, End Date of t0: 2023-05-31 23:47:52\n",
      "df_cad9: Start Date of t0: 2022-04-06 09:54:18, End Date of t0: 2023-05-31 23:56:18\n"
     ]
    }
   ],
   "source": [
    "# Check the start and end dates of t0 in all_interventions\n",
    "for dataset_name, dataset in zip(['df_interventions_bxl', 'df_interventions_bxl2', 'df_interventions1',\n",
    "                                  'df_interventions2', 'df_interventions3', 'df_cad9'], all_interventions):\n",
    "    dataset['t0'] = pd.to_datetime(dataset['t0'])\n",
    "    start_date = dataset['t0'].min()\n",
    "    end_date = dataset['t0'].max()\n",
    "    print(f\"{dataset_name}: Start Date of t0: {start_date}, End Date of t0: {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_interventions_bxl: Start Date of t0: 2022-09-06 09:49:21, End Date of t0: 2023-05-31 21:42:36\n",
      "df_interventions_bxl2: Start Date of t0: 2022-06-01 00:02:45, End Date of t0: 2022-09-06 14:13:51\n",
      "df_interventions1: Start Date of t0: 2022-06-01 00:00:20, End Date of t0: 2023-05-31 23:59:47\n",
      "df_interventions2: Start Date of t0: 2022-06-01 00:50:04, End Date of t0: 2023-05-31 23:50:45\n",
      "df_interventions3: Start Date of t0: 2022-06-01 00:10:36, End Date of t0: 2023-05-31 23:47:52\n",
      "df_cad9: Start Date of t0: 2022-06-01 00:03:06, End Date of t0: 2023-05-31 23:56:18\n"
     ]
    }
   ],
   "source": [
    "# Filter data before 2022-06-01 in each dataset and check the start and end dates\n",
    "for dataset_name, dataset in zip(['df_interventions_bxl', 'df_interventions_bxl2', 'df_interventions1',\n",
    "                                  'df_interventions2', 'df_interventions3', 'df_cad9'], all_interventions):\n",
    "    dataset['t0'] = pd.to_datetime(dataset['t0'])  # Convert 't0' column to datetime format\n",
    "    dataset.drop(dataset[dataset['t0'] < '2022-06-01'].index, inplace=True)  # Delete rows before 2022-06-01\n",
    "    start_date = dataset['t0'].min()  # Get the start date of t0\n",
    "    end_date = dataset['t0'].max()  # Get the end date of t0\n",
    "    print(f\"{dataset_name}: Start Date of t0: {start_date}, End Date of t0: {end_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data sets as a Parquet file\n",
    "df_interventions_bxl.to_parquet('../../1_Data/CLEANED/df_interventions_bxl.parquet')\n",
    "df_interventions_bxl2.to_parquet('../../1_Data/CLEANED/df_interventions_bxl2.parquet')\n",
    "df_interventions1.to_parquet('../../1_Data/CLEANED/df_interventions1.parquet')\n",
    "df_interventions2.to_parquet('../../1_Data/CLEANED/df_interventions2.parquet')\n",
    "df_interventions3.to_parquet('../../1_Data/CLEANED/df_interventions3.parquet')\n",
    "df_cad9.to_parquet('../../1_Data/CLEANED/df_cad9.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Path to your Parquet gzip file\n",
    "file_path_5 = '../../1_Data/CLEANED/df_interventions_bxl.parquet'\n",
    "file_path_6 = '../../1_Data/CLEANED/df_interventions_bxl2.parquet'\n",
    "file_path_7 = '../../1_Data/CLEANED/df_interventions1.parquet'\n",
    "file_path_8 = '../../1_Data/CLEANED/df_interventions2.parquet'\n",
    "file_path_9 = '../../1_Data/CLEANED/df_interventions3.parquet'\n",
    "file_path_10 = '../../1_Data/CLEANED/df_cad9.parquet'\n",
    "\n",
    "# Read the Parquet file into a pandas DataFrame\n",
    "df_interventions_bxl = pd.read_parquet(file_path_5, engine='pyarrow')\n",
    "df_interventions_bxl2 = pd.read_parquet(file_path_6, engine='pyarrow')\n",
    "df_interventions1 = pd.read_parquet(file_path_7, engine='pyarrow')\n",
    "df_interventions2 = pd.read_parquet(file_path_8, engine='pyarrow')\n",
    "df_interventions3 = pd.read_parquet(file_path_9, engine='pyarrow')\n",
    "df_cad9 = pd.read_parquet(file_path_10, engine='pyarrow')\n",
    "\n",
    "# Define your dataset\n",
    "all_datasets = [df_ambulance_locations, df_pit_locations, df_mug_locations, df_aed_locations, df_interventions_bxl, df_interventions_bxl2, df_interventions1, df_interventions2, df_interventions3, df_cad9]\n",
    "all_interventions = [df_interventions_bxl, df_interventions_bxl2, df_interventions1, df_interventions2, df_interventions3, df_cad9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data set\n",
    "#df_cad9[['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']].head(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with non-consecutive timestamps in Dataset 1:\n",
      "[Timestamp('2022-09-06 11:26:48'), Timestamp('2022-09-06 11:30:55'), Timestamp('2022-09-06 11:32:40'), Timestamp('2022-09-06 11:36:27'), Timestamp('2022-09-06 12:11:46'), Timestamp('2022-09-06 12:11:00'), Timestamp('2022-09-06 12:44:17'), Timestamp('2022-09-06 13:32:42')]\n",
      "[Timestamp('2022-09-06 12:13:58'), Timestamp('2022-09-06 12:20:09'), Timestamp('2022-09-06 12:25:05'), Timestamp('2022-09-06 12:25:17'), Timestamp('2022-09-06 12:25:00'), Timestamp('2022-09-06 12:25:00')]\n",
      "[Timestamp('2022-09-06 12:24:25'), Timestamp('2022-09-06 12:32:05'), Timestamp('2022-09-06 12:27:29'), Timestamp('2022-09-06 13:27:03'), Timestamp('2022-09-06 13:32:48')]\n",
      "[Timestamp('2022-09-06 12:38:11'), Timestamp('2022-09-06 12:41:16'), Timestamp('2022-09-06 12:41:03'), Timestamp('2022-09-06 12:45:44'), Timestamp('2022-09-06 12:53:55'), Timestamp('2022-09-06 13:01:32'), Timestamp('2022-09-06 13:24:07')]\n",
      "[Timestamp('2022-09-06 12:51:41'), Timestamp('2022-09-06 12:57:28'), Timestamp('2022-09-06 12:57:29'), Timestamp('2022-09-07 12:57:00'), Timestamp('2022-09-06 13:20:26'), Timestamp('2022-09-06 13:23:06'), Timestamp('2022-09-06 13:50:05')]\n",
      "[Timestamp('2022-09-06 13:08:52'), Timestamp('2022-09-06 13:28:48'), Timestamp('2022-09-06 13:13:18'), Timestamp('2022-09-06 14:05:04'), Timestamp('2022-09-06 14:05:05'), Timestamp('2022-09-06 14:05:07'), Timestamp('2022-09-06 14:29:14'), Timestamp('2022-09-06 14:33:20')]\n",
      "[Timestamp('2022-09-06 13:29:41'), Timestamp('2022-09-06 13:52:42'), Timestamp('2022-09-06 13:52:32'), Timestamp('2022-09-06 13:55:10'), Timestamp('2022-09-06 14:10:02'), Timestamp('2022-09-06 14:12:47'), Timestamp('2022-09-06 14:22:02')]\n",
      "[Timestamp('2022-09-06 13:41:16'), Timestamp('2022-09-06 13:43:57'), Timestamp('2022-09-06 13:43:43'), Timestamp('2022-09-06 14:01:51'), Timestamp('2022-09-06 14:01:52'), Timestamp('2022-09-06 14:21:14'), Timestamp('2022-09-06 14:29:10')]\n",
      "[Timestamp('2022-09-06 14:05:12'), Timestamp('2022-09-06 14:26:16'), Timestamp('2022-09-06 14:10:43'), Timestamp('2022-09-06 14:19:19'), Timestamp('2022-09-06 14:27:46'), Timestamp('2022-09-06 14:34:31'), Timestamp('2022-09-06 14:56:12')]\n",
      "[Timestamp('2022-09-06 14:18:40'), Timestamp('2022-09-06 14:20:55'), Timestamp('2022-09-06 14:20:25'), Timestamp('2022-09-07 14:20:00'), Timestamp('2022-09-06 15:13:46'), Timestamp('2022-09-06 15:38:12')]\n",
      "Rows with non-consecutive timestamps in Dataset 2:\n",
      "[Timestamp('2022-06-01 00:43:04'), Timestamp('2022-06-01 00:45:39'), Timestamp('2022-06-01 00:48:40'), Timestamp('2022-06-01 01:01:28'), Timestamp('2022-06-01 01:01:28'), Timestamp('2022-06-01 02:00:14'), Timestamp('2022-06-01 01:17:03'), Timestamp('2022-06-01 01:21:07')]\n",
      "[Timestamp('2022-06-01 00:44:36'), Timestamp('2022-06-01 00:49:09'), Timestamp('2022-06-01 00:49:10'), Timestamp('2022-06-01 00:51:15'), Timestamp('2022-06-01 01:11:10'), Timestamp('2022-06-01 01:25:15'), Timestamp('2022-06-01 01:25:05'), Timestamp('2022-06-01 01:30:17')]\n",
      "[Timestamp('2022-06-01 02:35:53'), Timestamp('2022-06-01 02:25:02'), Timestamp('2022-06-01 02:27:27'), Timestamp('2022-06-01 02:39:00'), Timestamp('2022-06-01 02:39:01'), Timestamp('2022-06-01 03:04:43')]\n",
      "[Timestamp('2022-06-01 03:42:50'), Timestamp('2022-06-01 03:44:13'), Timestamp('2022-06-01 03:47:17'), Timestamp('2022-06-01 04:05:35'), Timestamp('2022-06-01 04:05:41'), Timestamp('2022-06-01 04:02:27'), Timestamp('2022-06-01 04:11:23')]\n",
      "[Timestamp('2022-06-01 06:16:20'), Timestamp('2022-06-01 06:19:29'), Timestamp('2022-06-01 06:35:05'), Timestamp('2022-06-01 06:29:58'), Timestamp('2022-06-01 06:46:42'), Timestamp('2022-06-01 07:17:29')]\n",
      "[Timestamp('2022-06-01 07:16:47'), Timestamp('2022-06-01 07:19:27'), Timestamp('2022-06-01 07:21:38'), Timestamp('2022-06-01 07:26:28'), Timestamp('2022-06-01 07:36:51'), Timestamp('2022-06-01 08:01:14'), Timestamp('2022-06-01 07:44:31')]\n",
      "[Timestamp('2022-06-01 07:30:15'), Timestamp('2022-06-01 07:32:08'), Timestamp('2022-06-01 07:34:01'), Timestamp('2022-06-01 07:44:43'), Timestamp('2022-06-01 07:36:33'), Timestamp('2022-06-01 07:44:25'), Timestamp('2022-06-01 07:44:51'), Timestamp('2022-06-01 07:48:23')]\n",
      "[Timestamp('2022-06-01 07:45:30'), Timestamp('2022-06-01 07:47:01'), Timestamp('2022-06-01 07:48:46'), Timestamp('2022-06-01 08:16:13'), Timestamp('2022-06-01 08:09:13'), Timestamp('2022-06-01 08:23:22'), Timestamp('2022-06-01 08:36:14')]\n",
      "[Timestamp('2022-06-01 08:19:12'), Timestamp('2022-06-01 08:20:25'), Timestamp('2022-06-01 08:22:37'), Timestamp('2022-06-01 08:31:20'), Timestamp('2022-06-01 08:52:56'), Timestamp('2022-06-01 09:31:44'), Timestamp('2022-06-01 09:13:16'), Timestamp('2022-06-01 09:26:49')]\n",
      "[Timestamp('2022-06-01 08:41:03'), Timestamp('2022-06-01 08:42:17'), Timestamp('2022-06-01 08:45:29'), Timestamp('2022-06-01 08:53:12'), Timestamp('2022-06-01 10:11:55'), Timestamp('2022-06-01 10:11:57'), Timestamp('2022-06-01 10:06:16')]\n",
      "Rows with non-consecutive timestamps in Dataset 3:\n",
      "[Timestamp('2022-06-01 04:06:31'), Timestamp('2022-06-01 04:09:20'), Timestamp('2022-06-01 04:12:57'), Timestamp('2022-06-01 04:23:00'), Timestamp('2022-06-01 04:47:37'), Timestamp('2022-06-01 04:47:39'), Timestamp('2022-06-01 06:52:49'), Timestamp('2022-06-01 05:10:19')]\n",
      "[Timestamp('2022-06-01 06:45:53'), Timestamp('2022-06-01 06:48:30'), Timestamp('2022-06-01 06:52:06'), Timestamp('2022-06-01 07:04:46'), Timestamp('2022-06-01 06:49:27')]\n",
      "[Timestamp('2022-06-01 07:15:58'), Timestamp('2022-06-01 07:18:29'), Timestamp('2022-06-01 08:22:08'), Timestamp('2022-06-01 07:23:55'), Timestamp('2022-06-01 07:33:43'), Timestamp('2022-06-01 07:36:08'), Timestamp('2022-06-01 08:15:09')]\n",
      "[Timestamp('2022-06-01 11:43:31'), Timestamp('2022-06-01 11:49:51'), Timestamp('2022-06-01 11:47:13'), Timestamp('2022-06-01 11:59:32'), Timestamp('2022-06-01 12:09:31'), Timestamp('2022-06-01 12:19:16'), Timestamp('2022-06-01 12:36:06')]\n",
      "[Timestamp('2022-06-01 13:24:37'), Timestamp('2022-06-01 13:27:33'), Timestamp('2022-06-01 13:27:15'), Timestamp('2022-06-01 13:31:04'), Timestamp('2022-06-01 13:37:36'), Timestamp('2022-06-01 13:41:27'), Timestamp('2022-06-01 13:51:47'), Timestamp('2022-06-01 13:56:21')]\n",
      "[Timestamp('2022-06-02 00:27:06'), Timestamp('2022-06-02 00:29:24'), Timestamp('2022-06-02 00:33:25'), Timestamp('2022-06-02 00:41:24'), Timestamp('2022-06-02 00:49:46'), Timestamp('2022-06-02 00:48:56')]\n",
      "[Timestamp('2022-06-02 04:34:32'), Timestamp('2022-06-02 04:35:48'), Timestamp('2022-06-02 04:39:46'), Timestamp('2022-06-02 04:57:06'), Timestamp('2022-06-02 04:53:27'), Timestamp('2022-06-02 05:02:44'), Timestamp('2022-06-02 05:28:36'), Timestamp('2022-06-02 05:34:36')]\n",
      "[Timestamp('2022-06-02 05:57:41'), Timestamp('2022-06-02 05:58:48'), Timestamp('2022-06-02 06:02:32'), Timestamp('2022-06-02 06:58:28'), Timestamp('2022-06-02 06:42:31'), Timestamp('2022-06-02 06:44:56'), Timestamp('2022-06-02 07:07:14')]\n",
      "[Timestamp('2022-06-02 09:27:21'), Timestamp('2022-06-02 09:30:45'), Timestamp('2022-06-02 09:36:07'), Timestamp('2022-06-02 10:20:14'), Timestamp('2022-06-02 10:00:17'), Timestamp('2022-06-02 10:11:21'), Timestamp('2022-06-02 10:29:09'), Timestamp('2022-06-02 10:49:45')]\n",
      "[Timestamp('2022-06-02 11:09:03'), Timestamp('2022-06-02 11:19:42'), Timestamp('2022-06-02 11:15:38'), Timestamp('2022-06-02 11:24:04'), Timestamp('2022-06-02 11:56:05'), Timestamp('2022-06-02 12:08:03'), Timestamp('2022-06-02 12:25:35')]\n",
      "Rows with non-consecutive timestamps in Dataset 4:\n",
      "[Timestamp('2022-07-11 09:40:24'), Timestamp('2022-07-11 09:42:51'), Timestamp('2022-07-11 09:46:46'), Timestamp('2022-07-11 10:22:29'), Timestamp('2022-07-11 10:22:22'), Timestamp('2022-07-11 10:32:54'), Timestamp('2022-07-11 10:50:55'), Timestamp('2022-07-11 10:55:54')]\n",
      "[Timestamp('2022-07-11 13:06:19'), Timestamp('2022-07-12 13:08:01'), Timestamp('2022-07-12 13:10:00'), Timestamp('2022-07-12 13:21:00'), Timestamp('2022-07-11 13:29:54'), Timestamp('2022-07-11 13:39:04'), Timestamp('2022-07-11 13:52:48'), Timestamp('2022-07-11 13:57:55')]\n",
      "[Timestamp('2022-07-11 15:24:19'), Timestamp('2022-07-11 15:26:03'), Timestamp('2022-07-11 15:57:33'), Timestamp('2022-07-11 15:58:58'), Timestamp('2022-07-11 15:58:00'), Timestamp('2022-07-11 16:04:00'), Timestamp('2022-07-11 16:43:14'), Timestamp('2022-07-11 16:43:22')]\n",
      "[Timestamp('2022-07-11 20:34:05'), Timestamp('2022-07-11 20:58:57'), Timestamp('2022-07-11 20:38:23'), Timestamp('2022-07-11 20:46:12'), Timestamp('2022-07-11 21:39:36'), Timestamp('2022-07-11 21:48:50'), Timestamp('2022-07-11 22:03:13'), Timestamp('2022-07-11 22:39:04')]\n",
      "[Timestamp('2022-07-11 20:53:05'), Timestamp('2022-07-11 20:56:09'), Timestamp('2022-07-11 20:56:00'), Timestamp('2022-07-11 21:00:00'), Timestamp('2022-07-11 21:04:22')]\n",
      "[Timestamp('2022-07-11 20:53:05'), Timestamp('2022-07-12 20:56:00'), Timestamp('2022-07-12 20:57:00'), Timestamp('2022-07-12 21:00:00'), Timestamp('2022-07-11 21:18:30'), Timestamp('2022-07-11 21:55:18')]\n",
      "[Timestamp('2022-07-11 20:57:25'), Timestamp('2022-07-11 21:00:43'), Timestamp('2022-07-11 21:05:26'), Timestamp('2022-07-11 21:23:07'), Timestamp('2022-07-11 21:35:06'), Timestamp('2022-07-11 22:06:14'), Timestamp('2022-07-11 21:55:49'), Timestamp('2022-07-11 22:06:25')]\n",
      "[Timestamp('2022-07-11 21:09:17'), Timestamp('2022-07-11 21:10:52'), Timestamp('2022-07-11 21:17:52'), Timestamp('2022-07-11 21:17:44'), Timestamp('2022-07-11 21:38:33'), Timestamp('2022-07-11 22:08:30'), Timestamp('2022-07-11 22:08:32'), Timestamp('2022-07-11 22:19:22')]\n",
      "[Timestamp('2022-07-12 00:30:42'), Timestamp('2022-07-12 00:32:53'), Timestamp('2022-07-12 00:35:04'), Timestamp('2022-07-12 00:45:00'), Timestamp('2022-07-12 01:09:00'), Timestamp('2022-07-12 01:18:54'), Timestamp('2022-07-12 01:37:19'), Timestamp('2022-07-12 01:37:18')]\n",
      "[Timestamp('2022-07-12 01:13:29'), Timestamp('2022-07-12 01:17:54'), Timestamp('2022-07-12 01:27:59'), Timestamp('2022-07-12 01:27:45'), Timestamp('2022-07-12 02:05:11'), Timestamp('2022-07-12 02:12:00'), Timestamp('2022-07-12 02:34:56')]\n",
      "Rows with non-consecutive timestamps in Dataset 5:\n",
      "[Timestamp('2023-01-05 13:35:35'), Timestamp('2023-01-05 13:44:24'), Timestamp('2023-01-06 13:43:00'), Timestamp('2023-01-05 13:45:00'), Timestamp('2023-01-05 14:04:33'), Timestamp('2023-01-05 14:22:44'), Timestamp('2023-01-05 14:23:04'), Timestamp('2023-01-05 14:46:14')]\n",
      "[Timestamp('2023-01-05 16:32:30'), Timestamp('2023-01-05 16:40:16'), Timestamp('2023-01-05 16:41:13'), Timestamp('2023-01-05 17:10:43'), Timestamp('2023-01-05 16:57:29'), Timestamp('2023-01-05 17:17:20'), Timestamp('2023-01-05 17:17:46'), Timestamp('2023-01-05 17:39:37')]\n",
      "[Timestamp('2023-01-05 17:19:35'), Timestamp('2023-01-05 17:22:54'), Timestamp('2023-01-05 18:24:52'), Timestamp('2023-01-05 17:33:41'), Timestamp('2023-01-05 18:16:30'), Timestamp('2023-01-05 18:25:01'), Timestamp('2023-01-05 18:37:13'), Timestamp('2023-01-05 18:44:21')]\n",
      "[Timestamp('2023-01-05 19:01:15'), Timestamp('2023-01-05 19:18:47'), Timestamp('2023-01-05 19:26:44'), Timestamp('2023-01-05 19:29:01'), Timestamp('2023-01-05 19:58:29'), Timestamp('2023-01-05 19:42:32'), Timestamp('2023-01-05 19:58:50')]\n",
      "[Timestamp('2023-01-05 19:42:13'), Timestamp('2023-01-05 19:45:26'), Timestamp('2023-01-05 19:46:21'), Timestamp('2023-01-05 19:53:06'), Timestamp('2023-01-05 19:52:57')]\n",
      "[Timestamp('2023-01-06 05:15:30'), Timestamp('2023-01-06 05:28:01'), Timestamp('2023-01-06 05:31:00'), Timestamp('2023-01-06 05:36:55'), Timestamp('2023-01-06 06:28:47'), Timestamp('2023-01-06 05:57:06'), Timestamp('2023-01-06 06:55:56')]\n",
      "[Timestamp('2023-01-06 17:01:33'), Timestamp('2023-01-06 17:03:01'), Timestamp('2023-01-06 17:05:17'), Timestamp('2023-01-06 17:12:04'), Timestamp('2023-01-06 17:47:29'), Timestamp('2023-01-06 17:28:20'), Timestamp('2023-01-06 17:45:56'), Timestamp('2023-01-06 17:47:33')]\n",
      "[Timestamp('2023-01-07 22:29:14'), Timestamp('2023-01-07 22:51:40'), Timestamp('2023-01-07 22:53:06'), Timestamp('2023-01-07 23:09:59'), Timestamp('2023-01-07 23:20:39'), Timestamp('2023-01-07 23:41:44'), Timestamp('2023-01-07 23:28:08'), Timestamp('2023-01-07 23:41:47')]\n",
      "[Timestamp('2023-01-08 19:29:44'), Timestamp('2023-01-08 19:32:00'), Timestamp('2023-01-08 19:34:02'), Timestamp('2023-01-08 19:40:12'), Timestamp('2023-01-08 19:45:43'), Timestamp('2023-01-08 19:53:04'), Timestamp('2023-01-08 19:51:31')]\n",
      "[Timestamp('2023-01-09 13:52:56'), Timestamp('2023-01-09 13:55:57'), Timestamp('2023-01-09 13:57:51'), Timestamp('2023-01-09 14:10:29'), Timestamp('2023-01-09 14:10:00'), Timestamp('2023-01-09 14:58:51'), Timestamp('2023-01-09 15:54:42'), Timestamp('2023-01-09 15:54:57')]\n",
      "Rows with non-consecutive timestamps in Dataset 6:\n",
      "[Timestamp('2022-07-14 16:54:37'), Timestamp('2022-06-01 01:07:46'), Timestamp('2022-06-01 01:11:48'), Timestamp('2022-06-01 01:17:45'), Timestamp('2022-06-01 01:47:37'), Timestamp('2022-06-01 02:36:49'), Timestamp('2022-06-01 02:36:54'), Timestamp('2022-06-01 01:10:17')]\n",
      "[Timestamp('2022-07-14 16:54:37'), Timestamp('2022-06-01 01:10:19'), Timestamp('2022-06-01 01:11:48'), Timestamp('2022-06-01 01:17:45'), Timestamp('2022-06-01 01:47:37'), Timestamp('2022-06-01 02:36:49'), Timestamp('2022-06-01 02:36:54'), Timestamp('2022-06-01 01:10:17')]\n",
      "[Timestamp('2022-06-01 01:14:59'), Timestamp('2022-06-01 01:17:29'), Timestamp('2022-06-01 01:24:25'), Timestamp('2022-06-01 01:31:13'), Timestamp('2022-06-01 01:44:03'), Timestamp('2022-06-01 01:17:25')]\n",
      "[Timestamp('2022-06-01 02:30:43'), Timestamp('2022-06-01 02:33:06'), Timestamp('2022-06-01 02:37:59'), Timestamp('2022-06-01 02:50:47'), Timestamp('2022-06-01 03:19:02'), Timestamp('2022-06-01 03:35:26'), Timestamp('2022-06-01 03:50:20'), Timestamp('2022-06-01 02:33:05')]\n",
      "[Timestamp('2022-06-01 02:30:43'), Timestamp('2022-06-01 02:33:07'), Timestamp('2022-06-01 02:37:59'), Timestamp('2022-06-01 02:50:47'), Timestamp('2022-06-01 03:19:02'), Timestamp('2022-06-01 03:35:26'), Timestamp('2022-06-01 03:50:20'), Timestamp('2022-06-01 02:33:05')]\n",
      "[Timestamp('2022-06-01 02:35:50'), Timestamp('2022-06-01 02:39:37'), Timestamp('2022-06-01 02:42:43'), Timestamp('2022-06-01 02:47:34'), Timestamp('2022-06-01 02:52:23'), Timestamp('2022-06-01 02:55:36'), Timestamp('2022-06-01 03:20:54'), Timestamp('2022-06-01 02:39:35')]\n",
      "[Timestamp('2022-06-01 02:35:50'), Timestamp('2022-06-01 02:39:38'), Timestamp('2022-06-01 02:42:43'), Timestamp('2022-06-01 02:47:34'), Timestamp('2022-06-01 02:52:23'), Timestamp('2022-06-01 02:55:36'), Timestamp('2022-06-01 03:20:54'), Timestamp('2022-06-01 02:39:35')]\n",
      "[Timestamp('2022-06-01 03:47:44'), Timestamp('2022-06-01 03:49:29'), Timestamp('2022-06-01 03:54:39'), Timestamp('2022-06-01 04:02:41'), Timestamp('2022-06-01 04:06:22'), Timestamp('2022-06-01 04:09:11'), Timestamp('2022-06-01 04:19:57'), Timestamp('2022-06-01 03:49:28')]\n",
      "[Timestamp('2022-06-01 05:20:35'), Timestamp('2022-06-01 05:24:24'), Timestamp('2022-06-01 05:29:01'), Timestamp('2022-06-01 05:33:59'), Timestamp('2022-06-01 06:01:07'), Timestamp('2022-06-01 06:06:52'), Timestamp('2022-06-01 06:24:16'), Timestamp('2022-06-01 05:23:06')]\n",
      "[Timestamp('2022-06-01 05:20:35'), Timestamp('2022-06-01 05:24:25'), Timestamp('2022-06-01 05:29:01'), Timestamp('2022-06-01 05:33:59'), Timestamp('2022-06-01 06:01:07'), Timestamp('2022-06-01 06:06:52'), Timestamp('2022-06-01 06:24:16'), Timestamp('2022-06-01 05:23:06')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# Function to convert string timestamps to datetime objects\n",
    "def convert_to_datetime(timestamps):\n",
    "    return [pd.to_datetime(ts) for ts in timestamps]\n",
    "\n",
    "# Function to check if timestamps are consecutive\n",
    "def is_consecutive(timestamps):\n",
    "    # Generate combinations of pairs of timestamps\n",
    "    pairs = itertools.combinations(timestamps, 2)\n",
    "    # Check if all pairs are in increasing order\n",
    "    return all(pair[0] <= pair[1] for pair in pairs)\n",
    "\n",
    "# Function to find rows with non-consecutive timestamps\n",
    "def find_non_consecutive_rows(df):\n",
    "    non_consecutive_rows = []\n",
    "    count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        timestamps = [row[f't{i}'] for i in range(8) if not pd.isna(row[f't{i}'])]\n",
    "        timestamps = convert_to_datetime(timestamps)\n",
    "        if not is_consecutive(timestamps):\n",
    "            non_consecutive_rows.append(timestamps)\n",
    "            count += 1\n",
    "            if count == 10:\n",
    "                break\n",
    "    return non_consecutive_rows\n",
    "\n",
    "# Apply the processing to each dataset in all_interventions\n",
    "for i, dataset in enumerate(all_interventions):\n",
    "    non_consecutive_rows = find_non_consecutive_rows(dataset)\n",
    "    print(f\"Rows with non-consecutive timestamps in Dataset {i + 1}:\")\n",
    "    for row_values in non_consecutive_rows:\n",
    "        print(row_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# Function to convert string timestamps to datetime objects\n",
    "def convert_to_datetime(timestamps):\n",
    "    return [pd.to_datetime(ts) for ts in timestamps]\n",
    "\n",
    "# Function to check if timestamps are consecutive\n",
    "def is_consecutive(timestamps):\n",
    "    # Generate combinations of pairs of timestamps\n",
    "    pairs = itertools.combinations(timestamps, 2)\n",
    "    # Check if all pairs are in increasing order\n",
    "    return all(pair[0] <= pair[1] for pair in pairs)\n",
    "\n",
    "# Function to find the minimum number of timestamps to remove to make the sequence consecutive\n",
    "def min_timestamps_to_remove(timestamps):\n",
    "    min_removed_timestamps = None\n",
    "    min_removed_count = len(timestamps)\n",
    "    for r in range(1, len(timestamps)):  # Try removing 1 to len(timestamps)-1 timestamps\n",
    "        for combination in itertools.combinations(range(len(timestamps)), r):\n",
    "            reduced_timestamps = [timestamps[i] for i in range(len(timestamps)) if i not in combination]\n",
    "            if is_consecutive(reduced_timestamps):\n",
    "                return reduced_timestamps  # Stop if consecutive after removing just one timestamp\n",
    "            else:\n",
    "                removed_count = len(combination)\n",
    "                if removed_count < min_removed_count:\n",
    "                    min_removed_count = removed_count\n",
    "                    min_removed_timestamps = reduced_timestamps\n",
    "    return min_removed_timestamps\n",
    "\n",
    "# Function to process each row\n",
    "def process_row(row):\n",
    "    timestamps = [row[f't{i}'] for i in range(8) if not pd.isna(row[f't{i}'])]\n",
    "    timestamps = convert_to_datetime(timestamps)\n",
    "    if is_consecutive(timestamps):\n",
    "        return row\n",
    "    else:\n",
    "        corrected_timestamps = min_timestamps_to_remove(timestamps)\n",
    "        if corrected_timestamps is not None:\n",
    "            return pd.Series(corrected_timestamps, index=[f't{i}' for i in range(len(corrected_timestamps))])\n",
    "        else:\n",
    "            return row\n",
    "\n",
    "# Apply the processing to each dataset in other_interventions (VERY LONG RUNTIME = 113 MIN!)\n",
    "for dataset in all_interventions:\n",
    "    for index, row in dataset.iterrows():\n",
    "        dataset.loc[index, ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']] = process_row(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data set\n",
    "#df_interventions_bxl[['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']].head(20)\n",
    "#df_interventions_bxl2[['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']].head(20)\n",
    "#df_interventions1[['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']].head(20)\n",
    "#df_interventions2[['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']].head(20)\n",
    "#df_interventions3[['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']].head(20)\n",
    "#df_cad9[['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Timestamp objects to string before saving -> Parquet gives problems with timestamp objects\n",
    "df_interventions_bxl = df_interventions_bxl.astype(str)\n",
    "df_interventions_bxl2 = df_interventions_bxl2.astype(str)\n",
    "df_interventions1 = df_interventions1.astype(str)\n",
    "df_interventions2 = df_interventions2.astype(str)\n",
    "df_interventions3 = df_interventions3.astype(str)\n",
    "df_cad9 = df_cad9.astype(str)\n",
    "\n",
    "# Save data sets as a Parquet file\n",
    "df_interventions_bxl.to_parquet('../../1_Data/CLEANED/df_interventions_bxl.parquet')\n",
    "df_interventions_bxl2.to_parquet('../../1_Data/CLEANED/df_interventions_bxl2.parquet')\n",
    "df_interventions1.to_parquet('../../1_Data/CLEANED/df_interventions1.parquet')\n",
    "df_interventions2.to_parquet('../../1_Data/CLEANED/df_interventions2.parquet')\n",
    "df_interventions3.to_parquet('../../1_Data/CLEANED/df_interventions3.parquet')\n",
    "df_cad9.to_parquet('../../1_Data/CLEANED/df_cad9.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Path to your Parquet gzip file\n",
    "file_path_5 = '../../1_Data/CLEANED/df_interventions_bxl.parquet'\n",
    "file_path_6 = '../../1_Data/CLEANED/df_interventions_bxl2.parquet'\n",
    "file_path_7 = '../../1_Data/CLEANED/df_interventions1.parquet'\n",
    "file_path_8 = '../../1_Data/CLEANED/df_interventions2.parquet'\n",
    "file_path_9 = '../../1_Data/CLEANED/df_interventions3.parquet'\n",
    "file_path_10 = '../../1_Data/CLEANED/df_cad9.parquet'\n",
    "\n",
    "# Read the Parquet file into a pandas DataFrame\n",
    "df_interventions_bxl = pd.read_parquet(file_path_5, engine='pyarrow')\n",
    "df_interventions_bxl2 = pd.read_parquet(file_path_6, engine='pyarrow')\n",
    "df_interventions1 = pd.read_parquet(file_path_7, engine='pyarrow')\n",
    "df_interventions2 = pd.read_parquet(file_path_8, engine='pyarrow')\n",
    "df_interventions3 = pd.read_parquet(file_path_9, engine='pyarrow')\n",
    "df_cad9 = pd.read_parquet(file_path_10, engine='pyarrow')\n",
    "\n",
    "# Define your dataset\n",
    "all_datasets = [df_ambulance_locations, df_pit_locations, df_mug_locations, df_aed_locations, df_interventions_bxl, df_interventions_bxl2, df_interventions1, df_interventions2, df_interventions3, df_cad9]\n",
    "all_interventions = [df_interventions_bxl, df_interventions_bxl2, df_interventions1, df_interventions2, df_interventions3, df_cad9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         mission_id                          service_name  \\\n",
      "0       20222490011         FB PDS BRUX [PASI CitÈ] SIAMU   \n",
      "1       20222490011         FB PDS BRUX [PASI CitÈ] SIAMU   \n",
      "2       20222490012              HB UR BRUX CHU St Pierre   \n",
      "3       20222490015         FB PDS BRUX [PASI CitÈ] SIAMU   \n",
      "4       20222490019          FB PDS WOLL [PASI UCL] SIAMU   \n",
      "...             ...                                   ...   \n",
      "115642  20231510397         FB PDS BRUX [PASI CitÈ] SIAMU   \n",
      "115643  20231510397         FB PDS BRUX [PASI CitÈ] SIAMU   \n",
      "115644  20231510399   FB PDS ANDE [PASI Anderlecht] SIAMU   \n",
      "115645  20231510399   FB PDS ANDE [PASI Anderlecht] SIAMU   \n",
      "115646  20231510400  FB PDS BRUX [Caserne HÈliport] SIAMU   \n",
      "\n",
      "       postalcode_permanence                          cityname_permanence  \\\n",
      "0                       1000                            Brussel (Brussel)   \n",
      "1                       1000                            Brussel (Brussel)   \n",
      "2                       1000                            Brussel (Brussel)   \n",
      "3                       1000                            Brussel (Brussel)   \n",
      "4                       1200  Woluwe-Saint-Lambert (Woluwe-Saint-Lambert)   \n",
      "...                      ...                                          ...   \n",
      "115642                  1000                            Brussel (Brussel)   \n",
      "115643                  1000                            Brussel (Brussel)   \n",
      "115644                  1070                                   Anderlecht   \n",
      "115645                  1070                                   Anderlecht   \n",
      "115646                  1000                        Bruxelles (Bruxelles)   \n",
      "\n",
      "                      streetname_permanence housenumber_permanence  \\\n",
      "0                            Vesaliusstraat                   None   \n",
      "1                            Vesaliusstraat                   None   \n",
      "2                                 Rue Haute                   None   \n",
      "3                            Vesaliusstraat                   None   \n",
      "4       Avenue Hippocrate - Hippokrateslaan                   None   \n",
      "...                                     ...                    ...   \n",
      "115642                       Vesaliusstraat                   None   \n",
      "115643                       Vesaliusstraat                   None   \n",
      "115644                    Bergense Steenweg                   None   \n",
      "115645                    Bergense Steenweg                   None   \n",
      "115646                 Avenue de l'HÈliport                   None   \n",
      "\n",
      "       latitude_permanence longitude_permanence permanence_short_name  \\\n",
      "0                  5085097               436411             ABBRUX11A   \n",
      "1                  5085097               436411             ABBRUX03A   \n",
      "2                508343302             43454504             ABBRUX06A   \n",
      "3                  5085097               436411             ABBRUX03A   \n",
      "4                  5085211                44604             ABWOLL01A   \n",
      "...                    ...                  ...                   ...   \n",
      "115642             5085097               436411             ABBRUX10A   \n",
      "115643             5085097               436411             ABBRUX11A   \n",
      "115644             5083254               431199             ABANDE01A   \n",
      "115645             5083254               431199             ABANDE04A   \n",
      "115646             5085946               435181             ABBRUX04A   \n",
      "\n",
      "       permanence_long_name  ... number_of_transported_persons  \\\n",
      "0                AMB CITE 2  ...                           nan   \n",
      "1                AMB HELI 3  ...                           nan   \n",
      "2                 AMB HSP 1  ...                           nan   \n",
      "3                AMB HELI 3  ...                           nan   \n",
      "4                 AMB UCL 1  ...                           nan   \n",
      "...                     ...  ...                           ...   \n",
      "115642           AMB CITE 1  ...                           nan   \n",
      "115643           AMB CITE 2  ...                           nan   \n",
      "115644            AMB AND 1  ...                           nan   \n",
      "115645            AMB AND 4  ...                           nan   \n",
      "115646           AMB HELI 4  ...                           1.0   \n",
      "\n",
      "               abandon_reason t0_Hour t0_Day t0_Month t0_DayName t7_Hour  \\\n",
      "0                       Error       9      6        9    Tuesday     9.0   \n",
      "1                        None       9      6        9    Tuesday    10.0   \n",
      "2                       Error       9      6        9    Tuesday     9.0   \n",
      "3       Weigering van vervoer      10      6        9    Tuesday    11.0   \n",
      "4                 Geannuleerd      11      6        9    Tuesday    11.0   \n",
      "...                       ...     ...    ...      ...        ...     ...   \n",
      "115642  Weigering van vervoer      21     31        5  Wednesday    21.0   \n",
      "115643                  Error      21     31        5  Wednesday    21.0   \n",
      "115644                  Error      21     31        5  Wednesday    21.0   \n",
      "115645                   None      21     31        5  Wednesday    22.0   \n",
      "115646                   None      21     31        5  Wednesday    22.0   \n",
      "\n",
      "       t7_Day t7_Month t7_DayName  \n",
      "0         6.0      9.0    Tuesday  \n",
      "1         6.0      9.0    Tuesday  \n",
      "2         6.0      9.0    Tuesday  \n",
      "3         6.0      9.0    Tuesday  \n",
      "4         6.0      9.0    Tuesday  \n",
      "...       ...      ...        ...  \n",
      "115642   31.0      5.0  Wednesday  \n",
      "115643   31.0      5.0  Wednesday  \n",
      "115644   31.0      5.0  Wednesday  \n",
      "115645   31.0      5.0  Wednesday  \n",
      "115646   31.0      5.0  Wednesday  \n",
      "\n",
      "[115645 rows x 51 columns]\n",
      "        mission_id                          t0  \\\n",
      "0      20221520003  2022-06-01 00:02:45.000000   \n",
      "1      20221520005  2022-06-01 00:04:58.000000   \n",
      "2      20221520006  2022-06-01 00:07:43.000000   \n",
      "3      20221520006  2022-06-01 00:07:43.000000   \n",
      "4      20221520007  2022-06-01 00:09:18.000000   \n",
      "...            ...                         ...   \n",
      "38615  20222495211  2022-09-06 14:02:31.000000   \n",
      "38616  20222495213  2022-09-06 14:11:22.000000   \n",
      "38617  20222495214  2022-09-06 14:11:47.000000   \n",
      "38618  20222495214  2022-09-06 14:11:47.000000   \n",
      "38619  20222495215  2022-09-06 14:13:51.000000   \n",
      "\n",
      "                               cityname_intervention longitude_intervention  \\\n",
      "0                            schaerbeek (schaerbeek)          44067314597.0   \n",
      "1                            schaerbeek (schaerbeek)          43687983838.0   \n",
      "2                            koekelberg (koekelberg)          43323116697.0   \n",
      "3                            koekelberg (koekelberg)          43323116697.0   \n",
      "4                            schaerbeek (schaerbeek)          43836949556.0   \n",
      "...                                              ...                    ...   \n",
      "38615  saint-josse-ten-noode (saint-josse-ten-noode)          43692098773.0   \n",
      "38616                        anderlecht (anderlecht)          43028168071.0   \n",
      "38617                                  uccle (uccle)          43394384459.0   \n",
      "38618                                  uccle (uccle)          43394384459.0   \n",
      "38619                          bruxelles (bruxelles)          43896399138.0   \n",
      "\n",
      "      latitude_intervention number_of_transported_persons  \\\n",
      "0             50855162176.0                           nan   \n",
      "1             50863178373.0                           1.0   \n",
      "2             50857545689.0                           1.0   \n",
      "3             50857545689.0                           nan   \n",
      "4             50852246033.0                           nan   \n",
      "...                     ...                           ...   \n",
      "38615         50851438085.0                           nan   \n",
      "38616         50833989562.0                           nan   \n",
      "38617         50814874217.0                           1.0   \n",
      "38618         50814874217.0                           nan   \n",
      "38619         50841062093.0                           1.0   \n",
      "\n",
      "      permanence_long_name permanence_short_name        service_name  \\\n",
      "0              ZW STMICHEL            A-STMICHEL     ST-MICHEL (MED)   \n",
      "1                 ZW CITE2                A-CIT2          CITE (MED)   \n",
      "2                 ZW2 STAF                 A-EM2      Heliport (MED)   \n",
      "3                MUG HSP 1                 M-HSP           HSP (MED)   \n",
      "4                 ZW CITE1                A-CIT1          CITE (MED)   \n",
      "...                    ...                   ...                 ...   \n",
      "38615           MUG STJEAN                  M-SJ        STJEAN (MED)   \n",
      "38616              ZW AND4                A-AND4    Anderlecht (MED)   \n",
      "38617         ZW ELISABETH                 A-ELI  St-Elisabeth (MED)   \n",
      "38618            MUG HSP 1                 M-HSP           HSP (MED)   \n",
      "38619             ZW CITE1                A-CIT1          CITE (MED)   \n",
      "\n",
      "                cityname_permanence  ...  \\\n",
      "0        1040 etterbeek (etterbeek)  ...   \n",
      "1            1000 brussel (brussel)  ...   \n",
      "2            1000 brussel (brussel)  ...   \n",
      "3        1000 bruxelles (bruxelles)  ...   \n",
      "4            1000 brussel (brussel)  ...   \n",
      "...                             ...  ...   \n",
      "38615        1000 brussel (brussel)  ...   \n",
      "38616  1070 anderlecht (anderlecht)  ...   \n",
      "38617            1180 ukkel (ukkel)  ...   \n",
      "38618    1000 bruxelles (bruxelles)  ...   \n",
      "38619        1000 brussel (brussel)  ...   \n",
      "\n",
      "                                          eventtype_trip  \\\n",
      "0                                          P033 - Trauma   \n",
      "1                              P032 - Allergic reactions   \n",
      "2                            P010 - Respiratory problems   \n",
      "3                            P010 - Respiratory problems   \n",
      "4      P039 - Cardiac problem (other than thoracic pain)   \n",
      "...                                                  ...   \n",
      "38615                                  P011 - Chest pain   \n",
      "38616                              P096 - Out of service   \n",
      "38617                      P015 - Epilepsy - convulsions   \n",
      "38618                      P015 - Epilepsy - convulsions   \n",
      "38619                                      P033 - Trauma   \n",
      "\n",
      "            eventlevel_trip t0_Hour t0_Day t0_Month t0_DayName t7_Hour t7_Day  \\\n",
      "0                        N5       0      1        6  Wednesday     0.0    1.0   \n",
      "1                        N5       0      1        6  Wednesday     1.0    1.0   \n",
      "2                        N1       0      1        6  Wednesday     NaN    NaN   \n",
      "3                        N1       0      1        6  Wednesday     NaN    NaN   \n",
      "4                        N5       0      1        6  Wednesday     0.0    1.0   \n",
      "...                     ...     ...    ...      ...        ...     ...    ...   \n",
      "38615                    N1      14      6        9    Tuesday     NaN    NaN   \n",
      "38616  Buitendienststelling      14      6        9    Tuesday    15.0    6.0   \n",
      "38617                    N1      14      6        9    Tuesday     NaN    NaN   \n",
      "38618                    N1      14      6        9    Tuesday     NaN    NaN   \n",
      "38619                    N5      14      6        9    Tuesday    15.0    6.0   \n",
      "\n",
      "      t7_Month t7_DayName  \n",
      "0          6.0  Wednesday  \n",
      "1          6.0  Wednesday  \n",
      "2          NaN        NaN  \n",
      "3          NaN        NaN  \n",
      "4          6.0  Wednesday  \n",
      "...        ...        ...  \n",
      "38615      NaN        NaN  \n",
      "38616      9.0    Tuesday  \n",
      "38617      NaN        NaN  \n",
      "38618      NaN        NaN  \n",
      "38619      9.0    Tuesday  \n",
      "\n",
      "[36710 rows x 37 columns]\n",
      "         mission_id                 service_name postalcode_permanence  \\\n",
      "0       10221520001     HA UR MECH AZ St Maarten                2800.0   \n",
      "1       10221520002               BA KAPE AMBUCE                2950.0   \n",
      "2       10221520002       HA UR ANTW Stuivenberg                2060.0   \n",
      "3       10221520004  BA ANTW [Borgerhout] AMBUCE                2140.0   \n",
      "4       10221520005               BA WIJN AMBUCE                2110.0   \n",
      "...             ...                          ...                   ...   \n",
      "200622  50221920082      FH PDS BEAU Hainaut Est                6500.0   \n",
      "200623  50221920083            BH LOUV AS Grande                7110.0   \n",
      "200624  50221920084  BH GERP SAPG Poste Loverval                6280.0   \n",
      "200625  50221920085   FH PDS BINC Hainaut Centre                7130.0   \n",
      "200626  50221920085        HH UR LOUV CHU Tivoli                7100.0   \n",
      "\n",
      "                        cityname_permanence       streetname_permanence  \\\n",
      "0                       Mechelen (Mechelen)              Liersesteenweg   \n",
      "1                       Kapellen (Kapellen)             Essenhoutstraat   \n",
      "2                     Antwerpen (Antwerpen)      Lange Beeldekensstraat   \n",
      "3                    Antwerpen (Borgerhout)               Gijselsstraat   \n",
      "4                       Wijnegem (Wijnegem)               Bijkhoevelaan   \n",
      "...                                     ...                         ...   \n",
      "200622                  Beaumont (Beaumont)           Chauss√©e de Mons   \n",
      "200623  La Louvi√®re (Str√©py-Bracquegnies)             Rue de Nivelles   \n",
      "200624                 Gerpinnes (Loverval)  Chauss√©e de Philippeville   \n",
      "200625                      Binche (Binche)       Rue de la P√©pini√®re   \n",
      "200626          La Louvi√®re (La Louvi√®re)            Avenue Max Buset   \n",
      "\n",
      "       housenumber_permanence latitude_permanence longitude_permanence  \\\n",
      "0                        None            51.05102              4.47803   \n",
      "1                        None    51.3120753246937     4.42439817975528   \n",
      "2                        None            51.22249              4.43629   \n",
      "3                        None            51.21562              4.44392   \n",
      "4                        None            51.23355              4.49318   \n",
      "...                       ...                 ...                  ...   \n",
      "200622                   None            50.24187              4.23468   \n",
      "200623                   None    50.4697581691705     4.12800814431491   \n",
      "200624                   None            50.37336              4.46745   \n",
      "200625                    57A            50.41523                  nan   \n",
      "200626                   None             50.4778              4.20512   \n",
      "\n",
      "       permanence_short_name permanence_long_name  ...  \\\n",
      "0                  AAMECH01A        ZW MECHELEN 1  ...   \n",
      "1                  AAKAPE01A        ZW KAPELLEN 1  ...   \n",
      "2                  UAANTW01A      MUG ANTWERPEN 1  ...   \n",
      "3                  AAANTW07A       ZW ANTWERPEN 7  ...   \n",
      "4                  AAWIJN01A        ZW WIJNEGEM 1  ...   \n",
      "...                      ...                  ...  ...   \n",
      "200622             AHBEAU01A        ZS BEAUMONT 1  ...   \n",
      "200623             AHLOUV03A         AMB STREPY 1  ...   \n",
      "200624             AHGERP01A       AMB LOVERVAL 1  ...   \n",
      "200625             AHBINC01A          ZS BINCHE 1  ...   \n",
      "200626             UHLOUV02A     SMUR LOUV TIVOLI  ...   \n",
      "\n",
      "       number_of_transported_persons        abandon_reason t0_Hour t0_Day  \\\n",
      "0                                nan  Verzorgd ter plaatse       0      1   \n",
      "1                                1.0                  None       0      1   \n",
      "2                                nan                  None       0      1   \n",
      "3                                nan        Zonder patient       0      1   \n",
      "4                                1.0                  None       0      1   \n",
      "...                              ...                   ...     ...    ...   \n",
      "200622                           1.0                  None       6     11   \n",
      "200623                           1.0                  None       7     11   \n",
      "200624                           1.0                  None       7     11   \n",
      "200625                           1.0                  None       7     11   \n",
      "200626                           nan                  None       7     11   \n",
      "\n",
      "       t0_Month t0_DayName t7_Hour t7_Day t7_Month t7_DayName  \n",
      "0             6  Wednesday     0.0    1.0      6.0  Wednesday  \n",
      "1             6  Wednesday     1.0    1.0      6.0  Wednesday  \n",
      "2             6  Wednesday     1.0    1.0      6.0  Wednesday  \n",
      "3             6  Wednesday     0.0    1.0      6.0  Wednesday  \n",
      "4             6  Wednesday     NaN    NaN      NaN        NaN  \n",
      "...         ...        ...     ...    ...      ...        ...  \n",
      "200622        7     Monday     8.0   11.0      7.0     Monday  \n",
      "200623        7     Monday     8.0   11.0      7.0     Monday  \n",
      "200624        7     Monday     8.0   11.0      7.0     Monday  \n",
      "200625        7     Monday     8.0   11.0      7.0     Monday  \n",
      "200626        7     Monday     7.0   11.0      7.0     Monday  \n",
      "\n",
      "[200627 rows x 54 columns]\n",
      "         mission_id                       service_name postalcode_permanence  \\\n",
      "0       50221920087       FH PDS COMI Wallonie Picarde                7784.0   \n",
      "1       50221920088       FH PDS TOUR Wallonie Picarde                7500.0   \n",
      "2       50221920089         FH PDS LOUV Hainaut Centre                7100.0   \n",
      "3       50221920090  FW HVP HEUV [Nieuwkerke] Westhoek                8950.0   \n",
      "4       50221920092                BH SGHI Croix Rouge                7331.0   \n",
      "...             ...                                ...                   ...   \n",
      "200622  60230050005                BG BLEG Croix Rouge                4671.0   \n",
      "200623  60230050006                      BG AWAN PARAM                4340.0   \n",
      "200624  60230050006           HG UR LIEG CHC Montlegia                4000.0   \n",
      "200625  60230050007                BG LIEG Croix Rouge                4000.0   \n",
      "200626  60230050008                BG OUPE Croix Rouge                4680.0   \n",
      "\n",
      "                cityname_permanence              streetname_permanence  \\\n",
      "0       Comines-Warneton (Warneton)                  Chauss√©e d'Ypres   \n",
      "1                 Tournai (Tournai)                    Avenue de Maire   \n",
      "2       La Louvi√®re (La Louvi√®re)          Boulevard du Roi Baudouin   \n",
      "3           Heuvelland (Nieuwkerke)                    Dranouterstraat   \n",
      "4          Saint-Ghislain (Baudour)                     Rue Louis Caty   \n",
      "...                             ...                                ...   \n",
      "200622             Blegny (Barchon)                   Rue Pr√©s-Champs   \n",
      "200623         4342 awans (hognoul)                rue de la chauss√©e   \n",
      "200624               Li√®ge (Glain)  Boulevard de Patience et Beaujonc   \n",
      "200625              Li√®ge (Li√®ge)                        Rue Darchis   \n",
      "200626              Oupeye (Oupeye)                  Rue du Roi Albert   \n",
      "\n",
      "       housenumber_permanence latitude_permanence longitude_permanence  \\\n",
      "0                        None            50.76034              2.94023   \n",
      "1                        None            50.61652              3.37512   \n",
      "2                        None              50.468              4.19217   \n",
      "3                        None            50.74621               2.8225   \n",
      "4                        None    50.4696295383992     3.84255870289547   \n",
      "...                       ...                 ...                  ...   \n",
      "200622                   None            50.66292               5.6887   \n",
      "200623                   None    50.6806111429557     5.46905963987132   \n",
      "200624                   None            50.64698              5.53601   \n",
      "200625                   None    50.6373572421741     5.56655061117401   \n",
      "200626                   None    50.7028414885518     5.65266847991943   \n",
      "\n",
      "       permanence_short_name permanence_long_name  ...  \\\n",
      "0                  AHCOMI01A         ZS COMINES 1  ...   \n",
      "1                  AHTOUR01A         ZS TOURNAI 1  ...   \n",
      "2                  AHLOUV02A   ZS LA LOUVIERE PIM  ...   \n",
      "3                  AWHEUV01A        ZW NIEUWKERKE  ...   \n",
      "4                  AHSGHI01A  CR SAINT-GHISLAIN 1  ...   \n",
      "...                      ...                  ...  ...   \n",
      "200622             AGBLEG01A          CR.BLEGNY 1  ...   \n",
      "200623             AGAWAN01A           PR.PARAM 1  ...   \n",
      "200624             UGLIEG07A      SMUR MT LEGIA 1  ...   \n",
      "200625             AGLIEG04A           CR LIEGE 1  ...   \n",
      "200626             AGOUPE01A          CR.OUPEYE 1  ...   \n",
      "\n",
      "       number_of_transported_persons abandon_reason t0_Hour t0_Day t0_Month  \\\n",
      "0                                1.0           None       7     11        7   \n",
      "1                                1.0           None       7     11        7   \n",
      "2                                1.0           None       7     11        7   \n",
      "3                                nan           None       7     11        7   \n",
      "4                                1.0           None       7     11        7   \n",
      "...                              ...            ...     ...    ...      ...   \n",
      "200622                           1.0           None       1      5        1   \n",
      "200623                           1.0           None       1      5        1   \n",
      "200624                           nan           None       1      5        1   \n",
      "200625                           1.0           None       1      5        1   \n",
      "200626                           1.0           None       1      5        1   \n",
      "\n",
      "       t0_DayName t7_Hour t7_Day t7_Month t7_DayName  \n",
      "0          Monday     9.0   11.0      7.0     Monday  \n",
      "1          Monday     8.0   11.0      7.0     Monday  \n",
      "2          Monday     9.0   11.0      7.0     Monday  \n",
      "3          Monday    17.0   11.0      7.0     Monday  \n",
      "4          Monday     9.0   11.0      7.0     Monday  \n",
      "...           ...     ...    ...      ...        ...  \n",
      "200622   Thursday     1.0    5.0      1.0   Thursday  \n",
      "200623   Thursday     2.0    5.0      1.0   Thursday  \n",
      "200624   Thursday     1.0    5.0      1.0   Thursday  \n",
      "200625   Thursday     2.0    5.0      1.0   Thursday  \n",
      "200626   Thursday     2.0    5.0      1.0   Thursday  \n",
      "\n",
      "[200627 rows x 54 columns]\n",
      "         mission_id                        service_name postalcode_permanence  \\\n",
      "0       60230050009                 BG LIEG Croix Rouge                4000.0   \n",
      "1       60230050010  FG PDS PEPI Vesdre Ho√´gne Plateau                4860.0   \n",
      "2       60230050012                    BG LIEG Courtois                4030.0   \n",
      "3       60230050013             FG PDS ANS_ Li√®ge IILE                4000.0   \n",
      "4       60230050014             FG PDS FLEM Li√®ge IILE                4400.0   \n",
      "...             ...                                 ...                   ...   \n",
      "200622  90231510181                    FN PDS NAMU NAGE                5100.0   \n",
      "200623  90231510182                 FN PDS DINA DINAPHI                5500.0   \n",
      "200624  90231510183          FF PDS TUBI Brabant Wallon                1480.0   \n",
      "200625  90231510184                    FN PDS NAMU NAGE                5100.0   \n",
      "200626  90231510185                 BN NAMU Croix Rouge                5002.0   \n",
      "\n",
      "                cityname_permanence  streetname_permanence  \\\n",
      "0                   Li√®ge (Li√®ge)            Rue Darchis   \n",
      "1             Pepinster (Pepinster)         Rue Jean Simon   \n",
      "2               Li√®ge (Grivegn√©e)            Rue Belvaux   \n",
      "3                    li√®ge (glain)  p537;rue jean jaur√®s   \n",
      "4       Fl√©malle (Fl√©malle-Haute)            Grand'Route   \n",
      "...                             ...                    ...   \n",
      "200622               Namur (Jambes)      ChaussÈe de LiËge   \n",
      "200623              Dinant (Dinant)   Rue de Philippeville   \n",
      "200624              Tubize (Tubize)             Rue Ferrer   \n",
      "200625               Namur (Jambes)      ChaussÈe de LiËge   \n",
      "200626        Namur (Saint-Servais)     Rue de l'Industrie   \n",
      "\n",
      "       housenumber_permanence latitude_permanence longitude_permanence  \\\n",
      "0                        None    50.6373572421741     5.56655061117401   \n",
      "1                        None            50.56594              5.80254   \n",
      "2                        None     50.619827507778     5.60246993993783   \n",
      "3                        None            50.65224              5.52641   \n",
      "4                        None            50.59936              5.47213   \n",
      "...                       ...                 ...                  ...   \n",
      "200622                  55/57                 nan              488.008   \n",
      "200623                   None                 nan              489.808   \n",
      "200624                   None                 nan              419.797   \n",
      "200625                  55/57                 nan              488.008   \n",
      "200626                   None                 nan              484.735   \n",
      "\n",
      "       permanence_short_name permanence_long_name  ...  \\\n",
      "0                  AGLIEG05A           CR LIEGE 2  ...   \n",
      "1                  AGPEPI01A         ZS.PEPINST 1  ...   \n",
      "2                  AGLIEG02A            PR.COURT2  ...   \n",
      "3                  AGANS_01A             ZS ANS 1  ...   \n",
      "4                  AGFLEM01A        ZS FLEMALLE 1  ...   \n",
      "...                      ...                  ...  ...   \n",
      "200622             ANNAMU01A           ZS NAMUR 1  ...   \n",
      "200623             ANDINA01A          ZS DINANT 1  ...   \n",
      "200624             AFTUBI02A          ZS TUBIZE 2  ...   \n",
      "200625             ANNAMU01A           ZS NAMUR 1  ...   \n",
      "200626             ANNAMU04A           CR NAMUR 4  ...   \n",
      "\n",
      "       number_of_transported_persons         abandon_reason t0_Hour t0_Day  \\\n",
      "0                                nan  Weigering van vervoer       1      5   \n",
      "1                                1.0                   None       1      5   \n",
      "2                                1.0                   None       1      5   \n",
      "3                                nan   Verzorgd ter plaatse       1      5   \n",
      "4                                1.0                   None       1      5   \n",
      "...                              ...                    ...     ...    ...   \n",
      "200622                           1.0                   None      22     31   \n",
      "200623                           nan         Zonder patient      22     31   \n",
      "200624                           nan  Weigering van vervoer      23     31   \n",
      "200625                           nan   Verzorgd ter plaatse      23     31   \n",
      "200626                           1.0                   None      23     31   \n",
      "\n",
      "       t0_Month t0_DayName t7_Hour t7_Day t7_Month t7_DayName  \n",
      "0             1   Thursday     1.0    5.0      1.0   Thursday  \n",
      "1             1   Thursday     3.0    5.0      1.0   Thursday  \n",
      "2             1   Thursday     2.0    5.0      1.0   Thursday  \n",
      "3             1   Thursday     2.0    5.0      1.0   Thursday  \n",
      "4             1   Thursday     2.0    5.0      1.0   Thursday  \n",
      "...         ...        ...     ...    ...      ...        ...  \n",
      "200622        5  Wednesday     NaN    NaN      NaN        NaN  \n",
      "200623        5  Wednesday     NaN    NaN      NaN        NaN  \n",
      "200624        5  Wednesday     NaN    NaN      NaN        NaN  \n",
      "200625        5  Wednesday     NaN    NaN      NaN        NaN  \n",
      "200626        5  Wednesday     NaN    NaN      NaN        NaN  \n",
      "\n",
      "[200627 rows x 54 columns]\n",
      "         mission_id                 service_name latitude_permanence  \\\n",
      "0       21221520003             MV HVP VILV West         50.92527651   \n",
      "1       21221520004             MV HVP HALL West         50.74320027   \n",
      "2       21221520007             MV HVP VILV West         50.92527651   \n",
      "3       21221520007   HV UR VILV AZ Jan Portaels         50.92686858   \n",
      "4       21221520008  BB BRUX Hôpital Militair KA         50.90533142   \n",
      "...             ...                          ...                 ...   \n",
      "289396  90221870001             BF SART BHN AMBU                 nan   \n",
      "289397  90221870002   MF PDS BRAL Brabant Wallon         50.68635135   \n",
      "289398  90221870003   MF PDS WAVR Brabant Wallon         50.69898453   \n",
      "289399  90221880001             BF SART BHN AMBU                 nan   \n",
      "289400  90222280001      MH PDS BEAU Hainaut Est                 nan   \n",
      "\n",
      "       longitude_permanence permanence_short_name      permanence_long_name  \\\n",
      "0               4.423057226             AVVILV01A            ZW VILVOORDE 1   \n",
      "1               4.241053204             AVHALL02A                ZW HALLE 2   \n",
      "2               4.423057226             AVVILV01A            ZW VILVOORDE 1   \n",
      "3               4.420968223             UVVILV01A             MUG VILVOORDE   \n",
      "4               4.387661794             ABBRUX13A                AMB HMB 13   \n",
      "...                     ...                   ...                       ...   \n",
      "289396                  nan             AFSART02A  AMB SART-DAME-AVELINES 2   \n",
      "289397          4.395891638             AFBRAL03A     AMB BRAINE-L'ALLEUD 3   \n",
      "289398          4.615384751             AFWAVR01A               AMB WAVRE 1   \n",
      "289399                  nan             AFSART01A  AMB SART-DAME-AVELINES 1   \n",
      "289400                  nan             AHBEAU01A            AMB BEAUMONT 1   \n",
      "\n",
      "       vector_type               eventtype_trip eventlevel_trip  \\\n",
      "0        Ambulance          P034 - Skull trauma              N5   \n",
      "1        Ambulance  P010 - Respiratory problems              N5   \n",
      "2        Ambulance                         Y_TI               T   \n",
      "3              MUG                         Y_TI               T   \n",
      "4        Ambulance  P020 - Intoxication alcohol              N5   \n",
      "...            ...                          ...             ...   \n",
      "289396   Ambulance                    PERSONNES              EU   \n",
      "289397   Ambulance                    PERSONNES              EU   \n",
      "289398   Ambulance                    PERSONNES              EU   \n",
      "289399   Ambulance                    PERSONNES              EU   \n",
      "289400   Ambulance                    PERSONNES              EU   \n",
      "\n",
      "       cityname_intervention  ... departure_time_(t1reported)  \\\n",
      "0                   MACHELEN  ...                         2.0   \n",
      "1                    BEERSEL  ...                         3.0   \n",
      "2                  VILVOORDE  ...                         4.0   \n",
      "3                  VILVOORDE  ...                         1.0   \n",
      "4                   MACHELEN  ...                         7.0   \n",
      "...                      ...  ...                         ...   \n",
      "289396                  None  ...                         nan   \n",
      "289397                  None  ...                         nan   \n",
      "289398                  None  ...                         nan   \n",
      "289399             CHARLEROI  ...                         nan   \n",
      "289400                  None  ...                         nan   \n",
      "\n",
      "       departure_time_(t1confirmed) t0_Hour t0_Day t0_Month t0_DayName  \\\n",
      "0                               nan     0.0    1.0      6.0  Wednesday   \n",
      "1                               nan     0.0    1.0      6.0  Wednesday   \n",
      "2                               nan     1.0    1.0      6.0  Wednesday   \n",
      "3                               nan     1.0    1.0      6.0  Wednesday   \n",
      "4                               nan     1.0    1.0      6.0  Wednesday   \n",
      "...                             ...     ...    ...      ...        ...   \n",
      "289396                          nan     NaN    NaN      NaN        NaN   \n",
      "289397                          nan     NaN    NaN      NaN        NaN   \n",
      "289398                          nan     NaN    NaN      NaN        NaN   \n",
      "289399                          nan     NaN    NaN      NaN        NaN   \n",
      "289400                          nan     NaN    NaN      NaN        NaN   \n",
      "\n",
      "       t7_Hour t7_Day t7_Month t7_DayName  \n",
      "0          NaN    NaN      NaN        NaN  \n",
      "1          NaN    NaN      NaN        NaN  \n",
      "2          NaN    NaN      NaN        NaN  \n",
      "3          NaN    NaN      NaN        NaN  \n",
      "4          NaN    NaN      NaN        NaN  \n",
      "...        ...    ...      ...        ...  \n",
      "289396     NaN    NaN      NaN        NaN  \n",
      "289397     NaN    NaN      NaN        NaN  \n",
      "289398     NaN    NaN      NaN        NaN  \n",
      "289399     NaN    NaN      NaN        NaN  \n",
      "289400     NaN    NaN      NaN        NaN  \n",
      "\n",
      "[280563 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the desired date format\n",
    "date_format_2 = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "\n",
    "# Columns for which to add Hour, Day, Month, Name of the Day column\n",
    "date_columns_extra = ['t0', 't7']\n",
    "\n",
    "# Apply the function to each dataset and column\n",
    "for dataset in all_interventions:\n",
    "    for column in date_columns_extra:\n",
    "        # Replace 'None' with NaN\n",
    "        dataset[column] = dataset[column].replace('None', pd.NaT)\n",
    "        # Convert to datetime with mixed format inference\n",
    "        dataset[column] = pd.to_datetime(dataset[column], utc=True, format='mixed')\n",
    "        dataset[f'{column}_Hour'] = dataset[column].dt.hour\n",
    "        dataset[f'{column}_Day'] = dataset[column].dt.day\n",
    "        dataset[f'{column}_Month'] = dataset[column].dt.month\n",
    "        dataset[f'{column}_DayName'] = dataset[column].dt.day_name()\n",
    "        # Convert datetime column back to the desired string format\n",
    "        dataset[column] = dataset[column].dt.strftime(date_format_2)\n",
    "\n",
    "# Example usage:\n",
    "for dataset in all_interventions:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_interventions_bxl: Intervention - Inside Belgium: 0, Outside Belgium: 115645, NaNs: 0\n",
      "df_interventions_bxl: Permanence - Inside Belgium: 0, Outside Belgium: 115645, NaNs: 0\n",
      "df_interventions_bxl2: Intervention - Inside Belgium: 0, Outside Belgium: 36710, NaNs: 0\n",
      "df_interventions_bxl2: Permanence - Inside Belgium: 0, Outside Belgium: 36710, NaNs: 0\n",
      "df_interventions1: Intervention - Inside Belgium: 167165, Outside Belgium: 33462, NaNs: 0\n",
      "df_interventions1: Permanence - Inside Belgium: 166719, Outside Belgium: 33908, NaNs: 0\n",
      "df_interventions2: Intervention - Inside Belgium: 176437, Outside Belgium: 24190, NaNs: 0\n",
      "df_interventions2: Permanence - Inside Belgium: 173871, Outside Belgium: 26756, NaNs: 0\n",
      "df_interventions3: Intervention - Inside Belgium: 157142, Outside Belgium: 43485, NaNs: 0\n",
      "df_interventions3: Permanence - Inside Belgium: 156968, Outside Belgium: 43659, NaNs: 0\n",
      "df_cad9: Intervention - Inside Belgium: 280551, Outside Belgium: 12, NaNs: 0\n",
      "df_cad9: Permanence - Inside Belgium: 230212, Outside Belgium: 50351, NaNs: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the boundaries of Belgium\n",
    "belgium_longitude_range = (2.5, 6.3)\n",
    "belgium_latitude_range = (49.5, 51.5)\n",
    "\n",
    "# Define a function to check if coordinates are within the Belgium boundaries\n",
    "def is_within_belgium(longitude, latitude):\n",
    "    return belgium_longitude_range[0] <= longitude <= belgium_longitude_range[1] and \\\n",
    "           belgium_latitude_range[0] <= latitude <= belgium_latitude_range[1]\n",
    "\n",
    "# Check longitude and latitude in each dataset\n",
    "for dataset_name, dataset in zip(['df_interventions_bxl', 'df_interventions_bxl2', 'df_interventions1',\n",
    "                                  'df_interventions2', 'df_interventions3', 'df_cad9'], all_interventions):\n",
    "    # Initialize counters for coordinates inside and outside Belgium for intervention and permanence\n",
    "    inside_intervention_count = 0\n",
    "    outside_intervention_count = 0\n",
    "    inside_permanence_count = 0\n",
    "    outside_permanence_count = 0\n",
    "    nan_count_intervention = 0\n",
    "    nan_count_permanence = 0\n",
    "    \n",
    "    # Check if longitude and latitude are within Belgium boundaries for intervention\n",
    "    for _, row in dataset.iterrows():\n",
    "        if pd.isna(row['longitude_intervention']) or pd.isna(row['latitude_intervention']):\n",
    "            nan_count_intervention += 1\n",
    "        else:\n",
    "            longitude = float(row['longitude_intervention'])\n",
    "            latitude = float(row['latitude_intervention'])\n",
    "            if is_within_belgium(longitude, latitude):\n",
    "                inside_intervention_count += 1\n",
    "            else:\n",
    "                outside_intervention_count += 1\n",
    "    \n",
    "    # Check if longitude and latitude are within Belgium boundaries for permanence\n",
    "    for _, row in dataset.iterrows():\n",
    "        if pd.isna(row['longitude_permanence']) or pd.isna(row['latitude_permanence']):\n",
    "            nan_count_permanence += 1\n",
    "        else:\n",
    "            longitude = float(row['longitude_permanence'])\n",
    "            latitude = float(row['latitude_permanence'])\n",
    "            if is_within_belgium(longitude, latitude):\n",
    "                inside_permanence_count += 1\n",
    "            else:\n",
    "                outside_permanence_count += 1\n",
    "    \n",
    "    # Print the result for each dataset\n",
    "    print(f\"{dataset_name}: Intervention - Inside Belgium: {inside_intervention_count}, Outside Belgium: {outside_intervention_count}, NaNs: {nan_count_intervention}\")\n",
    "    print(f\"{dataset_name}: Permanence - Inside Belgium: {inside_permanence_count}, Outside Belgium: {outside_permanence_count}, NaNs: {nan_count_permanence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check format\n",
    "#df_interventions_bxl.longitude_intervention\n",
    "#df_interventions_bxl.latitude_intervention\n",
    "#df_interventions_bxl.longitude_permanence\n",
    "#df_interventions_bxl.latitude_permanence\n",
    "#df_interventions_bxl2.longitude_intervention\n",
    "#df_interventions_bxl2.latitude_intervention\n",
    "#df_interventions_bxl2.longitude_permanence\n",
    "#df_interventions_bxl2.latitude_permanence\n",
    "#df_interventions3.longitude_intervention\n",
    "#df_interventions3.latitude_intervention\n",
    "#df_interventions3.longitude_permanence\n",
    "#df_interventions3.latitude_permanence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to convert longitude and latitude within the ranges of Belgium\n",
    "def loc_convert(l, max_l):\n",
    "    l = float(l)\n",
    "    while l > max_l:\n",
    "        l /= 10\n",
    "    return l\n",
    "\n",
    "# Define the latitude and longitude ranges of Belgium\n",
    "belgium_longitude_range = (2.5, 6.3)\n",
    "belgium_latitude_range = (49.5, 51.5)\n",
    "\n",
    "# Adjust mislabelled lat/lon values and remove values outside the range for each DataFrame\n",
    "for dataset in all_interventions:\n",
    "    for col in ['longitude_intervention', 'latitude_intervention', 'longitude_permanence', 'latitude_permanence']:\n",
    "        dataset[col] = dataset[col].apply(loc_convert, args=(belgium_longitude_range[1],) if 'longitude' in col else (belgium_latitude_range[1],))\n",
    "        dataset[col] = dataset[col].where((dataset[col] >= belgium_longitude_range[0]) & (dataset[col] <= belgium_longitude_range[1]) if 'longitude' in col else (dataset[col] >= belgium_latitude_range[0]) & (dataset[col] <= belgium_latitude_range[1]), np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the country codes to be removed\n",
    "country_codes = ['NLD', 'DEU', 'FRA', 'LUX']\n",
    "\n",
    "# Iterate over each DataFrame in all_interventions\n",
    "for dataset in all_interventions:\n",
    "    # Create a boolean mask for rows where cityname_intervention starts with any of the country codes\n",
    "    mask = dataset['cityname_intervention'].str.startswith(tuple(country_codes), na=False)\n",
    "    # Remove the rows where the mask is True\n",
    "    dataset = dataset[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all datasets into one\n",
    "interventions_dataset = pd.concat(all_interventions, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numeric characters\n",
    "interventions_dataset['housenumber_permanence'] = interventions_dataset['housenumber_permanence'].str.extract('(\\d+)')\n",
    "interventions_dataset['postalcode_intervention'] = interventions_dataset['postalcode_intervention'].str.extract('(\\d+)')\n",
    "interventions_dataset['postalcode_destination_hospital'] = interventions_dataset['postalcode_destination_hospital'].str.extract('(\\d+)')\n",
    "interventions_dataset['housenumber_destination_hospital'] = interventions_dataset['housenumber_destination_hospital'].str.extract('(\\d+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"AMB\" with \"Ambulance\" in the \"vector_type\" column\n",
    "interventions_dataset['vector_type'] = interventions_dataset['vector_type'].replace('AMB', 'Ambulance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map the old values to the new values\n",
    "replacement_map = {\n",
    "    \"Brandziekenwagen\": \"Fire ambulance\",\n",
    "    \"Decontanimatieziekenwagen\": \"Decontamination ambulance\"\n",
    "}\n",
    "\n",
    "# Replace values in the \"vector_type\" column\n",
    "interventions_dataset['vector_type'] = interventions_dataset['vector_type'].replace(replacement_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary for translation\n",
    "translation_mapping = {\n",
    "    'Weigering van vervoer': 'Refusal of transport',\n",
    "    'Geannuleerd': 'Cancelled',\n",
    "    'Zonder patient': 'Without patient',\n",
    "    'Vervoerd door politie': 'Transported by police',\n",
    "    'Kwaadwillig': 'Malicious',\n",
    "    'Verzorgd ter plaatse': 'Cared for on-site',\n",
    "    'Overleden': 'Deceased',\n",
    "    'Vervoerd door derden': 'Transported by third parties',\n",
    "    'Weigering vervoer': 'Refusal of transport',\n",
    "    'Kwaadwillige oproep': 'Malicious call',\n",
    "    'Geannuleerde rit': 'Cancelled trip',\n",
    "    'Loos alarm goed bedoeld': 'False alarm well-intentioned',\n",
    "    'Vervoer door derden': 'Transport by third parties',\n",
    "    'Dood Ter Plaatse': 'Dead On Site',\n",
    "    'Ter plaatste behandeld': 'Treated on site'\n",
    "}\n",
    "\n",
    "# Replace values using the mapping dictionary\n",
    "interventions_dataset['abandon_reason'] = interventions_dataset['abandon_reason'].replace(translation_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"None\" values with NaN\n",
    "interventions_dataset.replace(\"None\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3: 6 datasets\n",
      "longitude_intervention: 6 datasets\n",
      "service_name: 6 datasets\n",
      "permanence_short_name: 6 datasets\n",
      "t5: 6 datasets\n",
      "longitude_permanence: 6 datasets\n",
      "t2: 6 datasets\n",
      "latitude_permanence: 6 datasets\n",
      "vector_type: 6 datasets\n",
      "t7_Hour: 6 datasets\n",
      "eventlevel_trip: 6 datasets\n",
      "name_destination_hospital: 6 datasets\n",
      "eventtype_trip: 6 datasets\n",
      "t0_Month: 6 datasets\n",
      "t7: 6 datasets\n",
      "mission_id: 6 datasets\n",
      "permanence_long_name: 6 datasets\n",
      "t0_DayName: 6 datasets\n",
      "t0_Hour: 6 datasets\n",
      "t7_Day: 6 datasets\n",
      "t7_DayName: 6 datasets\n",
      "t0: 6 datasets\n",
      "t7_Month: 6 datasets\n",
      "t6: 6 datasets\n",
      "cityname_intervention: 6 datasets\n",
      "t1: 6 datasets\n",
      "latitude_intervention: 6 datasets\n",
      "t4: 6 datasets\n",
      "t0_Day: 6 datasets\n",
      "streetname_permanence: 5 datasets\n",
      "cityname_destination_hospital: 5 datasets\n",
      "cityname_permanence: 5 datasets\n",
      "streetname_destination_hospital: 5 datasets\n",
      "housenumber_destination_hospital: 5 datasets\n",
      "number_of_transported_persons: 5 datasets\n",
      "abandon_reason: 5 datasets\n",
      "departure_time_(t1reported): 5 datasets\n",
      "intervention_time_(t1reported): 5 datasets\n",
      "t1confirmed: 5 datasets\n",
      "housenumber_permanence: 5 datasets\n",
      "eventtype_firstcall: 4 datasets\n",
      "postalcode_permanence: 4 datasets\n",
      "calculated_distance_destination: 4 datasets\n",
      "eventlevel_firstcall: 4 datasets\n",
      "intervention_duration: 4 datasets\n",
      "unavailable_time: 4 datasets\n",
      "waiting_time: 4 datasets\n",
      "postalcode_intervention: 4 datasets\n",
      "calculated_traveltime_destinatio: 4 datasets\n",
      "t9: 4 datasets\n",
      "postalcode_destination_hospital: 4 datasets\n",
      "province_intervention: 4 datasets\n",
      "intervention_time_(t1confirmed): 4 datasets\n",
      "departure_time_(t1confirmed): 4 datasets\n"
     ]
    }
   ],
   "source": [
    "# Get unique column names across all datasets\n",
    "all_columns = set().union(*(dataset.columns for dataset in all_interventions))\n",
    "\n",
    "# Count occurrences of each column across datasets\n",
    "column_count = {column: sum(column in dataset.columns for dataset in all_interventions) for column in all_columns}\n",
    "\n",
    "# Print column counts\n",
    "for column, count in sorted(column_count.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{column}: {count} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data sets as a Parquet file\n",
    "df_interventions_bxl.to_parquet('../../1_Data/CLEANED/df_interventions_bxl.parquet')\n",
    "df_interventions_bxl2.to_parquet('../../1_Data/CLEANED/df_interventions_bxl2.parquet')\n",
    "df_interventions1.to_parquet('../../1_Data/CLEANED/df_interventions1.parquet')\n",
    "df_interventions2.to_parquet('../../1_Data/CLEANED/df_interventions2.parquet')\n",
    "df_interventions3.to_parquet('../../1_Data/CLEANED/df_interventions3.parquet')\n",
    "df_cad9.to_parquet('../../1_Data/CLEANED/df_cad9.parquet')\n",
    "interventions_dataset.to_parquet('../../1_Data/CLEANED/interventions_dataset.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "# Path to your Parquet gzip file\n",
    "file_path_5 = '../../1_Data/CLEANED/df_interventions_bxl.parquet'\n",
    "file_path_6 = '../../1_Data/CLEANED/df_interventions_bxl2.parquet'\n",
    "file_path_7 = '../../1_Data/CLEANED/df_interventions1.parquet'\n",
    "file_path_8 = '../../1_Data/CLEANED/df_interventions2.parquet'\n",
    "file_path_9 = '../../1_Data/CLEANED/df_interventions3.parquet'\n",
    "file_path_10 = '../../1_Data/CLEANED/df_cad9.parquet'\n",
    "file_path_11 = '../../1_Data/CLEANED/interventions_dataset.parquet'\n",
    "\n",
    "# Read the Parquet file into a pandas DataFrame\n",
    "df_interventions_bxl = pd.read_parquet(file_path_5, engine='pyarrow')\n",
    "df_interventions_bxl2 = pd.read_parquet(file_path_6, engine='pyarrow')\n",
    "df_interventions1 = pd.read_parquet(file_path_7, engine='pyarrow')\n",
    "df_interventions2 = pd.read_parquet(file_path_8, engine='pyarrow')\n",
    "df_interventions3 = pd.read_parquet(file_path_9, engine='pyarrow')\n",
    "df_cad9 = pd.read_parquet(file_path_10, engine='pyarrow')\n",
    "interventions_dataset = pd.read_parquet(file_path_11, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset of the data\n",
    "subset_selected_events = interventions_dataset[\n",
    "    interventions_dataset['eventtype_trip'].isin([\n",
    "    'P002 - Agression - fight - rape',\n",
    "    'P003 - Cardiac arrest',\n",
    "    'P010 - Respiratory problems',\n",
    "    'P020 - Intoxication alcohol',\n",
    "    'P021 - Intoxication drugs',\n",
    "    'P032 - Allergic reactions',\n",
    "    'P036 - Heat stroke - solar stroke',\n",
    "    'P072 - Sick child < 15 years with fever',\n",
    "    'P080 - COVID-19']) \n",
    "]\n",
    "\n",
    "subset_correlation_heatmap1 = interventions_dataset[\n",
    "    interventions_dataset['eventlevel_trip'].isin(['N0', 'N1', 'N2', 'N3', 'N4', 'N5'])\n",
    "    & interventions_dataset['eventlevel_firstcall'].isin(['N0', 'N1', 'N2', 'N3', 'N4', 'N5'])\n",
    "]\n",
    "subset_correlation_heatmap2 = interventions_dataset[\n",
    "    interventions_dataset['eventtype_trip'].isin([\n",
    "    'P002 - Agression - fight - rape',\n",
    "    'P003 - Cardiac arrest',\n",
    "    'P010 - Respiratory problems',\n",
    "    'P020 - Intoxication alcohol',\n",
    "    'P021 - Intoxication drugs',\n",
    "    'P032 - Allergic reactions',\n",
    "    'P036 - Heat stroke - solar stroke',\n",
    "    'P072 - Sick child < 15 years with fever',\n",
    "    'P080 - COVID-19']) \n",
    "    & interventions_dataset['eventtype_firstcall'].isin([\n",
    "    'P002 - Agression - fight - rape',\n",
    "    'P003 - Cardiac arrest',\n",
    "    'P010 - Respiratory problems',\n",
    "    'P020 - Intoxication alcohol',\n",
    "    'P021 - Intoxication drugs',\n",
    "    'P032 - Allergic reactions',\n",
    "    'P036 - Heat stroke - solar stroke',\n",
    "    'P072 - Sick child < 15 years with fever',\n",
    "    'P080 - COVID-19']) \n",
    "]\n",
    "subset_correlation_heatmap3 = interventions_dataset[\n",
    "    interventions_dataset['eventtype_trip'].isin([\n",
    "        'P003 - Cardiac arrest',\n",
    "        'P004 - Stroke',\n",
    "        'P011 - Chest pain']) \n",
    "    & interventions_dataset['eventlevel_trip'].isin(['N0', 'N1', 'N2', 'N3', 'N4', 'N5']) \n",
    "]\n",
    "subset_correlation_heatmap4 = interventions_dataset[\n",
    "    interventions_dataset['eventlevel_trip'].isin(['N0', 'N1', 'N2', 'N3', 'N4', 'N5']) \n",
    "    & interventions_dataset['vector_type'].isin(['MUG', 'PIT', 'Ambulance'])\n",
    "]\n",
    "subset_correlation_heatmap5 = interventions_dataset[\n",
    "    interventions_dataset['eventtype_trip'].isin([\n",
    "    'P002 - Agression - fight - rape',\n",
    "    'P003 - Cardiac arrest',\n",
    "    'P010 - Respiratory problems',\n",
    "    'P020 - Intoxication alcohol',\n",
    "    'P021 - Intoxication drugs',\n",
    "    'P032 - Allergic reactions',\n",
    "    'P036 - Heat stroke - solar stroke',\n",
    "    'P072 - Sick child < 15 years with fever',\n",
    "    'P080 - COVID-19',\n",
    "    'P099 - Interhospital transport',\n",
    "    'P011 - Chest pain',\n",
    "    'P019 - Unconscious - syncope',\n",
    "    'FI (1.3.0) fire building',\n",
    "    'HG (2.1.1) gas odour',\n",
    "    'HG (2.1.2) gas leak',\n",
    "    'TI (3.3.2) CO intoxication'\n",
    "    ]) \n",
    "    & interventions_dataset['vector_type'].isin(['MUG', 'PIT', 'Ambulance', 'Ambulance Exceptional', 'Fire ambulance'])\n",
    "]\n",
    "subset_correlation_heatmap6 = interventions_dataset[\n",
    "    interventions_dataset['eventtype_trip'].isin([\n",
    "    'P003 - Cardiac arrest',\n",
    "    'P067 - Social problem']) \n",
    "]\n",
    "subset_line_charts = interventions_dataset[\n",
    "    interventions_dataset['eventtype_trip'].isin([\n",
    "    'P002 - Agression - fight - rape',\n",
    "    'P003 - Cardiac arrest',\n",
    "    'P010 - Respiratory problems',\n",
    "    'P012 - Non-traumatic abdominal pain',\n",
    "    'P013 - Non-traumatic back pain',\n",
    "    'P020 - Intoxication alcohol',\n",
    "    'P021 - Intoxication drugs',\n",
    "    'P022 - Intoxication medication',\n",
    "    'P029 - Obstruction of the respiratory tract',\n",
    "    'P031 - Psychiatric problem',\n",
    "    'P032 - Allergic reactions',\n",
    "    'P067 - Social problem',\n",
    "    'P080 - COVID-19',\n",
    "    'P097 - Collocation (planned)'\n",
    "    ]) \n",
    "]\n",
    "\n",
    "subset_map_subplots = interventions_dataset[\n",
    "    interventions_dataset['eventtype_trip'].isin([\n",
    "    'P001 - Traffic accident',\n",
    "    'P032 - Allergic reactions',\n",
    "    'P036 - Heat stroke - solar stroke',\n",
    "    'P072 - Sick child < 15 years with fever',\n",
    "    'P080 - COVID-19']) \n",
    "]\n",
    "\n",
    "subset_cardiac = interventions_dataset[interventions_dataset['eventtype_trip'].isin(['P003 - Cardiac arrest'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save subset\n",
    "subset_selected_events.to_parquet('../../1_Data/CLEANED/subset_selected_events.parquet')\n",
    "subset_correlation_heatmap1.to_parquet('../../1_Data/CLEANED/subset_correlation_heatmap1.parquet')\n",
    "subset_correlation_heatmap2.to_parquet('../../1_Data/CLEANED/subset_correlation_heatmap2.parquet')\n",
    "subset_correlation_heatmap3.to_parquet('../../1_Data/CLEANED/subset_correlation_heatmap3.parquet')\n",
    "subset_correlation_heatmap4.to_parquet('../../1_Data/CLEANED/subset_correlation_heatmap4.parquet')\n",
    "subset_correlation_heatmap5.to_parquet('../../1_Data/CLEANED/subset_correlation_heatmap5.parquet')\n",
    "subset_correlation_heatmap6.to_parquet('../../1_Data/CLEANED/subset_correlation_heatmap6.parquet')\n",
    "subset_line_charts.to_parquet('../../1_Data/CLEANED/subset_line_charts.parquet')\n",
    "subset_map_subplots.to_parquet('../../1_Data/CLEANED/subset_map_subplots.parquet')\n",
    "subset_cardiac.to_parquet('../../1_Data/CLEANED/subset_cardiac.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'mission_id' has more than 100 unique values. Skipping printing...\n",
      "Column 'service_name' has more than 100 unique values. Skipping printing...\n",
      "Column 'postalcode_permanence' has more than 100 unique values. Skipping printing...\n",
      "Column 'cityname_permanence' has more than 100 unique values. Skipping printing...\n",
      "Column 'streetname_permanence' has more than 100 unique values. Skipping printing...\n",
      "Column: housenumber_permanence\n",
      "[None '11' '74' '148816' '148510' '322' '201' '627' '61' '149104' '145735'\n",
      " '151600' '148100' '152391' '92' '142260' '63' '149925' '151370' '10' '1'\n",
      " '5' '72' '100' '32' '115' '133' '7' '550' '77' '47' '54' '142' '46' '13'\n",
      " '9' '147' '37' '57' '55' '34' '12' '26' '6' '24' '2' '19' '111' '23']\n",
      "\n",
      "Column 'latitude_permanence' has more than 100 unique values. Skipping printing...\n",
      "Column 'longitude_permanence' has more than 100 unique values. Skipping printing...\n",
      "Column 'permanence_short_name' has more than 100 unique values. Skipping printing...\n",
      "Column 'permanence_long_name' has more than 100 unique values. Skipping printing...\n",
      "Column: vector_type\n",
      "['Ambulance' 'MUG' 'PIT' 'Fire ambulance' 'Decontamination ambulance' None\n",
      " 'Ambulance Event' 'Ambulance Disaster' 'MUG Event' 'PIT Event'\n",
      " 'Ambulance Exceptional' 'MUG Disaster' 'PIT Disaster']\n",
      "\n",
      "Column 'eventtype_firstcall' has more than 100 unique values. Skipping printing...\n",
      "Column: eventlevel_firstcall\n",
      "['N5' None 'N3' 'N1' 'N4' 'N2' 'N6' 'N0' 'N8' 'N7A' 'N7B']\n",
      "\n",
      "Column 'eventtype_trip' has more than 100 unique values. Skipping printing...\n",
      "Column: eventlevel_trip\n",
      "['N5' 'N1' 'N3' None 'N4' 'N2' 'N0' 'N6' 'N8' 'Buitendienststelling' 'N7'\n",
      " 'Interventieplan' 'N7A' 'N7B' 'T' 'TS' 'S' 'G' 'B' 'VO' 'TP' 'EK' '5Z'\n",
      " 'L' 'GE' 'IF' 'PO' 'KN' 'SO' 'AG' 'TC' 'PR' 'OR' 'OF' 'VL' 'IZ' 'OD' 'BI'\n",
      " 'EU']\n",
      "\n",
      "Column 'postalcode_intervention' has more than 100 unique values. Skipping printing...\n",
      "Column 'cityname_intervention' has more than 100 unique values. Skipping printing...\n",
      "Column 'latitude_intervention' has more than 100 unique values. Skipping printing...\n",
      "Column 'longitude_intervention' has more than 100 unique values. Skipping printing...\n",
      "Column 't0' has more than 100 unique values. Skipping printing...\n",
      "Column 't1' has more than 100 unique values. Skipping printing...\n",
      "Column 't1confirmed' has more than 100 unique values. Skipping printing...\n",
      "Column 't2' has more than 100 unique values. Skipping printing...\n",
      "Column 't3' has more than 100 unique values. Skipping printing...\n",
      "Column 't4' has more than 100 unique values. Skipping printing...\n",
      "Column 't5' has more than 100 unique values. Skipping printing...\n",
      "Column 't6' has more than 100 unique values. Skipping printing...\n",
      "Column 't7' has more than 100 unique values. Skipping printing...\n",
      "Column 't9' has more than 100 unique values. Skipping printing...\n",
      "Column 'intervention_time_(t1reported)' has more than 100 unique values. Skipping printing...\n",
      "Column 'waiting_time' has more than 100 unique values. Skipping printing...\n",
      "Column 'intervention_duration' has more than 100 unique values. Skipping printing...\n",
      "Column 'departure_time_(t1reported)' has more than 100 unique values. Skipping printing...\n",
      "Column 'unavailable_time' has more than 100 unique values. Skipping printing...\n",
      "Column 'name_destination_hospital' has more than 100 unique values. Skipping printing...\n",
      "Column: postalcode_destination_hospital\n",
      "[None '1000' '1200' '1070' '1160' '1050' '1030' '1180' '1020' '1090'\n",
      " '1190' '1120' '1420' '1730' '1850' '1800' '3070' '3000' '2800' '1500'\n",
      " '9300' '2060' '7500' '2840' '1340' '1400' '9090' '3360' '1210' '4240']\n",
      "\n",
      "Column 'cityname_destination_hospital' has more than 100 unique values. Skipping printing...\n",
      "Column 'streetname_destination_hospital' has more than 100 unique values. Skipping printing...\n",
      "Column: housenumber_destination_hospital\n",
      "[None '10' '808' '201' '66' '63' '150' '322' '36' '206' '101' '11' '142'\n",
      " '79' '15' '1' '43' '35' '5' '40' '65' '34' '517' '49' '102' '435' '100'\n",
      " '164' '294' '109' '172' '9' '80' '76' '4' '22']\n",
      "\n",
      "Column 'calculated_traveltime_destinatio' has more than 100 unique values. Skipping printing...\n",
      "Column 'calculated_distance_destination' has more than 100 unique values. Skipping printing...\n",
      "Column: number_of_transported_persons\n",
      "['nan' '1.0' '2.0' '4.0' '3.0' None]\n",
      "\n",
      "Column: abandon_reason\n",
      "['Error' None 'Refusal of transport' 'Cancelled' 'Without patient'\n",
      " 'Transported by police' 'Malicious' 'Cared for on-site' 'Deceased'\n",
      " 'Transported by third parties' 'Malicious call' 'Cancelled trip'\n",
      " 'False alarm well-intentioned' 'Transport by third parties'\n",
      " 'Dead On Site' 'Treated on site']\n",
      "\n",
      "Column: t0_Hour\n",
      "[ 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23.  0.  1.  2.\n",
      "  3.  4.  5.  6.  7.  8. nan]\n",
      "\n",
      "Column: t0_Day\n",
      "[ 6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23.\n",
      " 24. 25. 26. 27. 28. 29. 30.  1.  2.  3.  4.  5. 31. nan]\n",
      "\n",
      "Column: t0_Month\n",
      "[ 9. 10. 11. 12.  1.  2.  3.  4.  5.  6.  7.  8. nan]\n",
      "\n",
      "Column: t0_DayName\n",
      "['Tuesday' 'Wednesday' 'Thursday' 'Friday' 'Saturday' 'Sunday' 'Monday'\n",
      " None]\n",
      "\n",
      "Column: t7_Hour\n",
      "[ 9. 10. 11. nan 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. 23.  0.  1.\n",
      "  2.  3.  4.  5.  6.  7.  8.]\n",
      "\n",
      "Column: t7_Day\n",
      "[ 6. nan  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22.\n",
      " 23. 24. 25. 26. 27. 28. 29. 30.  1.  2.  3.  4.  5. 31.]\n",
      "\n",
      "Column: t7_Month\n",
      "[ 9. nan 10. 11. 12.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      "\n",
      "Column: t7_DayName\n",
      "['Tuesday' None 'Wednesday' 'Thursday' 'Friday' 'Saturday' 'Sunday'\n",
      " 'Monday']\n",
      "\n",
      "Column: province_intervention\n",
      "[None 'ANT' 'VBR' 'BRW' 'WVL' 'OVL' 'HAI' 'LIE' 'LIM' 'LUX' 'NAM' 'BXL'\n",
      " 'NL']\n",
      "\n",
      "Column 'intervention_time_(t1confirmed)' has more than 100 unique values. Skipping printing...\n",
      "Column 'departure_time_(t1confirmed)' has more than 100 unique values. Skipping printing...\n"
     ]
    }
   ],
   "source": [
    "for column in interventions_dataset.columns:\n",
    "    unique_values = interventions_dataset[column].unique()\n",
    "    if len(unique_values) <= 100:\n",
    "        print(f\"Column: {column}\")\n",
    "        print(unique_values)\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Column '{column}' has more than 100 unique values. Skipping printing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique postal codes in the dataset: 12\n"
     ]
    }
   ],
   "source": [
    "# Assuming interventions_dataset is your DataFrame\n",
    "num_postal_codes = interventions_dataset['province_intervention'].nunique()\n",
    "print(\"Number of unique postal codes in the dataset:\", num_postal_codes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
